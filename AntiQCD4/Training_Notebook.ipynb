{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravindhv10/CPP_Wrappers/blob/master/AntiQCD4/Training_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrGDgE1HvDvJ",
        "colab_type": "text"
      },
      "source": [
        "This program will not generate the jet images, it will only train the autoencoder\n",
        "and evaluate the results. The jet images can be found in:\n",
        "\n",
        "https://drive.google.com/drive/folders/1i5DY9duzDuumQz636u5YQeYQEt_7TYa8?usp=sharing\n",
        "\n",
        "Please download those images to your google drive and use the colab - drive integration.\n",
        "\n",
        "A program to generate jet images is available at\n",
        "\n",
        "https://github.com/aravindhv10/CPP_Wrappers/blob/master/AntiQCD4/JetImageFormation.hh\n",
        "\n",
        "in the form of the class `BoxImageGen`.\n",
        "The images used in this program were produced using `BoxImageGen<40,float,true>` with the ratio $m_J/E_J=0.5$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxkahq1kgvrr",
        "colab_type": "code",
        "outputId": "c9c7f515-8aef-4f31-aba7-025a169839ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "# This program will not generate the jet images, it will only train the autoencoder\n",
        "# and evaluate the results. The jet images can be found in:\n",
        "# https://drive.google.com/drive/folders/1i5DY9duzDuumQz636u5YQeYQEt_7TYa8?usp=sharing\n",
        "# Please download those images to your google drive and use the colab - drive integration.\n",
        "import lzma\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def READ_XZ (filename):\n",
        "    file = lzma.LZMAFile(filename)\n",
        "    type_bytes = file.read(-1)\n",
        "    type_array = np.frombuffer(type_bytes,dtype='float32')                                                \n",
        "    return type_array\n",
        "\n",
        "def Count(array,val):\n",
        "  count = 0.0\n",
        "  for e in range(array.shape[0]):\n",
        "    if array[e]>val :\n",
        "      count=count+1.0\n",
        "  return count / array.shape[0]\n",
        "\n",
        "width=40\n",
        "batch_size=200\n",
        "ModelName = \"Model_40_24_8_24_40_40\"\n",
        "\n",
        "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
        "sess = tf.Session(config=config)\n",
        "keras.backend.set_session(sess)\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0EEbtJ1sC-P",
        "colab_type": "text"
      },
      "source": [
        "# Defining network architecture (we use `Arch-2`)\n",
        "\n",
        "We also define some functions to make training convinent here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddbNPzUutJdS",
        "colab_type": "code",
        "outputId": "2e4b2c0e-3b34-4387-f576-ef7be86efdf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# this is our input placeholder\n",
        "input_img = Input(shape=(width*width,))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "Layer1 = Dense(24*24, activation='relu')(input_img)\n",
        "Layer2 = Dense(8*8, activation='relu')(Layer1)\n",
        "Layer3 = Dense(24*24, activation='relu')(Layer2)\n",
        "Layer4 = Dense(40*40, activation='relu')(Layer3)\n",
        "Out = Dense(40*40, activation='softmax')(Layer4)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, Out)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "def EvalOnFile (InFileName,OutFileName):\n",
        "  data = READ_XZ (InFileName)\n",
        "  x_train = data.reshape(-1,width*width)\n",
        "  x_out = autoencoder.predict(x_train,200,use_multiprocessing=True)\n",
        "  diff = x_train - x_out\n",
        "  lrnorm = np.ones((diff.shape[0]))\n",
        "  for e in range(diff.shape[0]):\n",
        "    lrnorm[e] = np.linalg.norm(diff[e])\n",
        "  lrnorm.tofile(OutFileName)\n",
        "  print(lrnorm.shape)\n",
        "\n",
        "def TrainOnFile (filename,testfilename,totalepochs):\n",
        "  data = READ_XZ (filename)\n",
        "  x_train = data.reshape(-1,width*width)\n",
        "  datatest = READ_XZ (testfilename)\n",
        "  x_test = datatest.reshape(-1,width*width)\n",
        "  autoencoder.fit(\n",
        "      x_train, x_train, epochs=totalepochs,\n",
        "      batch_size=200, shuffle=True,\n",
        "      validation_data=(x_test, x_test)\n",
        "  )\n",
        "  autoencoder.save(ModelName)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3ZZwIeUsSKL",
        "colab_type": "text"
      },
      "source": [
        "# Mounting folder from Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35qBCEBhg2zv",
        "colab_type": "code",
        "outputId": "35ab2cb1-033c-4728-c871-1ab9bae12199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# Please download the files from the link below and appropriately change this program:\n",
        "# https://drive.google.com/drive/folders/1i5DY9duzDuumQz636u5YQeYQEt_7TYa8?usp=sharing\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsyLJeKmsYk-",
        "colab_type": "text"
      },
      "source": [
        "Verify the files are correctly mounted and available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0acEza2g46D",
        "colab_type": "code",
        "outputId": "b535db71-743a-4a6d-cfb3-adcd99533a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%cd /gdrive/My\\ Drive/JetImages/QCD/\n",
        "!ls ./TEST/BoxImages/0.xz\n",
        "!ls ./TRAIN/BoxImages/0.xz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/JetImages/QCD\n",
            "./TEST/BoxImages/0.xz\n",
            "./TRAIN/BoxImages/0.xz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEsQHmSCsfIp",
        "colab_type": "text"
      },
      "source": [
        "Load the model in case a trained one is already available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUrbPzQ3wQBA",
        "colab_type": "code",
        "outputId": "ab322df8-2162-439e-b692-238766da587d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/QCD\n",
        "autoencoder = keras.models.load_model(ModelName)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/JetImages/QCD\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgIZshM6sk7E",
        "colab_type": "text"
      },
      "source": [
        "# The training step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlDnFjzKKwsf",
        "colab_type": "code",
        "outputId": "e2598809-e21d-40ff-a2ba-e02d8462f742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/QCD\n",
        "for e in range(4):\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/0.xz\",\"./TEST/BoxImages/0.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/1.xz\",\"./TEST/BoxImages/1.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/2.xz\",\"./TEST/BoxImages/2.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/3.xz\",\"./TEST/BoxImages/3.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/4.xz\",\"./TEST/BoxImages/4.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/5.xz\",\"./TEST/BoxImages/5.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/6.xz\",\"./TEST/BoxImages/6.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/7.xz\",\"./TEST/BoxImages/7.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/8.xz\",\"./TEST/BoxImages/8.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/9.xz\",\"./TEST/BoxImages/9.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/10.xz\",\"./TEST/BoxImages/10.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/11.xz\",\"./TEST/BoxImages/11.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/12.xz\",\"./TEST/BoxImages/12.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/13.xz\",\"./TEST/BoxImages/13.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/14.xz\",\"./TEST/BoxImages/14.xz\",10)\n",
        "  TrainOnFile(\"./TRAIN/BoxImages/15.xz\",\"./TEST/BoxImages/15.xz\",10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "100000/100000 [==============================] - 5s 47us/step - loss: 1.0627e-04 - val_loss: 1.0634e-04\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 1.0621e-04 - val_loss: 1.0624e-04\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 9.6311e-05 - val_loss: 5.9276e-05\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8847e-05 - val_loss: 3.0651e-05\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 2.6664e-05 - val_loss: 2.3519e-05\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 2.1724e-05 - val_loss: 2.0635e-05\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 1.9779e-05 - val_loss: 1.9115e-05\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 1.8726e-05 - val_loss: 1.8336e-05\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 1.7859e-05 - val_loss: 1.7425e-05\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.6875e-05 - val_loss: 1.6549e-05\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.6198e-05 - val_loss: 1.6035e-05\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 1.5692e-05 - val_loss: 1.5579e-05\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.5201e-05 - val_loss: 1.5058e-05\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.4754e-05 - val_loss: 1.4708e-05\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.4372e-05 - val_loss: 1.4255e-05\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.4005e-05 - val_loss: 1.3937e-05\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.3614e-05 - val_loss: 1.3538e-05\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.3265e-05 - val_loss: 1.3252e-05\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.2968e-05 - val_loss: 1.2967e-05\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.2692e-05 - val_loss: 1.2683e-05\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.2533e-05 - val_loss: 1.2338e-05\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 1.2328e-05 - val_loss: 1.2162e-05\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.2175e-05 - val_loss: 1.2048e-05\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.2025e-05 - val_loss: 1.1900e-05\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.1879e-05 - val_loss: 1.1748e-05\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.1742e-05 - val_loss: 1.1652e-05\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.1599e-05 - val_loss: 1.1510e-05\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.1478e-05 - val_loss: 1.1388e-05\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.1355e-05 - val_loss: 1.1277e-05\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.1220e-05 - val_loss: 1.1140e-05\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.1077e-05 - val_loss: 1.1106e-05\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.0968e-05 - val_loss: 1.1058e-05\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.0872e-05 - val_loss: 1.0956e-05\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.0769e-05 - val_loss: 1.0822e-05\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.0672e-05 - val_loss: 1.0728e-05\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.0576e-05 - val_loss: 1.0622e-05\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 1.0481e-05 - val_loss: 1.0561e-05\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 1.0400e-05 - val_loss: 1.0435e-05\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.0313e-05 - val_loss: 1.0432e-05\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.0241e-05 - val_loss: 1.0309e-05\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.0301e-05 - val_loss: 1.0329e-05\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 1.0206e-05 - val_loss: 1.0284e-05\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.0118e-05 - val_loss: 1.0169e-05\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 1.0041e-05 - val_loss: 1.0099e-05\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 9.9610e-06 - val_loss: 1.0036e-05\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 9.8882e-06 - val_loss: 9.9848e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 9.8160e-06 - val_loss: 9.9071e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 9.7435e-06 - val_loss: 9.8389e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 9.6901e-06 - val_loss: 9.8035e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 9.6151e-06 - val_loss: 9.7470e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 9.6597e-06 - val_loss: 9.5944e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 9.5865e-06 - val_loss: 9.5060e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 9.5218e-06 - val_loss: 9.4821e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 9.4676e-06 - val_loss: 9.4151e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 9.4058e-06 - val_loss: 9.3492e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 9.3517e-06 - val_loss: 9.3045e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 9.2953e-06 - val_loss: 9.2750e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 9.2446e-06 - val_loss: 9.2007e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 9.1910e-06 - val_loss: 9.1551e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 9.1326e-06 - val_loss: 9.0958e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 9.0694e-06 - val_loss: 8.9412e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 8.9925e-06 - val_loss: 8.9293e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.9263e-06 - val_loss: 8.8557e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.8674e-06 - val_loss: 8.7999e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.8045e-06 - val_loss: 8.7643e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.7496e-06 - val_loss: 8.7158e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.6954e-06 - val_loss: 8.6511e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.6416e-06 - val_loss: 8.5872e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.5878e-06 - val_loss: 8.5389e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.5375e-06 - val_loss: 8.5051e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.6332e-06 - val_loss: 8.4847e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 8.5704e-06 - val_loss: 8.4284e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.5181e-06 - val_loss: 8.3927e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.4735e-06 - val_loss: 8.3690e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.4277e-06 - val_loss: 8.3327e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.3883e-06 - val_loss: 8.2933e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.3515e-06 - val_loss: 8.2765e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.3095e-06 - val_loss: 8.2232e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.2703e-06 - val_loss: 8.2039e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.2308e-06 - val_loss: 8.1521e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 8.2694e-06 - val_loss: 8.1410e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.2127e-06 - val_loss: 8.1180e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.1660e-06 - val_loss: 8.0654e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.1326e-06 - val_loss: 8.0324e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 8.0984e-06 - val_loss: 8.0000e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.0552e-06 - val_loss: 7.9752e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 8.0204e-06 - val_loss: 7.9470e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.9909e-06 - val_loss: 7.9415e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.9507e-06 - val_loss: 7.9015e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.9179e-06 - val_loss: 7.8523e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.8914e-06 - val_loss: 7.9466e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 7.8393e-06 - val_loss: 7.8881e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.7976e-06 - val_loss: 7.8684e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.7627e-06 - val_loss: 7.8503e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.7275e-06 - val_loss: 7.8135e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.6963e-06 - val_loss: 7.8002e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.6688e-06 - val_loss: 7.7715e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.6344e-06 - val_loss: 7.7428e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.6070e-06 - val_loss: 7.7325e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.5813e-06 - val_loss: 7.6769e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.6974e-06 - val_loss: 7.7758e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.6463e-06 - val_loss: 7.7287e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.6042e-06 - val_loss: 7.6932e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.5600e-06 - val_loss: 7.6740e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.5252e-06 - val_loss: 7.6426e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.4878e-06 - val_loss: 7.5978e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.4533e-06 - val_loss: 7.5745e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.4188e-06 - val_loss: 7.5509e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.3882e-06 - val_loss: 7.5314e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.3653e-06 - val_loss: 7.5107e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 7.4654e-06 - val_loss: 7.4843e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 7.4093e-06 - val_loss: 7.4612e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.3711e-06 - val_loss: 7.4096e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.3407e-06 - val_loss: 7.3868e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.3092e-06 - val_loss: 7.3954e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.2771e-06 - val_loss: 7.3407e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.2471e-06 - val_loss: 7.3308e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.2182e-06 - val_loss: 7.3391e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.1925e-06 - val_loss: 7.2741e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 7.1627e-06 - val_loss: 7.2633e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.2044e-06 - val_loss: 7.1965e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 7.1578e-06 - val_loss: 7.1655e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.1188e-06 - val_loss: 7.1600e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.0860e-06 - val_loss: 7.1196e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.0519e-06 - val_loss: 7.0941e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 7.0244e-06 - val_loss: 7.0970e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.9929e-06 - val_loss: 7.0548e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.9656e-06 - val_loss: 7.0330e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.9422e-06 - val_loss: 7.0430e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.9159e-06 - val_loss: 6.9979e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 7.0328e-06 - val_loss: 7.0887e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.9908e-06 - val_loss: 7.0634e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.9505e-06 - val_loss: 7.0181e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.9235e-06 - val_loss: 7.0103e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 6.8925e-06 - val_loss: 6.9893e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 6.8637e-06 - val_loss: 6.9663e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 6.8394e-06 - val_loss: 6.9482e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.8126e-06 - val_loss: 6.9441e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.7858e-06 - val_loss: 6.9089e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.7673e-06 - val_loss: 6.9239e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.8281e-06 - val_loss: 6.9183e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 6.7757e-06 - val_loss: 6.9197e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.7440e-06 - val_loss: 6.8796e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.7104e-06 - val_loss: 6.8793e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.6781e-06 - val_loss: 6.8427e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.6552e-06 - val_loss: 6.8433e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.6260e-06 - val_loss: 6.8170e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.6042e-06 - val_loss: 6.7831e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.5781e-06 - val_loss: 6.7922e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.5556e-06 - val_loss: 6.7610e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.6825e-06 - val_loss: 6.6412e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 6.6337e-06 - val_loss: 6.6140e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.5968e-06 - val_loss: 6.5681e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.5670e-06 - val_loss: 6.5782e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.5410e-06 - val_loss: 6.5517e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.5158e-06 - val_loss: 6.5239e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.4899e-06 - val_loss: 6.5228e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.4716e-06 - val_loss: 6.4835e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.4429e-06 - val_loss: 6.4770e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.4198e-06 - val_loss: 6.4579e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.4815e-06 - val_loss: 6.5513e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.4325e-06 - val_loss: 6.5042e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.4013e-06 - val_loss: 6.4848e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.3749e-06 - val_loss: 6.4675e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.3466e-06 - val_loss: 6.4764e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.3193e-06 - val_loss: 6.4324e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.2993e-06 - val_loss: 6.4277e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.2789e-06 - val_loss: 6.4256e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.2566e-06 - val_loss: 6.3951e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.2318e-06 - val_loss: 6.3767e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.3536e-06 - val_loss: 6.4004e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.3058e-06 - val_loss: 6.4068e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.2689e-06 - val_loss: 6.3697e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.2398e-06 - val_loss: 6.3466e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.2140e-06 - val_loss: 6.3381e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.1893e-06 - val_loss: 6.3315e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.1658e-06 - val_loss: 6.3137e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.1429e-06 - val_loss: 6.3160e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.1231e-06 - val_loss: 6.3029e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.0992e-06 - val_loss: 6.2793e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.2250e-06 - val_loss: 6.1548e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.1725e-06 - val_loss: 6.1214e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.1393e-06 - val_loss: 6.1088e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.1133e-06 - val_loss: 6.1069e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.0897e-06 - val_loss: 6.0755e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.0630e-06 - val_loss: 6.0765e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 6.0446e-06 - val_loss: 6.0670e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.0200e-06 - val_loss: 6.0548e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.0055e-06 - val_loss: 6.0430e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.9863e-06 - val_loss: 6.0416e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 6.0644e-06 - val_loss: 6.0228e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 6.0155e-06 - val_loss: 6.0241e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.9828e-06 - val_loss: 6.0255e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.9583e-06 - val_loss: 5.9870e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.9355e-06 - val_loss: 5.9774e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.9136e-06 - val_loss: 5.9729e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.8943e-06 - val_loss: 5.9601e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.8738e-06 - val_loss: 5.9456e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.8571e-06 - val_loss: 5.9446e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.8375e-06 - val_loss: 5.9362e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.9924e-06 - val_loss: 5.9992e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.9459e-06 - val_loss: 5.9655e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.9129e-06 - val_loss: 5.9611e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.8861e-06 - val_loss: 5.9516e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.8622e-06 - val_loss: 5.9254e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.8440e-06 - val_loss: 5.9098e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.8242e-06 - val_loss: 5.9160e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.8031e-06 - val_loss: 5.8952e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.7874e-06 - val_loss: 5.8925e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.7692e-06 - val_loss: 5.8968e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.9092e-06 - val_loss: 5.8486e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 5.8597e-06 - val_loss: 5.8357e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 5.8281e-06 - val_loss: 5.8131e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 5.8037e-06 - val_loss: 5.8046e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.7819e-06 - val_loss: 5.7979e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.7592e-06 - val_loss: 5.7942e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.7411e-06 - val_loss: 5.7807e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.7228e-06 - val_loss: 5.7675e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.7048e-06 - val_loss: 5.7445e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.6852e-06 - val_loss: 5.7542e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.7370e-06 - val_loss: 5.7314e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.6865e-06 - val_loss: 5.7044e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.6542e-06 - val_loss: 5.6864e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.6306e-06 - val_loss: 5.6877e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.6065e-06 - val_loss: 5.6662e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5914e-06 - val_loss: 5.6563e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5708e-06 - val_loss: 5.6287e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5556e-06 - val_loss: 5.6382e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5392e-06 - val_loss: 5.6265e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5213e-06 - val_loss: 5.6172e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.6870e-06 - val_loss: 5.5964e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 5.6398e-06 - val_loss: 5.5765e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.6101e-06 - val_loss: 5.5588e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5850e-06 - val_loss: 5.5390e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5624e-06 - val_loss: 5.5561e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5431e-06 - val_loss: 5.5327e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5231e-06 - val_loss: 5.5230e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5076e-06 - val_loss: 5.5085e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.4896e-06 - val_loss: 5.5123e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.4759e-06 - val_loss: 5.5059e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.5761e-06 - val_loss: 5.5068e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.5272e-06 - val_loss: 5.4766e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.4969e-06 - val_loss: 5.4529e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.4734e-06 - val_loss: 5.4439e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.4558e-06 - val_loss: 5.4382e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.4338e-06 - val_loss: 5.4500e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.4168e-06 - val_loss: 5.4209e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.3998e-06 - val_loss: 5.4077e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.3837e-06 - val_loss: 5.3950e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.3681e-06 - val_loss: 5.3838e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.4388e-06 - val_loss: 5.4599e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 5.3900e-06 - val_loss: 5.4405e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.3622e-06 - val_loss: 5.4426e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.3432e-06 - val_loss: 5.4321e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.3210e-06 - val_loss: 5.4222e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.3065e-06 - val_loss: 5.4160e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2857e-06 - val_loss: 5.4105e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2709e-06 - val_loss: 5.4038e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2555e-06 - val_loss: 5.3875e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2384e-06 - val_loss: 5.3855e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.3911e-06 - val_loss: 5.4330e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 5.3371e-06 - val_loss: 5.4047e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.3101e-06 - val_loss: 5.4132e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 5.2843e-06 - val_loss: 5.3669e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.2630e-06 - val_loss: 5.3717e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2441e-06 - val_loss: 5.3635e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2264e-06 - val_loss: 5.3552e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2077e-06 - val_loss: 5.3602e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1933e-06 - val_loss: 5.3482e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1762e-06 - val_loss: 5.3343e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.3363e-06 - val_loss: 5.3919e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.2843e-06 - val_loss: 5.3832e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2553e-06 - val_loss: 5.3564e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2286e-06 - val_loss: 5.3507e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2095e-06 - val_loss: 5.3453e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1904e-06 - val_loss: 5.3430e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1729e-06 - val_loss: 5.3184e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1589e-06 - val_loss: 5.3025e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1407e-06 - val_loss: 5.3049e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1283e-06 - val_loss: 5.3025e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.2155e-06 - val_loss: 5.2346e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.1641e-06 - val_loss: 5.2204e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1334e-06 - val_loss: 5.2056e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1130e-06 - val_loss: 5.1969e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.0923e-06 - val_loss: 5.1899e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0747e-06 - val_loss: 5.1769e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0592e-06 - val_loss: 5.1780e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0452e-06 - val_loss: 5.1649e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0298e-06 - val_loss: 5.1630e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0201e-06 - val_loss: 5.1639e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 5.1790e-06 - val_loss: 5.2306e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 5.1245e-06 - val_loss: 5.2036e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0931e-06 - val_loss: 5.1811e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0765e-06 - val_loss: 5.1883e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0555e-06 - val_loss: 5.1843e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0359e-06 - val_loss: 5.1706e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0191e-06 - val_loss: 5.1624e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0029e-06 - val_loss: 5.1413e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9876e-06 - val_loss: 5.1379e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9751e-06 - val_loss: 5.1334e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.1047e-06 - val_loss: 5.1467e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.0515e-06 - val_loss: 5.1359e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0183e-06 - val_loss: 5.1411e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9954e-06 - val_loss: 5.1151e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9775e-06 - val_loss: 5.1051e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9589e-06 - val_loss: 5.0928e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.9439e-06 - val_loss: 5.0983e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9290e-06 - val_loss: 5.0802e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9128e-06 - val_loss: 5.0760e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8980e-06 - val_loss: 5.0788e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 5.0601e-06 - val_loss: 5.0324e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 5.0087e-06 - val_loss: 5.0317e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9790e-06 - val_loss: 4.9943e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9547e-06 - val_loss: 4.9931e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.9326e-06 - val_loss: 4.9846e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9144e-06 - val_loss: 4.9776e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9033e-06 - val_loss: 4.9744e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8874e-06 - val_loss: 4.9685e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8739e-06 - val_loss: 4.9660e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8604e-06 - val_loss: 4.9702e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9566e-06 - val_loss: 4.9989e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.8983e-06 - val_loss: 4.9768e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8708e-06 - val_loss: 4.9651e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8476e-06 - val_loss: 4.9752e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8293e-06 - val_loss: 4.9561e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8117e-06 - val_loss: 4.9563e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7957e-06 - val_loss: 4.9460e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7822e-06 - val_loss: 4.9444e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7673e-06 - val_loss: 4.9465e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7569e-06 - val_loss: 4.9391e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.9263e-06 - val_loss: 4.9536e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.8737e-06 - val_loss: 4.9266e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8434e-06 - val_loss: 4.9299e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.8192e-06 - val_loss: 4.9247e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7987e-06 - val_loss: 4.9180e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7811e-06 - val_loss: 4.9137e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7659e-06 - val_loss: 4.8999e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.7488e-06 - val_loss: 4.9109e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7352e-06 - val_loss: 4.8977e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7233e-06 - val_loss: 4.8722e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.8617e-06 - val_loss: 4.8314e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.8111e-06 - val_loss: 4.8097e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7790e-06 - val_loss: 4.8009e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7539e-06 - val_loss: 4.7964e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7321e-06 - val_loss: 4.7900e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7168e-06 - val_loss: 4.7921e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7030e-06 - val_loss: 4.7867e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6890e-06 - val_loss: 4.7780e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6751e-06 - val_loss: 4.7963e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6606e-06 - val_loss: 4.7675e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7931e-06 - val_loss: 4.7816e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.7389e-06 - val_loss: 4.7665e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7118e-06 - val_loss: 4.7766e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6889e-06 - val_loss: 4.7610e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6689e-06 - val_loss: 4.7595e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6529e-06 - val_loss: 4.7452e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.6375e-06 - val_loss: 4.7451e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6236e-06 - val_loss: 4.7351e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6111e-06 - val_loss: 4.7344e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5981e-06 - val_loss: 4.7293e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7873e-06 - val_loss: 4.7585e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.7327e-06 - val_loss: 4.7247e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.7040e-06 - val_loss: 4.7173e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6777e-06 - val_loss: 4.7211e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6630e-06 - val_loss: 4.7047e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6471e-06 - val_loss: 4.7119e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6312e-06 - val_loss: 4.6956e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6167e-06 - val_loss: 4.6966e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.6045e-06 - val_loss: 4.6918e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 4.5934e-06 - val_loss: 4.6861e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.7610e-06 - val_loss: 4.6911e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.7092e-06 - val_loss: 4.6886e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6826e-06 - val_loss: 4.6868e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6576e-06 - val_loss: 4.6642e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6394e-06 - val_loss: 4.6756e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6233e-06 - val_loss: 4.6646e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6074e-06 - val_loss: 4.6507e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5920e-06 - val_loss: 4.6505e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5776e-06 - val_loss: 4.6456e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5673e-06 - val_loss: 4.6372e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6480e-06 - val_loss: 4.6261e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.5947e-06 - val_loss: 4.6169e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5646e-06 - val_loss: 4.6006e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.5429e-06 - val_loss: 4.5866e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5247e-06 - val_loss: 4.5874e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5072e-06 - val_loss: 4.5743e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4923e-06 - val_loss: 4.5799e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4804e-06 - val_loss: 4.5707e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.4687e-06 - val_loss: 4.5654e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4552e-06 - val_loss: 4.5830e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.6377e-06 - val_loss: 4.6119e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.5849e-06 - val_loss: 4.5783e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5553e-06 - val_loss: 4.5685e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5336e-06 - val_loss: 4.5691e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5137e-06 - val_loss: 4.5550e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4951e-06 - val_loss: 4.5450e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4833e-06 - val_loss: 4.5532e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4682e-06 - val_loss: 4.5582e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.4569e-06 - val_loss: 4.5511e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4436e-06 - val_loss: 4.5417e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.5831e-06 - val_loss: 4.5298e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.5268e-06 - val_loss: 4.5041e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4952e-06 - val_loss: 4.5020e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4757e-06 - val_loss: 4.4920e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4567e-06 - val_loss: 4.4930e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4397e-06 - val_loss: 4.4745e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.4255e-06 - val_loss: 4.4805e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4139e-06 - val_loss: 4.4777e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3996e-06 - val_loss: 4.4903e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3873e-06 - val_loss: 4.4651e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4925e-06 - val_loss: 4.5415e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.4378e-06 - val_loss: 4.5214e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4082e-06 - val_loss: 4.5073e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3880e-06 - val_loss: 4.5059e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3698e-06 - val_loss: 4.4968e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3529e-06 - val_loss: 4.4908e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3402e-06 - val_loss: 4.4985e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 4.3248e-06 - val_loss: 4.5085e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.3151e-06 - val_loss: 4.4907e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3019e-06 - val_loss: 4.4777e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4721e-06 - val_loss: 4.5017e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.4197e-06 - val_loss: 4.4919e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3920e-06 - val_loss: 4.4799e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3706e-06 - val_loss: 4.4774e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3529e-06 - val_loss: 4.4860e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3379e-06 - val_loss: 4.4678e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3228e-06 - val_loss: 4.4652e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3100e-06 - val_loss: 4.4683e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2986e-06 - val_loss: 4.4657e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.2898e-06 - val_loss: 4.4690e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.4704e-06 - val_loss: 4.4856e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.4059e-06 - val_loss: 4.4940e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3788e-06 - val_loss: 4.4966e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.3551e-06 - val_loss: 4.4717e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3364e-06 - val_loss: 4.4811e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.3215e-06 - val_loss: 4.4723e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3068e-06 - val_loss: 4.4576e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2943e-06 - val_loss: 4.4610e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.2823e-06 - val_loss: 4.4595e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2716e-06 - val_loss: 4.4655e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3898e-06 - val_loss: 4.4165e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.3355e-06 - val_loss: 4.4067e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3048e-06 - val_loss: 4.3951e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2852e-06 - val_loss: 4.3961e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2682e-06 - val_loss: 4.3659e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2526e-06 - val_loss: 4.3789e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.2390e-06 - val_loss: 4.3658e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.2282e-06 - val_loss: 4.3693e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.2166e-06 - val_loss: 4.3734e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2044e-06 - val_loss: 4.3598e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.3712e-06 - val_loss: 4.4112e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.3117e-06 - val_loss: 4.4072e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2852e-06 - val_loss: 4.3778e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2631e-06 - val_loss: 4.3869e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2468e-06 - val_loss: 4.3742e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2331e-06 - val_loss: 4.3816e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2189e-06 - val_loss: 4.3857e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2063e-06 - val_loss: 4.3828e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1942e-06 - val_loss: 4.3836e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1886e-06 - val_loss: 4.3743e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3337e-06 - val_loss: 4.3624e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.2803e-06 - val_loss: 4.3556e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2493e-06 - val_loss: 4.3393e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2277e-06 - val_loss: 4.3479e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2135e-06 - val_loss: 4.3424e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1981e-06 - val_loss: 4.3369e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1849e-06 - val_loss: 4.3352e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1749e-06 - val_loss: 4.3269e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1631e-06 - val_loss: 4.3301e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1537e-06 - val_loss: 4.3418e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.3280e-06 - val_loss: 4.3101e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.2686e-06 - val_loss: 4.2818e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.2404e-06 - val_loss: 4.2843e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.2203e-06 - val_loss: 4.2797e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2011e-06 - val_loss: 4.2753e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1884e-06 - val_loss: 4.2623e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1746e-06 - val_loss: 4.2644e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1642e-06 - val_loss: 4.2751e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.1537e-06 - val_loss: 4.2674e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1429e-06 - val_loss: 4.2636e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.2562e-06 - val_loss: 4.3086e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.2001e-06 - val_loss: 4.2986e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1728e-06 - val_loss: 4.2787e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1533e-06 - val_loss: 4.2827e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1348e-06 - val_loss: 4.2680e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.1225e-06 - val_loss: 4.2780e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1127e-06 - val_loss: 4.2728e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1001e-06 - val_loss: 4.2575e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0859e-06 - val_loss: 4.2563e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0775e-06 - val_loss: 4.2511e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.2469e-06 - val_loss: 4.3012e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.1898e-06 - val_loss: 4.2815e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1616e-06 - val_loss: 4.2755e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.1395e-06 - val_loss: 4.2739e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.1211e-06 - val_loss: 4.2637e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1088e-06 - val_loss: 4.2687e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.0964e-06 - val_loss: 4.2584e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0841e-06 - val_loss: 4.2557e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0724e-06 - val_loss: 4.2622e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0619e-06 - val_loss: 4.2548e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.2095e-06 - val_loss: 4.2038e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1509e-06 - val_loss: 4.1826e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1252e-06 - val_loss: 4.1706e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.1033e-06 - val_loss: 4.1580e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0867e-06 - val_loss: 4.1739e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0721e-06 - val_loss: 4.1620e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0586e-06 - val_loss: 4.1723e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0463e-06 - val_loss: 4.1607e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0361e-06 - val_loss: 4.1656e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 4.0253e-06 - val_loss: 4.1566e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1664e-06 - val_loss: 4.1569e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.1107e-06 - val_loss: 4.1422e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0822e-06 - val_loss: 4.1363e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0613e-06 - val_loss: 4.1426e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0448e-06 - val_loss: 4.1351e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0302e-06 - val_loss: 4.1313e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0172e-06 - val_loss: 4.1331e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0049e-06 - val_loss: 4.1352e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9937e-06 - val_loss: 4.1290e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9839e-06 - val_loss: 4.1251e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.1751e-06 - val_loss: 4.1386e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.1191e-06 - val_loss: 4.1264e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0886e-06 - val_loss: 4.1179e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.0705e-06 - val_loss: 4.1161e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.0535e-06 - val_loss: 4.1258e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 4.0405e-06 - val_loss: 4.1153e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.0284e-06 - val_loss: 4.1079e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0150e-06 - val_loss: 4.1119e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0027e-06 - val_loss: 4.1047e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9938e-06 - val_loss: 4.1038e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.1607e-06 - val_loss: 4.1225e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.1051e-06 - val_loss: 4.1076e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0775e-06 - val_loss: 4.0995e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0585e-06 - val_loss: 4.1017e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0438e-06 - val_loss: 4.0982e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0267e-06 - val_loss: 4.0915e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0154e-06 - val_loss: 4.0917e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0019e-06 - val_loss: 4.0857e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9914e-06 - val_loss: 4.0843e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.9838e-06 - val_loss: 4.1000e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0903e-06 - val_loss: 4.0627e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.0316e-06 - val_loss: 4.0572e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 4.0071e-06 - val_loss: 4.0372e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9855e-06 - val_loss: 4.0476e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9677e-06 - val_loss: 4.0285e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9532e-06 - val_loss: 4.0327e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9440e-06 - val_loss: 4.0225e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.9312e-06 - val_loss: 4.0198e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9201e-06 - val_loss: 4.0228e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.9105e-06 - val_loss: 4.0233e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0872e-06 - val_loss: 4.0456e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.0322e-06 - val_loss: 4.0401e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 4.0066e-06 - val_loss: 4.0339e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9842e-06 - val_loss: 4.0333e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9670e-06 - val_loss: 4.0292e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9543e-06 - val_loss: 4.0228e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9418e-06 - val_loss: 4.0189e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9329e-06 - val_loss: 4.0293e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9222e-06 - val_loss: 4.0222e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9111e-06 - val_loss: 4.0239e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 4.0538e-06 - val_loss: 4.0234e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9980e-06 - val_loss: 4.0009e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9700e-06 - val_loss: 3.9978e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9465e-06 - val_loss: 4.0044e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9333e-06 - val_loss: 3.9883e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.9195e-06 - val_loss: 3.9850e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9058e-06 - val_loss: 3.9896e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8944e-06 - val_loss: 3.9849e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8824e-06 - val_loss: 3.9828e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8755e-06 - val_loss: 3.9778e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 3.9886e-06 - val_loss: 4.0356e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 3.9339e-06 - val_loss: 4.0115e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9049e-06 - val_loss: 4.0100e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8870e-06 - val_loss: 4.0144e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8692e-06 - val_loss: 4.0069e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8541e-06 - val_loss: 4.0192e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8436e-06 - val_loss: 3.9928e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8328e-06 - val_loss: 3.9950e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8215e-06 - val_loss: 4.0076e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8134e-06 - val_loss: 4.0000e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.9845e-06 - val_loss: 4.0305e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.9261e-06 - val_loss: 4.0243e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.9012e-06 - val_loss: 4.0065e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8783e-06 - val_loss: 4.0130e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8642e-06 - val_loss: 3.9984e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8503e-06 - val_loss: 3.9904e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8380e-06 - val_loss: 3.9996e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8277e-06 - val_loss: 3.9951e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8166e-06 - val_loss: 3.9876e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8060e-06 - val_loss: 3.9947e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 3.9789e-06 - val_loss: 4.0405e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.9199e-06 - val_loss: 4.0029e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8895e-06 - val_loss: 4.0039e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8711e-06 - val_loss: 3.9951e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8537e-06 - val_loss: 4.0026e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8391e-06 - val_loss: 3.9839e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8267e-06 - val_loss: 3.9973e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8155e-06 - val_loss: 3.9859e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8063e-06 - val_loss: 3.9808e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 3.7972e-06 - val_loss: 3.9786e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9304e-06 - val_loss: 3.9639e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 3.8774e-06 - val_loss: 3.9412e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 3.8473e-06 - val_loss: 3.9411e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 4s 36us/step - loss: 3.8265e-06 - val_loss: 3.9357e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8117e-06 - val_loss: 3.9413e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.7994e-06 - val_loss: 3.9346e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7867e-06 - val_loss: 3.9404e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.7767e-06 - val_loss: 3.9429e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.7652e-06 - val_loss: 3.9319e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.7569e-06 - val_loss: 3.9316e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.9209e-06 - val_loss: 3.9507e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 3.8646e-06 - val_loss: 3.9374e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8370e-06 - val_loss: 3.9381e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8159e-06 - val_loss: 3.9290e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.8014e-06 - val_loss: 3.9297e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.7878e-06 - val_loss: 3.9302e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.7772e-06 - val_loss: 3.9285e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 35us/step - loss: 3.7653e-06 - val_loss: 3.9249e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7560e-06 - val_loss: 3.9136e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7464e-06 - val_loss: 3.9207e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 3.8959e-06 - val_loss: 3.9158e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8418e-06 - val_loss: 3.9084e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8154e-06 - val_loss: 3.9092e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7937e-06 - val_loss: 3.8950e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7804e-06 - val_loss: 3.8992e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7662e-06 - val_loss: 3.8928e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7522e-06 - val_loss: 3.8931e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7433e-06 - val_loss: 3.8963e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7335e-06 - val_loss: 3.8973e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7234e-06 - val_loss: 3.8915e-06\n",
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8911e-06 - val_loss: 3.8913e-06\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 4s 35us/step - loss: 3.8377e-06 - val_loss: 3.8628e-06\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.8092e-06 - val_loss: 3.8597e-06\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7907e-06 - val_loss: 3.8370e-06\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7735e-06 - val_loss: 3.8516e-06\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7603e-06 - val_loss: 3.8562e-06\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7499e-06 - val_loss: 3.8418e-06\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7380e-06 - val_loss: 3.8353e-06\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7278e-06 - val_loss: 3.8471e-06\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 3s 34us/step - loss: 3.7188e-06 - val_loss: 3.8480e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZPArMuuspDr",
        "colab_type": "text"
      },
      "source": [
        "Once again save the model (although it is already saved each epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmTDszW0vIP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/QCD\n",
        "autoencoder.save(ModelName)\n",
        "# autoencoder = keras.models.load_model(ModelName)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvm0FcTlyAhX",
        "colab_type": "code",
        "outputId": "782e4a48-3f09-42b5-b1be-fe91832982fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/QCD\n",
        "!ls -lh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 99M\n",
            "-rw------- 1 root root  52M Nov 29 13:14 Model_40_24_8_24_40_40\n",
            "-rw------- 1 root root  47M Nov 29 13:14 Model_40_24_8_24_40_40.xz\n",
            "drwx------ 3 root root 4.0K Nov 28 14:52 TEST\n",
            "drwx------ 3 root root 4.0K Nov 28 14:52 TRAIN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crdPBw2IHS87",
        "colab_type": "code",
        "outputId": "030da5b7-4a91-4be9-eb3a-aade68b6bcf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/QCD\n",
        "!xz -z9evvfk Model_40_24_8_24_40_40"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/JetImages/QCD\n",
            "xz: Filter chain: --lzma2=dict=64MiB,lc=3,lp=0,pb=2,mode=normal,nice=273,mf=bt4,depth=512\n",
            "xz: 674 MiB of memory is required. The limiter is disabled.\n",
            "xz: Decompression will need 65 MiB of memory.\n",
            "Model_40_24_8_24_40_40 (1/1)\n",
            "  100 %         46.8 MiB / 51.3 MiB = 0.912   1.6 MiB/s       0:32             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXisTEW0sxuz",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate using the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZucFWExcHi69",
        "colab_type": "code",
        "outputId": "aab69ea0-a12a-4049-e2d3-ed30e23e737a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/QCD\n",
        "EvalOnFile(\"./TEST/BoxImages/0.xz\",\"./TEST/BoxImages/0_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/1.xz\",\"./TEST/BoxImages/1_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/2.xz\",\"./TEST/BoxImages/2_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/3.xz\",\"./TEST/BoxImages/3_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/4.xz\",\"./TEST/BoxImages/4_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/5.xz\",\"./TEST/BoxImages/5_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/6.xz\",\"./TEST/BoxImages/6_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/7.xz\",\"./TEST/BoxImages/7_out\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/JetImages/QCD\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW8VLgf6fwY1",
        "colab_type": "code",
        "outputId": "3b627c11-6aed-4ff2-ff05-ab7035ab435d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/TOP\n",
        "EvalOnFile(\"./TEST/BoxImages/0.xz\",\"./TEST/BoxImages/0_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/1.xz\",\"./TEST/BoxImages/1_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/2.xz\",\"./TEST/BoxImages/2_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/3.xz\",\"./TEST/BoxImages/3_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/4.xz\",\"./TEST/BoxImages/4_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/5.xz\",\"./TEST/BoxImages/5_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/6.xz\",\"./TEST/BoxImages/6_out\")\n",
        "EvalOnFile(\"./TEST/BoxImages/7.xz\",\"./TEST/BoxImages/7_out\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/JetImages/TOP\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n",
            "(20000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puitEbhUgmCa",
        "colab_type": "code",
        "outputId": "46bae138-8da7-42de-dba9-57eadb52449b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/\n",
        "!cat TOP/TEST/BoxImages/*_out > TOP_OUT\n",
        "!cat QCD/TEST/BoxImages/*_out > QCD_OUT\n",
        "!ls TOP/TEST/BoxImages/*_out TOP_OUT -lh\n",
        "!ls QCD/TEST/BoxImages/*_out QCD_OUT -lh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/JetImages\n",
            "-rw------- 1 root root 1.3M Nov 30 03:15 TOP_OUT\n",
            "-rw------- 1 root root 157K Nov 30 03:15 TOP/TEST/BoxImages/0_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 TOP/TEST/BoxImages/1_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 TOP/TEST/BoxImages/2_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 TOP/TEST/BoxImages/3_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 TOP/TEST/BoxImages/4_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 TOP/TEST/BoxImages/5_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 TOP/TEST/BoxImages/6_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 TOP/TEST/BoxImages/7_out\n",
            "-rw------- 1 root root 1.3M Nov 30 03:15 QCD_OUT\n",
            "-rw------- 1 root root 157K Nov 30 03:15 QCD/TEST/BoxImages/0_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 QCD/TEST/BoxImages/1_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 QCD/TEST/BoxImages/2_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 QCD/TEST/BoxImages/3_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 QCD/TEST/BoxImages/4_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 QCD/TEST/BoxImages/5_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 QCD/TEST/BoxImages/6_out\n",
            "-rw------- 1 root root 157K Nov 30 03:15 QCD/TEST/BoxImages/7_out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hyQWsactCdj",
        "colab_type": "text"
      },
      "source": [
        "# Plotting the loss and ROC:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMAaGLuojzLt",
        "colab_type": "code",
        "outputId": "15feaa9e-ec48-4a4e-996d-98f1a76dd802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/\n",
        "qcdloss = np.fromfile(\"QCD_OUT\", dtype=float, count=-1, sep='', offset=0)\n",
        "toploss = np.fromfile(\"TOP_OUT\", dtype=float, count=-1, sep='', offset=0)\n",
        "qcdloss=np.sort(qcdloss)\n",
        "toploss=np.sort(toploss)\n",
        "print(qcdloss.shape)\n",
        "print(toploss.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/JetImages\n",
            "(160000,)\n",
            "(160000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNgk_QkCj3gB",
        "colab_type": "code",
        "outputId": "8185c50b-9603-4230-d3cb-1cac948f7d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.hist(toploss,100,(0.0,0.4),density=True,histtype='step')\n",
        "plt.hist(qcdloss,100,(0.0,0.4),density=True,histtype='step')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASQklEQVR4nO3dfaxkdX3H8fdXKCoWF+je2ga43rVB\nEh5stGNrpT4VrFufMNE00mBQMTe1PrRqS0DaumnThLZGSqOJudEtGg0+UGtJrVaqIrER9C6iqyAK\nSNfdonsRvbYVwdVv/7hn4TB7Z+fhnJkzZ+b9SjZ75syZe76cZT7729/vd34nMhNJUvs8rOkCJEmj\nMcAlqaUMcElqKQNcklrKAJekljpykifbunVrLi0tTfKUktR6u3btujszF7r3TzTAl5aWWF1dneQp\nJan1IuK/NttvF4oktZQBLkktZYBLUksZ4JLUUga4JLWUAS5JLWWAS1JLGeCS1FIGuCS11ETvxJwJ\nl50B63sO3b9lEd6we/L1SJpbBviw1vfAjvVD9+/YMvlaJM01A7wuWxYfDHFb45ImoG+AR8RO4PnA\n/sw8vbT/dcBrgJ8CH8vMC8dWZRuUA9vWuKQJGGQQ8wpge3lHRDwLOAf41cw8DXhr/aVJkg6nb4Bn\n5nXAPV27Xw1cmpn3FcfsH0NtkqTDGHUa4eOBp0XEDRHx2Yh4cq8DI2I5IlYjYnVtbW3E00mSuo0a\n4EcCxwNPAf4U+FBExGYHZuZKZnYys7OwcMgDJSRJIxo1wPcCH8kNXwB+BmytryxJUj+jBvhHgWcB\nRMTjgaOAu+sqSpLU3yDTCK8EnglsjYi9wFuAncDOiPgqcD9wfmbmOAuVJD1U3wDPzHN7vHVezbVI\nkobgYlaS1FIGuCS1lAEuSS3lYlaDKC8hu2Wx2VokqWCAD6LXErKS1CADfBxcWlbSBBjg4+DSspIm\nwEFMSWopA1ySWsoAl6SWMsAlqaUMcElqKQNcklrKAJekljLAJamlDHBJaqm+AR4ROyNif/H0ne73\n3hQRGRE+D7OXg7fV79iysSiWJNVkkFvprwDeDry3vDMiTgJ+B9hTf1kzxNvqJY1J3xZ4Zl4H3LPJ\nW5cBFwI+C1OSGjBSH3hEnAPsy8wvD3DsckSsRsTq2traKKeTJG1i6ACPiKOBNwN/McjxmbmSmZ3M\n7CwsLAx7OklSD6O0wH8F2AZ8OSLuBE4EboyIX6qzMEnS4Q29Hnhm7gZ+8eDrIsQ7mXl3jXVJkvoY\nZBrhlcDngVMiYm9EXDD+siRJ/fRtgWfmuX3eX6qtGknSwLwTU5JaygCXpJYywCWppXwq/SQdXBfl\n4Hb5NntJGpIBPkmuiyKpRnahSFJLGeCS1FIGuCS1lAEuSS1lgEtSSxngktRSTiNsinPCJVVkgDfF\nOeGSKjLAe7nsDFgvnte8ZbHZWiRpEwZ4L+t7YMd601VIUk8OYkpSSw3yRJ6dEbE/Ir5a2vd3EfH1\niPhKRPxzRBw73jIlSd0GaYFfAWzv2ncNcHpmPgH4BnBxzXVJkvroG+CZeR1wT9e+T2bmgeLl9Ww8\nmV6SNEF19IG/Evh4rzcjYjkiViNidW1trYbTSZKgYoBHxCXAAeD9vY7JzJXM7GRmZ2FhocrpJEkl\nI08jjIiXA88HzsrMrK0iSdJARgrwiNgOXAg8IzN/VG9JkqRBDDKN8Erg88ApEbE3Ii4A3g4cA1wT\nETdFxDvHXKckqUvfFnhmnrvJ7nePoRZJ0hC8E1OSWsq1UKaBS8tKGoEBPg1cWlbSCOxCkaSWMsAl\nqaUMcElqKQNcklrKAJekljLAJamlDHBJaikDXJJaygCXpJYywCWppQxwSWopA1ySWsrFrKaNKxNK\nGlDfAI+InWw8+3J/Zp5e7Dse+CCwBNwJ/F5mfn98Zc4RVyaUNKBBulCuALZ37bsI+FRmngx8qngt\nSZqgvgGemdcB93TtPgd4T7H9HuBFNdclSepj1EHMx2TmXcX2d4DH9DowIpYjYjUiVtfW1kY8nSSp\nW+VZKJmZQB7m/ZXM7GRmZ2FhoerpJEmFUQP8uxHxywDF7/vrK0mSNIhRA/xq4Pxi+3zgX+opR5I0\nqL4BHhFXAp8HTomIvRFxAXAp8OyI+CZwdvFakjRBfeeBZ+a5Pd46q+ZaJElD8E7MGXLmpZ9m3w/u\nPWT/Ccc+kv+86LcbqEjSOBngM2TfD+7lzkufd8j+pYs+1kA1ksbNAJ9mPdZFKbe0B2ldn3DsIx8I\ncVvj0uwwwKdZj3VRyi3tQVrX5cC2NS7NDgO85bpb15LmhwHecsN2h9idIs0OA3zO2J0izQ4DfI7Z\nGpfazQBvkbr7um2NS+1mgLfIZnO8Jc0vA1yA3SlSGxngZZedAet7Nra3LDZby4TZnSK1jwFetr4H\ndqw3XYUkDaTyE3kkSc2wBd4Se3MrJ26yLoqk+WULvCV+675/2Oje2bH+YD+9pLlWqQUeEW8AXsXG\nQ413A6/IzB/XUZgOXXVQkspGDvCIOAF4PXBqZt4bER8CXgpcUVNtc6/X+t7jVp5SePC10wql6VO1\nD/xI4JER8RPgaOC/q5ekpnWHtdMKpek0coBn5r6IeCuwB7gX+GRmfrK2yjQ1vMlHmk5VulCOA84B\ntgE/AD4cEedl5vu6jlsGlgEWF+fr5phZ4U0+0nSqMgvlbOBbmbmWmT8BPgI8tfugzFzJzE5mdhYW\nFiqcTpJUViXA9wBPiYijIyKAs4Bb6ilLktRPlT7wGyLiKuBG4ADwJWClrsLmlVMHJQ2q0iyUzHwL\n8JaaahHNTR2U1D7eiSlJLeVaKG20ZRFcF0WaewZ4G5UD+2CQS5o7BriG4k090vQwwDUUb+qRpoeD\nmJLUUrbANTK7U6RmGeAamd0pUrPsQpGklrIFPgUq3T7vnHBpbhngU6DS7fNTMifc/nBp8gxw1cL+\ncGny7AOXpJYywCWppQxwSWopA1ySWqpSgEfEsRFxVUR8PSJuiYjfrKswSdLhVZ2Fcjnwicx8SUQc\nBRxdQ01qOacUSpMxcoBHxBbg6cDLATLzfuD+espSmzmlUJqMKl0o24A14B8j4ksR8a6IeFRNdUmS\n+qjShXIk8CTgdcUT6i8HLgL+vHxQRCwDywCLi4sVTqe+pvC2ertTpPGpEuB7gb2ZeUPx+io2Avwh\nMnMFWAHodDpZ4XwzpdL6J71MyW31ZXanSOMzcoBn5nci4tsRcUpm3gqcBdxcX2mzrdL6J5JE9Vko\nrwPeX8xAuQN4RfWSJEmDqBTgmXkT0KmpFknSEFyNUBPjgKZULwN8Vk3hjBQHNKV6GeCzagpnpEiq\nl4tZSVJLzXcL/LIzYH3Pg6+3eKORpPaY7wBf3wM71puuYi45oClVN98BrsY4oClVZ4CrcbbGpdEY\n4GqcrXFpNAb4BI1lAStJc8sAnyAXsOrP7hRpcAb4PJjCuzJ7sTtFGpwBPg+8K1OaSd6JKUktZYBL\nUksZ4JLUUpUDPCKOKJ5K/691FCRJGkwdg5h/BNwCPLqGnyU9wCmF0uFVCvCIOBF4HvDXwBtrqUgq\nOKVQOryqLfC/By4EjqmhFk1Ci+aEl9kalw41coBHxPOB/Zm5KyKeeZjjloFlgMVF19tuXEvnhNsa\nlw5VpQV+JvDCiHgu8Ajg0RHxvsw8r3xQZq4AKwCdTicrnK+VXP9E0riMHOCZeTFwMUDRAv+T7vCW\n659IGh/ngUtSS9WyFkpmXgtcW8fPkvpxQFPa4GJWah0HNKUNBvg8a+mUQkkbDPB51tIphWV2p2ie\nGeBqNbtTNM8McM0MW+OaNwa4Zoatcc0bA1wbHNCUWscA14YZGNAssztF88AA10yyO0XzwAAfAxew\nkjQJBvgYuIDVdLE7RbPKANehygOaB1+3eFDT7hTNKgNch+oO6xkY1DzI1rhmiQGuuWJrXLPE9cAl\nqaXmrwV+2Rmwvmdje4vP6Jxndqeo7eYvwNf3wI71pqtolxm9S9PuFLVdlafSnwS8F3gMkMBKZl5e\nV2GaIjN2l6Y0K6q0wA8Ab8rMGyPiGGBXRFyTmTfXVJs0MXanqI2qPJX+LuCuYvt/IuIW4ARgLgPc\nuy/bze4UtVEtfeARsQQ8Ebhhk/eWgWWAxcXZHTT07svZYWtcbVE5wCPi54F/Av44M3/Y/X5mrgAr\nAJ1OJ6ueTw2b0QHNMlvjaotKAR4RP8dGeL8/Mz9ST0maanM2oGlrXNOsyiyUAN4N3JKZb6uvJGl6\n2BrXNKvSAj8TeBmwOyJuKva9OTP/rXpZaoU56E4pK7fGD762Ra4mVZmF8jkgaqxFbTNn3SndYW2L\nXE2bvzsxpZrYP66mGeAVOPe7ZM66U8D+cTXPAK/Aud8lc9ad0s3WuJpggEs1KAf2mZd+2jDXRBjg\nqt8cdqeUGeaaFANc9SsH9mVnGOYFw1x1M8A1XnPeN17moKfqNh8B7lN4NGUc9FQd5iPAfQrPdJjz\nvvEyu1ZUh/kI8Bo597sC+8Y3ZZhrVAb4kJz7XRP7xjdlmGsYBriaV+5a6d5vyxx4aJiXGezzzQBX\n83qFtN0sD+gV0rbS55sBrullN0tfdrnMNwN8AA5cTgG7WfoyzOePAT4ABy6nwCDdLGVzHuz2n8+H\nqs/E3A5cDhwBvCszL62lqjpUvHnHVndLDBvsvcxw4A/Sfz4sw386ROZoD4qPiCOAbwDPBvYCXwTO\nzcybe32m0+nk6urqSOcb2o4tlW7eWbroY7a650n5L/xeZjjkh1Vu4HQz3OsXEbsys9O9v0oL/NeB\n2zLzjuIEHwDOAXoG+LSz1T3HBgnmYVv1k1D+S6X7X51j/MvmcAFdpWVfxTz+xVGlBf4SYHtmvqp4\n/TLgNzLztV3HLQPLxctTgFtHrHUrcPeInx0n6xqOdQ3HuoYzrXVBtdoem5kL3TvHPoiZmSvAStWf\nExGrm/0TomnWNRzrGo51DWda64Lx1PawCp/dB5xUen1isU+SNAFVAvyLwMkRsS0ijgJeClxdT1mS\npH5G7kLJzAMR8Vrg39mYRrgzM79WW2WHqtwNMybWNRzrGo51DWda64Ix1DbyIKYkqVlVulAkSQ0y\nwCWppaYiwCNie0TcGhG3RcRFm7z/8Ij4YPH+DRGxVHrv4mL/rRHxnGmoKyKWIuLeiLip+PXOCdf1\n9Ii4MSIOFPP1y++dHxHfLH6dP0V1/bR0vWodDB+grjdGxM0R8ZWI+FREPLb0XpPX63B1NXm9/iAi\ndhfn/lxEnFp6r8nv46Z1Nf19LB334ojIiOiU9lW7XpnZ6C82BkBvBx4HHAV8GTi165g/BN5ZbL8U\n+GCxfWpx/MOBbcXPOWIK6loCvtrg9VoCngC8F3hJaf/xwB3F78cV28c1XVfx3v82eL2eBRxdbL+6\n9OfY9PXatK4puF6PLm2/EPhEsd3097FXXY1+H4vjjgGuA64HOnVdr2logT9wS35m3g8cvCW/7Bzg\nPcX2VcBZERHF/g9k5n2Z+S3gtuLnNV3XOPWtKzPvzMyvAD/r+uxzgGsy857M/D5wDbB9Cuoap0Hq\n+kxm/qh4eT0b9zRA89erV13jNEhdPyy9fBRwcCZEo9/Hw9Q1ToPkBMBfAX8D/Li0r/L1moYAPwH4\ndun13mLfpsdk5gFgHfiFAT/bRF0A2yLiSxHx2Yh4Wk01DVrXOD477p/9iIhYjYjrI+JFNdU0Sl0X\nAB8f8bOTqgsavl4R8ZqIuB34W+D1w3y2gbqgwe9jRDwJOCkzuxeIqXy9XA98PO4CFjPzexHxa8BH\nI+K0rhaCHuqxmbkvIh4HfDoidmfm7ZMsICLOAzrAMyZ53n561NXo9crMdwDviIjfB/4MqHV8YFQ9\n6mrs+xgRDwPeBrx8HD9/Glrgg9yS/8AxEXEksAX43oCfnXhdxT+JvgeQmbvY6Nt6/ATrGsdnx/qz\nM3Nf8fsdwLXAEydZV0ScDVwCvDAz7xvmsw3U1fj1KvkAcPBfAI1fr83qavj7eAxwOnBtRNwJPAW4\nuhjIrH69xtGxP+QgwJFsDA5t48FBgNO6jnkNDx0s/FCxfRoPHQS4g/oGTarUtXCwDjYGN/YBx0+q\nrtKxV3DoIOa32BiQO67Ynoa6jgMeXmxvBb7JJgNBY/xzfCIbX+qTu/Y3er0OU1fT1+vk0vYLgNVi\nu+nvY6+6puL7WBx/LQ8OYla+XpX/A2q6CM9l4+EQtwOXFPv+ko1WB8AjgA+z0cn/BeBxpc9eUnzu\nVuB3p6Eu4MXA14CbgBuBF0y4riez0Z/2f2z8S+Vrpc++sqj3NuAV01AX8FRgd/E/827gggnX9R/A\nd4s/r5uAq6fkem1a1xRcr8tL/39/hlJgNfx93LSupr+PXcdeSxHgdVwvb6WXpJaahj5wSdIIDHBJ\naikDXJJaygCXpJYywCWppQxwSWopA1ySWur/AVLNhIvUS8vZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgKxFF9um_mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dx = (0.4 - 0.0) / 100.0\n",
        "qcdeff = np.ones((100))\n",
        "topeff = np.ones((100))\n",
        "for i in range(100):\n",
        "  xval = i*dx\n",
        "  qcdeff[i]=1.0/(Count(qcdloss,xval)+0.0000000001)\n",
        "  topeff[i]=Count(toploss,xval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lrqSlFMrJQK",
        "colab_type": "code",
        "outputId": "366845d1-7d86-4819-a1ba-22b4a8a6c3a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.yscale('log')\n",
        "plt.plot(topeff,qcdeff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1328f71fd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRPIqMsttXvL",
        "colab_type": "code",
        "outputId": "d36fbf0f-dc2d-4b11-900c-792eb054fd4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/My Drive/JetImages/\n",
        "\n",
        "def ReadLossMass(lossname,massname):\n",
        "  loss = np.fromfile(lossname, dtype=float, count=-1, sep='', offset=0)\n",
        "  mass = np.fromfile(massname, dtype='float32', count=-1, sep='', offset=0)\n",
        "  out = np.ones((mass.shape[0],2))\n",
        "  for i in range(mass.shape[0]):\n",
        "    out[i][0] = loss[i]\n",
        "    out[i][1] = mass[i]\n",
        "  return out\n",
        "\n",
        "def GetQCDPair () :\n",
        "  pair = ReadLossMass(\"QCD/TEST/BoxImages/0_out\",\"QCD/TEST/Mass/0\")\n",
        "  pair = np.append (pair,ReadLossMass(\"QCD/TEST/BoxImages/1_out\",\"QCD/TEST/Mass/1\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"QCD/TEST/BoxImages/2_out\",\"QCD/TEST/Mass/2\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"QCD/TEST/BoxImages/3_out\",\"QCD/TEST/Mass/3\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"QCD/TEST/BoxImages/4_out\",\"QCD/TEST/Mass/4\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"QCD/TEST/BoxImages/5_out\",\"QCD/TEST/Mass/5\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"QCD/TEST/BoxImages/6_out\",\"QCD/TEST/Mass/6\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"QCD/TEST/BoxImages/7_out\",\"QCD/TEST/Mass/7\"),0)\n",
        "  return pair\n",
        "\n",
        "def GetTOPPair () :\n",
        "  pair = ReadLossMass(\"TOP/TEST/BoxImages/0_out\",\"TOP/TEST/Mass/0\")\n",
        "  pair = np.append (pair,ReadLossMass(\"TOP/TEST/BoxImages/1_out\",\"TOP/TEST/Mass/1\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"TOP/TEST/BoxImages/2_out\",\"TOP/TEST/Mass/2\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"TOP/TEST/BoxImages/3_out\",\"TOP/TEST/Mass/3\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"TOP/TEST/BoxImages/4_out\",\"TOP/TEST/Mass/4\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"TOP/TEST/BoxImages/5_out\",\"TOP/TEST/Mass/5\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"TOP/TEST/BoxImages/6_out\",\"TOP/TEST/Mass/6\"),0)\n",
        "  pair = np.append (pair,ReadLossMass(\"TOP/TEST/BoxImages/7_out\",\"TOP/TEST/Mass/7\"),0)\n",
        "  return pair\n",
        "\n",
        "qcdpair = GetQCDPair()\n",
        "toppair = GetTOPPair()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/JetImages\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t0syCF1r6Fc",
        "colab_type": "text"
      },
      "source": [
        "# The 2D Histogram of QCD Loss vs Mass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-z5t2Moo63K",
        "colab_type": "code",
        "outputId": "55083e79-0e4e-4d0e-f619-0e57dc08c82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "#plt.hist(qcdpair[:,1],100,(0.0,300.0),density=True,histtype='step')\n",
        "#plt.hist(toppair[:,1],100,(0.0,300.0),density=True,histtype='step')\n",
        "plt.hist2d(qcdpair[:,1],qcdpair[:,0],bins=100,range=[[0,400],[0.0,0.3]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JpOltxcqrQH",
        "colab_type": "code",
        "outputId": "6c2d52f2-a731-4ed5-a977-4fb1798cc0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "def QCDMassBin(minmass,maxmass):\n",
        "  ret = np.ones((1))\n",
        "  for e in range(qcdpair.shape[0]):\n",
        "    if (minmass < qcdpair[e][1]) and (qcdpair[e][1] < maxmass) :\n",
        "      if e == 0 :\n",
        "        ret[e] = qcdpair[e][0]\n",
        "      else:\n",
        "        ret = np.append(ret,qcdpair[e][0])\n",
        "  return ret\n",
        "\n",
        "plt.hist(QCDMassBin(0,100),100,(0.0,0.4),density=True,histtype='step')\n",
        "plt.hist(QCDMassBin(100,200),100,(0.0,0.4),density=True,histtype='step')\n",
        "plt.hist(QCDMassBin(200,300),100,(0.0,0.4),density=True,histtype='step')\n",
        "plt.hist(QCDMassBin(300,400),100,(0.0,0.4),density=True,histtype='step')\n",
        "plt.hist(QCDMassBin(400,500),100,(0.0,0.4),density=True,histtype='step')\n",
        "plt.hist(QCDMassBin(500,5000),100,(0.0,0.4),density=True,histtype='step')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neCIYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll3RxBUttVXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}