{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kaSmxiM1pOy"
   },
   "source": [
    "# Install important packages for boosted decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "3YcJ6BqmdpF4",
    "outputId": "c3f3636a-9764-4831-9153-757d94448d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.18.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (0.14.1)\n",
      "Collecting shap\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/77/b504e43e21a2ba543a1ac4696718beb500cfa708af2fb57cb54ce299045c/shap-0.35.0.tar.gz (273kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 6.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.18.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.0.3)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.41.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (0.14.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->shap) (1.12.0)\n",
      "Building wheels for collected packages: shap\n",
      "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for shap: filename=shap-0.35.0-cp36-cp36m-linux_x86_64.whl size=394115 sha256=d457e47cb9b879ed3b83bd1f2b598f729413fab7b6e716fa13415eda482398a1\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f7/0f/b57055080cf8894906b3bd3616d2fc2bfd0b12d5161bcb24ac\n",
      "Successfully built shap\n",
      "Installing collected packages: shap\n",
      "Successfully installed shap-0.35.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbHkhREt1xDY"
   },
   "source": [
    "# Importing packages and defining functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ZRI91XeseG37",
    "outputId": "9ee4e0e3-212a-4c81-8207-cb7a7fd2469e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ],
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import lzma\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb#t\n",
    "import shap\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "# from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "#from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def READ_XZ (filename):\n",
    "    file = lzma.LZMAFile(filename)\n",
    "    type_bytes = file.read(-1)\n",
    "    type_array = np.frombuffer(type_bytes,dtype='float32')                                                \n",
    "    return type_array\n",
    "\n",
    "def Count(array,val):\n",
    "  count = 0.0\n",
    "  for e in range(array.shape[0]):\n",
    "    if array[e]>val :\n",
    "      count=count+1.0\n",
    "  return count / array.shape[0]\n",
    "\n",
    "width=40\n",
    "batch_size=200\n",
    "ModelName = \"Model_40_24_8_24_40_40\"\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIuHYzA114v7"
   },
   "source": [
    "# Defining autoencoder model, Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Jxe-btKsedGC",
    "outputId": "caa04795-9bbe-423e-9eb5-8674d8548314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# this is our input placeholder\n",
    "input_img = Input(shape=(width*width,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "Layer1 = Dense(24*24, activation='relu')(input_img)\n",
    "Layer2 = Dense(8*8, activation='relu')(Layer1)\n",
    "Layer3 = Dense(24*24, activation='relu')(Layer2)\n",
    "Layer4 = Dense(40*40, activation='relu')(Layer3)\n",
    "Out = Dense(40*40, activation='softmax')(Layer4)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, Out)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "def NAME(eventtype,purpose,i,obs) :\n",
    "  return \"./\"+eventtype+\"/\"+purpose+\"/\"+obs+\".\"+str(i)+\".bin.xz\"\n",
    "#\n",
    "\n",
    "def EvalOnFile (InFileName,OutFileName):\n",
    "  data = READ_XZ (InFileName)\n",
    "  x_train = data.reshape(-1,width*width)\n",
    "  x_out = autoencoder.predict(x_train,200,use_multiprocessing=True)\n",
    "  diff = x_train - x_out\n",
    "  lrnorm = np.ones((diff.shape[0]))\n",
    "  for e in range(diff.shape[0]):\n",
    "    lrnorm[e] = np.linalg.norm(diff[e])\n",
    "  lrnorm.tofile(OutFileName)\n",
    "  print(lrnorm.shape)\n",
    "BATCH_SIZE=512\n",
    "def TrainOnFile (filename,testfilename,totalepochs):\n",
    "  data = READ_XZ (filename)\n",
    "  x_train = data.reshape(-1,width*width)\n",
    "  datatest = READ_XZ (testfilename)\n",
    "  x_test = datatest.reshape(-1,width*width)\n",
    "  autoencoder.fit(\n",
    "      x_train, x_train, epochs=totalepochs,\n",
    "      batch_size=BATCH_SIZE, shuffle=True,\n",
    "      validation_data=(x_test, x_test)\n",
    "  )\n",
    "  autoencoder.save(ModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ra1jGrHP2AO0"
   },
   "source": [
    "# Mount google drive to access data\n",
    "\n",
    "Get the data from the link [https://drive.google.com/drive/folders/1_voPoiETqfWmCmBeUCKiF5oXqzbr-ZWt?usp=sharing] (this link is public and anyone can download the image data and trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "y2h5ZJ3BekCa",
    "outputId": "7ec7aa65-035f-42e7-e1bf-7d053913d1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n",
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gsjr6xhv2DT0"
   },
   "source": [
    "# Check the files exist and make a copy of the autoencoder model for backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "nPhJv6EcenS6",
    "outputId": "eaa882d9-7907-45fc-f4bd-1d70fe97f14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/S2\n",
      "Model_40_24_8_24_40_40\tQCD  TOP  topeff_loss\n",
      " anime\t\t    DATA_SCIENCE_PROBLEM\t myget\n",
      "'Colab Notebooks'   Model_40_24_8_24_40_40.bak\t programs.squashfs-xz\n",
      " DABBA_FOLDER\t    myencoder\t\t\t S2\n"
     ]
    }
   ],
   "source": [
    "%cd /gdrive/My Drive/S2\n",
    "!ls\n",
    "!cp ./Model_40_24_8_24_40_40 ../Model_40_24_8_24_40_40.bak\n",
    "!ls ../\n",
    "# !tar -xf S2.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "92Q6zEKj2JsN"
   },
   "source": [
    "# CD to the main data directory and load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "sLZXAjXDk3rW",
    "outputId": "d63e3d96-3af0-477d-ce86-685f046683e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/S2\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd /gdrive/My Drive/S2\n",
    "autoencoder = keras.models.load_model(ModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6HQKvp32N-H"
   },
   "source": [
    "# Train another round if required\n",
    "\n",
    "(You are strongly advised not to do this but to just use the trained model...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7hQM2KA-eu5B",
    "outputId": "1f6c676d-144d-46a1-e576-967ed319a318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/S2\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 36us/step - loss: 5.3439e-06 - val_loss: 6.0588e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.2064e-06 - val_loss: 6.0297e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1515e-06 - val_loss: 6.0431e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.1172e-06 - val_loss: 6.0204e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0928e-06 - val_loss: 6.0374e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0727e-06 - val_loss: 6.0436e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0526e-06 - val_loss: 6.0485e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0372e-06 - val_loss: 6.0457e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0232e-06 - val_loss: 6.0580e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0119e-06 - val_loss: 6.0527e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 35us/step - loss: 5.3838e-06 - val_loss: 6.0070e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.2599e-06 - val_loss: 6.0151e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.2102e-06 - val_loss: 6.0279e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.1776e-06 - val_loss: 6.0142e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.1502e-06 - val_loss: 6.0279e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.1319e-06 - val_loss: 6.0312e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.1132e-06 - val_loss: 6.0434e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0989e-06 - val_loss: 6.0430e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0880e-06 - val_loss: 6.0486e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0737e-06 - val_loss: 6.0608e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.3305e-06 - val_loss: 5.7355e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.2022e-06 - val_loss: 5.7310e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.1483e-06 - val_loss: 5.7203e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.1134e-06 - val_loss: 5.7257e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0860e-06 - val_loss: 5.7487e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0670e-06 - val_loss: 5.7181e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0484e-06 - val_loss: 5.7143e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0341e-06 - val_loss: 5.7335e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0207e-06 - val_loss: 5.7330e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0074e-06 - val_loss: 5.7636e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 5.3311e-06 - val_loss: 6.0435e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.2039e-06 - val_loss: 6.0338e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 5.1493e-06 - val_loss: 6.0127e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.1127e-06 - val_loss: 6.0306e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 5.0894e-06 - val_loss: 6.0351e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0692e-06 - val_loss: 6.0563e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0490e-06 - val_loss: 6.0330e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 5.0367e-06 - val_loss: 6.0509e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0234e-06 - val_loss: 6.0530e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0101e-06 - val_loss: 6.0522e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 5.3539e-06 - val_loss: 6.0291e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.2280e-06 - val_loss: 6.0271e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.1729e-06 - val_loss: 6.0177e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 5.1402e-06 - val_loss: 6.0307e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.1142e-06 - val_loss: 6.0241e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0913e-06 - val_loss: 6.0453e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 5.0763e-06 - val_loss: 6.0177e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0614e-06 - val_loss: 6.0403e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0473e-06 - val_loss: 6.0415e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0352e-06 - val_loss: 6.0519e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.3202e-06 - val_loss: 5.7461e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.1970e-06 - val_loss: 5.7180e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.1388e-06 - val_loss: 5.7216e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.1073e-06 - val_loss: 5.7122e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0805e-06 - val_loss: 5.7253e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0620e-06 - val_loss: 5.7211e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0436e-06 - val_loss: 5.7398e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0274e-06 - val_loss: 5.7218e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0170e-06 - val_loss: 5.7392e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0016e-06 - val_loss: 5.7603e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 238us/step - loss: 5.3745e-06 - val_loss: 6.0544e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 218us/step - loss: 5.1871e-06 - val_loss: 6.0871e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 222us/step - loss: 5.0819e-06 - val_loss: 6.0935e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 215us/step - loss: 5.0085e-06 - val_loss: 6.0962e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.9547e-06 - val_loss: 6.1078e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.9141e-06 - val_loss: 6.1215e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.8801e-06 - val_loss: 6.1267e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.8540e-06 - val_loss: 6.1304e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.8325e-06 - val_loss: 6.1465e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.8121e-06 - val_loss: 6.1511e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 5.4020e-06 - val_loss: 6.0333e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.2583e-06 - val_loss: 6.0245e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.2008e-06 - val_loss: 6.0276e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.1650e-06 - val_loss: 6.0280e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.1388e-06 - val_loss: 6.0405e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.1204e-06 - val_loss: 6.0290e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 5.1008e-06 - val_loss: 6.0568e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0846e-06 - val_loss: 6.0510e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0719e-06 - val_loss: 6.0596e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0574e-06 - val_loss: 6.0620e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 35us/step - loss: 5.3417e-06 - val_loss: 6.0155e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.2210e-06 - val_loss: 6.0244e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1671e-06 - val_loss: 6.0094e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1275e-06 - val_loss: 6.0150e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1049e-06 - val_loss: 6.0116e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0834e-06 - val_loss: 6.0175e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0650e-06 - val_loss: 6.0258e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0489e-06 - val_loss: 6.0586e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0335e-06 - val_loss: 6.0364e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0244e-06 - val_loss: 6.0483e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 5.3350e-06 - val_loss: 5.7800e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 47us/step - loss: 5.1759e-06 - val_loss: 5.8308e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 5.0695e-06 - val_loss: 5.8324e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.9897e-06 - val_loss: 5.8112e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.9282e-06 - val_loss: 5.8279e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.8734e-06 - val_loss: 5.8559e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.8240e-06 - val_loss: 5.8410e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.7847e-06 - val_loss: 5.8674e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 48us/step - loss: 4.7490e-06 - val_loss: 5.8792e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.7174e-06 - val_loss: 5.8837e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 35us/step - loss: 5.2912e-06 - val_loss: 5.9985e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1550e-06 - val_loss: 6.0145e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1009e-06 - val_loss: 5.9916e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 5.0621e-06 - val_loss: 5.9977e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0381e-06 - val_loss: 6.0008e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0186e-06 - val_loss: 6.0064e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0013e-06 - val_loss: 6.0141e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9853e-06 - val_loss: 6.0222e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9715e-06 - val_loss: 6.0132e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9604e-06 - val_loss: 6.0377e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.3349e-06 - val_loss: 5.9985e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.2155e-06 - val_loss: 5.9809e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1572e-06 - val_loss: 5.9907e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1263e-06 - val_loss: 5.9936e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 5.1021e-06 - val_loss: 6.0007e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0833e-06 - val_loss: 6.0025e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0649e-06 - val_loss: 6.0048e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0465e-06 - val_loss: 6.0145e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0335e-06 - val_loss: 6.0495e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0239e-06 - val_loss: 6.0341e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.2830e-06 - val_loss: 5.6890e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.1533e-06 - val_loss: 5.7093e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.1000e-06 - val_loss: 5.6916e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0631e-06 - val_loss: 5.6932e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0367e-06 - val_loss: 5.6895e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0175e-06 - val_loss: 5.6925e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9988e-06 - val_loss: 5.6940e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9830e-06 - val_loss: 5.7154e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9710e-06 - val_loss: 5.7118e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9604e-06 - val_loss: 5.7028e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 5.2794e-06 - val_loss: 5.9946e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.1572e-06 - val_loss: 5.9872e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.1002e-06 - val_loss: 5.9789e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 5.0647e-06 - val_loss: 5.9965e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0410e-06 - val_loss: 5.9983e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0168e-06 - val_loss: 5.9909e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0015e-06 - val_loss: 5.9999e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9841e-06 - val_loss: 6.0177e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9734e-06 - val_loss: 6.0260e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9621e-06 - val_loss: 6.0172e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 5.3135e-06 - val_loss: 5.9873e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.1836e-06 - val_loss: 5.9904e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.1276e-06 - val_loss: 5.9786e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0907e-06 - val_loss: 5.9962e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0670e-06 - val_loss: 5.9915e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0475e-06 - val_loss: 6.0108e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0311e-06 - val_loss: 6.0070e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0136e-06 - val_loss: 6.0215e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0018e-06 - val_loss: 6.0180e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.9886e-06 - val_loss: 6.0293e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.2741e-06 - val_loss: 5.6914e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.1450e-06 - val_loss: 5.7152e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0939e-06 - val_loss: 5.6949e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0573e-06 - val_loss: 5.6921e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0360e-06 - val_loss: 5.6904e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0132e-06 - val_loss: 5.7099e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9959e-06 - val_loss: 5.7285e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9804e-06 - val_loss: 5.7169e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9661e-06 - val_loss: 5.7003e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9553e-06 - val_loss: 5.7305e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 236us/step - loss: 5.3467e-06 - val_loss: 6.0302e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 220us/step - loss: 5.1398e-06 - val_loss: 6.0506e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 210us/step - loss: 5.0293e-06 - val_loss: 6.0530e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.9574e-06 - val_loss: 6.0704e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 206us/step - loss: 4.9018e-06 - val_loss: 6.0824e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 223us/step - loss: 4.8563e-06 - val_loss: 6.0805e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 207us/step - loss: 4.8237e-06 - val_loss: 6.0948e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.7941e-06 - val_loss: 6.1216e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 206us/step - loss: 4.7733e-06 - val_loss: 6.1321e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.7558e-06 - val_loss: 6.1370e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 5.3614e-06 - val_loss: 5.9940e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.2054e-06 - val_loss: 5.9960e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.1516e-06 - val_loss: 5.9773e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.1172e-06 - val_loss: 5.9863e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0915e-06 - val_loss: 5.9947e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0689e-06 - val_loss: 5.9923e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0525e-06 - val_loss: 6.0006e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0383e-06 - val_loss: 6.0051e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 5.0231e-06 - val_loss: 6.0387e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0109e-06 - val_loss: 6.0365e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.2927e-06 - val_loss: 6.0263e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1703e-06 - val_loss: 5.9964e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 5.1170e-06 - val_loss: 5.9857e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 5.0821e-06 - val_loss: 6.0238e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0537e-06 - val_loss: 5.9909e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0351e-06 - val_loss: 6.0026e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0174e-06 - val_loss: 6.0025e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0035e-06 - val_loss: 6.0043e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9883e-06 - val_loss: 5.9973e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9766e-06 - val_loss: 6.0273e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 5.2780e-06 - val_loss: 5.7550e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 5.1350e-06 - val_loss: 5.7810e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 5.0275e-06 - val_loss: 5.7996e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 4.9431e-06 - val_loss: 5.7996e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.8779e-06 - val_loss: 5.7870e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.8221e-06 - val_loss: 5.8040e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.7774e-06 - val_loss: 5.7987e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.7401e-06 - val_loss: 5.8276e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.7040e-06 - val_loss: 5.8378e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 49us/step - loss: 4.6707e-06 - val_loss: 5.8445e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.2447e-06 - val_loss: 5.9901e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 5.1104e-06 - val_loss: 5.9634e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0516e-06 - val_loss: 5.9745e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0177e-06 - val_loss: 5.9894e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9926e-06 - val_loss: 5.9855e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9702e-06 - val_loss: 5.9759e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9544e-06 - val_loss: 5.9991e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9406e-06 - val_loss: 5.9808e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9254e-06 - val_loss: 5.9895e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9107e-06 - val_loss: 5.9963e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.2914e-06 - val_loss: 5.9949e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1699e-06 - val_loss: 5.9904e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 5.1145e-06 - val_loss: 5.9642e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0792e-06 - val_loss: 5.9593e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0554e-06 - val_loss: 5.9792e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0348e-06 - val_loss: 5.9765e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0188e-06 - val_loss: 6.0005e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0029e-06 - val_loss: 6.0010e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9891e-06 - val_loss: 5.9885e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9775e-06 - val_loss: 5.9909e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.2307e-06 - val_loss: 5.7151e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.1061e-06 - val_loss: 5.6822e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0526e-06 - val_loss: 5.6746e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0198e-06 - val_loss: 5.6759e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9931e-06 - val_loss: 5.6821e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9737e-06 - val_loss: 5.6756e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9587e-06 - val_loss: 5.6694e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9432e-06 - val_loss: 5.6819e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9286e-06 - val_loss: 5.6820e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9177e-06 - val_loss: 5.6849e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 5.2312e-06 - val_loss: 5.9927e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.1054e-06 - val_loss: 5.9598e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0538e-06 - val_loss: 5.9621e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0203e-06 - val_loss: 5.9735e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.9937e-06 - val_loss: 5.9704e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9705e-06 - val_loss: 5.9725e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.9563e-06 - val_loss: 5.9863e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9393e-06 - val_loss: 5.9888e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9269e-06 - val_loss: 5.9887e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.9162e-06 - val_loss: 6.0065e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 5.2625e-06 - val_loss: 5.9633e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.1400e-06 - val_loss: 5.9749e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 5.0826e-06 - val_loss: 5.9726e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 5.0492e-06 - val_loss: 5.9909e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0225e-06 - val_loss: 5.9701e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0053e-06 - val_loss: 5.9952e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9854e-06 - val_loss: 5.9835e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9711e-06 - val_loss: 5.9820e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.9578e-06 - val_loss: 5.9819e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9445e-06 - val_loss: 5.9944e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.2308e-06 - val_loss: 5.7112e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.1034e-06 - val_loss: 5.6693e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0464e-06 - val_loss: 5.6789e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0123e-06 - val_loss: 5.6799e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9874e-06 - val_loss: 5.6757e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9648e-06 - val_loss: 5.6806e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9483e-06 - val_loss: 5.6772e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9341e-06 - val_loss: 5.6745e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9214e-06 - val_loss: 5.6940e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9092e-06 - val_loss: 5.6626e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 242us/step - loss: 5.2829e-06 - val_loss: 5.9952e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 218us/step - loss: 5.0869e-06 - val_loss: 6.0121e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.9817e-06 - val_loss: 6.0333e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.9055e-06 - val_loss: 6.0520e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.8517e-06 - val_loss: 6.0670e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 217us/step - loss: 4.8113e-06 - val_loss: 6.0700e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.7789e-06 - val_loss: 6.0755e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 201us/step - loss: 4.7518e-06 - val_loss: 6.0804e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.7295e-06 - val_loss: 6.0965e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 213us/step - loss: 4.7087e-06 - val_loss: 6.1117e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 5.3078e-06 - val_loss: 5.9654e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.1600e-06 - val_loss: 5.9648e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.1035e-06 - val_loss: 5.9630e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0642e-06 - val_loss: 5.9873e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 5.0405e-06 - val_loss: 5.9745e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0217e-06 - val_loss: 5.9863e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0008e-06 - val_loss: 5.9871e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9864e-06 - val_loss: 5.9883e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9714e-06 - val_loss: 5.9900e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9596e-06 - val_loss: 6.0144e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.2453e-06 - val_loss: 5.9688e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.1234e-06 - val_loss: 5.9527e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0689e-06 - val_loss: 5.9512e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0349e-06 - val_loss: 5.9655e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0094e-06 - val_loss: 5.9856e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9893e-06 - val_loss: 5.9765e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9692e-06 - val_loss: 5.9820e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9540e-06 - val_loss: 5.9763e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9411e-06 - val_loss: 5.9792e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9278e-06 - val_loss: 5.9866e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 5.2065e-06 - val_loss: 5.7214e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 5.0635e-06 - val_loss: 5.7600e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.9636e-06 - val_loss: 5.7713e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.8789e-06 - val_loss: 5.7735e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.8186e-06 - val_loss: 5.7859e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.7672e-06 - val_loss: 5.7922e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.7206e-06 - val_loss: 5.7849e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 38us/step - loss: 4.6824e-06 - val_loss: 5.8018e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.6510e-06 - val_loss: 5.8336e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.6199e-06 - val_loss: 5.8352e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.2051e-06 - val_loss: 5.9475e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 5.0655e-06 - val_loss: 5.9283e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0076e-06 - val_loss: 5.9447e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9739e-06 - val_loss: 5.9365e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9469e-06 - val_loss: 5.9601e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9250e-06 - val_loss: 5.9585e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9092e-06 - val_loss: 5.9449e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8953e-06 - val_loss: 5.9725e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8822e-06 - val_loss: 5.9642e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8701e-06 - val_loss: 5.9643e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.2437e-06 - val_loss: 5.9473e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.1219e-06 - val_loss: 5.9532e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 5.0665e-06 - val_loss: 5.9329e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0336e-06 - val_loss: 5.9368e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0080e-06 - val_loss: 5.9577e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9866e-06 - val_loss: 5.9365e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9684e-06 - val_loss: 5.9559e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9543e-06 - val_loss: 5.9539e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9425e-06 - val_loss: 5.9697e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9284e-06 - val_loss: 5.9837e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.1907e-06 - val_loss: 5.7025e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0646e-06 - val_loss: 5.6366e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0078e-06 - val_loss: 5.6775e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9737e-06 - val_loss: 5.6377e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9455e-06 - val_loss: 5.6573e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9267e-06 - val_loss: 5.6492e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9091e-06 - val_loss: 5.6709e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8948e-06 - val_loss: 5.6804e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8832e-06 - val_loss: 5.6828e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8703e-06 - val_loss: 5.6752e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.1882e-06 - val_loss: 5.9788e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 5.0583e-06 - val_loss: 5.9399e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 5.0040e-06 - val_loss: 5.9474e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9700e-06 - val_loss: 5.9612e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9463e-06 - val_loss: 5.9400e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9227e-06 - val_loss: 5.9579e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.9073e-06 - val_loss: 5.9757e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8916e-06 - val_loss: 5.9656e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8787e-06 - val_loss: 5.9610e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.8666e-06 - val_loss: 5.9636e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 4s 35us/step - loss: 5.2197e-06 - val_loss: 5.9366e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0903e-06 - val_loss: 5.9206e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0352e-06 - val_loss: 5.9173e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9972e-06 - val_loss: 5.9404e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.9739e-06 - val_loss: 5.9253e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9524e-06 - val_loss: 5.9435e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9356e-06 - val_loss: 5.9483e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9188e-06 - val_loss: 5.9531e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9062e-06 - val_loss: 5.9712e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8939e-06 - val_loss: 5.9698e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.1799e-06 - val_loss: 5.7073e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 5.0515e-06 - val_loss: 5.6810e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9976e-06 - val_loss: 5.6697e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9600e-06 - val_loss: 5.6716e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9340e-06 - val_loss: 5.6604e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9111e-06 - val_loss: 5.6634e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8957e-06 - val_loss: 5.6718e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8819e-06 - val_loss: 5.6882e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8668e-06 - val_loss: 5.6631e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8530e-06 - val_loss: 5.6818e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 235us/step - loss: 5.2431e-06 - val_loss: 5.9889e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 215us/step - loss: 5.0375e-06 - val_loss: 5.9938e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.9321e-06 - val_loss: 5.9882e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.8479e-06 - val_loss: 6.0208e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 213us/step - loss: 4.7935e-06 - val_loss: 6.0338e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 222us/step - loss: 4.7512e-06 - val_loss: 6.0388e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.7157e-06 - val_loss: 6.0420e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 201us/step - loss: 4.6925e-06 - val_loss: 6.0550e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 201us/step - loss: 4.6700e-06 - val_loss: 6.0667e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.6503e-06 - val_loss: 6.0822e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 5.2669e-06 - val_loss: 5.9592e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.1079e-06 - val_loss: 5.9241e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0507e-06 - val_loss: 5.9187e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0137e-06 - val_loss: 5.9568e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9857e-06 - val_loss: 5.9442e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9661e-06 - val_loss: 5.9662e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.9490e-06 - val_loss: 5.9565e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.9337e-06 - val_loss: 5.9666e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9200e-06 - val_loss: 5.9672e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9063e-06 - val_loss: 5.9925e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 35us/step - loss: 5.2044e-06 - val_loss: 5.9446e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0738e-06 - val_loss: 5.9256e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0175e-06 - val_loss: 5.9466e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9831e-06 - val_loss: 5.9251e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9572e-06 - val_loss: 5.9520e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9349e-06 - val_loss: 5.9559e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9168e-06 - val_loss: 5.9608e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9019e-06 - val_loss: 5.9610e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8867e-06 - val_loss: 5.9568e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8764e-06 - val_loss: 5.9781e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 57us/step - loss: 5.1842e-06 - val_loss: 5.6993e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 5.0209e-06 - val_loss: 5.7414e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.9201e-06 - val_loss: 5.7619e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.8358e-06 - val_loss: 5.7548e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.7710e-06 - val_loss: 5.7625e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.7213e-06 - val_loss: 5.7607e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.6765e-06 - val_loss: 5.7781e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.6360e-06 - val_loss: 5.8009e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.5990e-06 - val_loss: 5.8119e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.5706e-06 - val_loss: 5.8179e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.1635e-06 - val_loss: 5.9304e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0139e-06 - val_loss: 5.8959e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9550e-06 - val_loss: 5.9159e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9179e-06 - val_loss: 5.9180e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8954e-06 - val_loss: 5.9285e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8718e-06 - val_loss: 5.9514e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8565e-06 - val_loss: 5.9273e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8382e-06 - val_loss: 5.9506e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8255e-06 - val_loss: 5.9333e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8122e-06 - val_loss: 5.9596e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.2067e-06 - val_loss: 5.9495e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0762e-06 - val_loss: 5.9071e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0174e-06 - val_loss: 5.9043e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9797e-06 - val_loss: 5.9201e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9534e-06 - val_loss: 5.9208e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9327e-06 - val_loss: 5.9195e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9133e-06 - val_loss: 5.9231e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8985e-06 - val_loss: 5.9419e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8846e-06 - val_loss: 5.9412e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8728e-06 - val_loss: 5.9471e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 25us/step - loss: 5.1466e-06 - val_loss: 5.6842e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0130e-06 - val_loss: 5.6653e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9563e-06 - val_loss: 5.6610e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9215e-06 - val_loss: 5.6400e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8963e-06 - val_loss: 5.6383e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8777e-06 - val_loss: 5.6649e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8580e-06 - val_loss: 5.6502e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8405e-06 - val_loss: 5.6665e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8272e-06 - val_loss: 5.6786e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8142e-06 - val_loss: 5.6686e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 5.1476e-06 - val_loss: 5.9086e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 5.0180e-06 - val_loss: 5.9060e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9580e-06 - val_loss: 5.9015e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9226e-06 - val_loss: 5.9248e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8954e-06 - val_loss: 5.9115e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8743e-06 - val_loss: 5.9215e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8583e-06 - val_loss: 5.9198e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.8432e-06 - val_loss: 5.9241e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8283e-06 - val_loss: 5.9327e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8153e-06 - val_loss: 5.9584e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 35us/step - loss: 5.1777e-06 - val_loss: 5.9161e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 5.0352e-06 - val_loss: 5.9052e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9832e-06 - val_loss: 5.8939e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9443e-06 - val_loss: 5.9019e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9200e-06 - val_loss: 5.9102e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8953e-06 - val_loss: 5.9307e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8777e-06 - val_loss: 5.9100e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8649e-06 - val_loss: 5.9292e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8504e-06 - val_loss: 5.9199e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8361e-06 - val_loss: 5.9481e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.1297e-06 - val_loss: 5.6435e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9975e-06 - val_loss: 5.6279e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9410e-06 - val_loss: 5.6243e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9026e-06 - val_loss: 5.6303e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8753e-06 - val_loss: 5.6438e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8573e-06 - val_loss: 5.6432e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8384e-06 - val_loss: 5.6298e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8224e-06 - val_loss: 5.6582e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8072e-06 - val_loss: 5.6450e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7957e-06 - val_loss: 5.6411e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 243us/step - loss: 5.1557e-06 - val_loss: 5.9437e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 223us/step - loss: 4.9713e-06 - val_loss: 5.9731e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 211us/step - loss: 4.8639e-06 - val_loss: 5.9780e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.7866e-06 - val_loss: 6.0007e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.7339e-06 - val_loss: 6.0249e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 222us/step - loss: 4.6892e-06 - val_loss: 6.0248e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.6562e-06 - val_loss: 6.0275e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.6276e-06 - val_loss: 6.0258e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.6032e-06 - val_loss: 6.0449e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.5825e-06 - val_loss: 6.0555e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 5.2130e-06 - val_loss: 5.9565e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 5.0554e-06 - val_loss: 5.9027e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9944e-06 - val_loss: 5.9220e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9553e-06 - val_loss: 5.8970e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9277e-06 - val_loss: 5.9165e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.9052e-06 - val_loss: 5.9398e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.8865e-06 - val_loss: 5.9293e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.8697e-06 - val_loss: 5.9199e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.8537e-06 - val_loss: 5.9416e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.8414e-06 - val_loss: 5.9419e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 5.1619e-06 - val_loss: 5.9093e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0246e-06 - val_loss: 5.9022e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9632e-06 - val_loss: 5.9091e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9276e-06 - val_loss: 5.9068e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8990e-06 - val_loss: 5.9324e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8796e-06 - val_loss: 5.9290e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8604e-06 - val_loss: 5.9145e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8428e-06 - val_loss: 5.9455e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8268e-06 - val_loss: 5.9383e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8140e-06 - val_loss: 5.9237e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 38us/step - loss: 5.1444e-06 - val_loss: 5.6615e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.9893e-06 - val_loss: 5.7157e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.8803e-06 - val_loss: 5.7349e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 48us/step - loss: 4.7952e-06 - val_loss: 5.7118e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.7276e-06 - val_loss: 5.7276e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.6714e-06 - val_loss: 5.7501e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.6238e-06 - val_loss: 5.7559e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.5832e-06 - val_loss: 5.7536e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.5449e-06 - val_loss: 5.7714e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.5140e-06 - val_loss: 5.7780e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 35us/step - loss: 5.1097e-06 - val_loss: 5.8818e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9540e-06 - val_loss: 5.8838e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9028e-06 - val_loss: 5.8668e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8666e-06 - val_loss: 5.8689e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8394e-06 - val_loss: 5.8846e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8161e-06 - val_loss: 5.8880e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7979e-06 - val_loss: 5.9076e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7834e-06 - val_loss: 5.9063e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7692e-06 - val_loss: 5.9003e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7564e-06 - val_loss: 5.9275e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 5.1587e-06 - val_loss: 5.8744e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 5.0132e-06 - val_loss: 5.8844e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9599e-06 - val_loss: 5.8932e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9222e-06 - val_loss: 5.8841e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8962e-06 - val_loss: 5.8987e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8711e-06 - val_loss: 5.9022e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8516e-06 - val_loss: 5.9000e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8398e-06 - val_loss: 5.9126e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8251e-06 - val_loss: 5.8957e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8104e-06 - val_loss: 5.9307e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.1025e-06 - val_loss: 5.6548e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.9595e-06 - val_loss: 5.6568e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9037e-06 - val_loss: 5.6178e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8701e-06 - val_loss: 5.6329e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8441e-06 - val_loss: 5.6183e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8206e-06 - val_loss: 5.6305e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8042e-06 - val_loss: 5.6315e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7861e-06 - val_loss: 5.6417e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7720e-06 - val_loss: 5.6316e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7599e-06 - val_loss: 5.6524e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 35us/step - loss: 5.1033e-06 - val_loss: 5.8759e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.9595e-06 - val_loss: 5.8954e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.9005e-06 - val_loss: 5.8805e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8658e-06 - val_loss: 5.8810e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.8394e-06 - val_loss: 5.8763e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8184e-06 - val_loss: 5.8947e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8028e-06 - val_loss: 5.9037e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7835e-06 - val_loss: 5.8952e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7685e-06 - val_loss: 5.8989e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7583e-06 - val_loss: 5.9134e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 5.1291e-06 - val_loss: 5.9018e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9823e-06 - val_loss: 5.8638e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.9231e-06 - val_loss: 5.8670e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8889e-06 - val_loss: 5.8816e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8627e-06 - val_loss: 5.8752e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 4s 36us/step - loss: 4.8396e-06 - val_loss: 5.8851e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.8239e-06 - val_loss: 5.8834e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.8073e-06 - val_loss: 5.8999e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 35us/step - loss: 4.7916e-06 - val_loss: 5.9234e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 4s 37us/step - loss: 4.7797e-06 - val_loss: 5.9206e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 25us/step - loss: 5.0814e-06 - val_loss: 5.6120e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9499e-06 - val_loss: 5.6053e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8862e-06 - val_loss: 5.6093e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8554e-06 - val_loss: 5.6087e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8258e-06 - val_loss: 5.5924e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8009e-06 - val_loss: 5.6095e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7829e-06 - val_loss: 5.6289e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 25us/step - loss: 4.7701e-06 - val_loss: 5.5923e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 25us/step - loss: 4.7552e-06 - val_loss: 5.6086e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 25us/step - loss: 4.7417e-06 - val_loss: 5.6332e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 234us/step - loss: 5.1110e-06 - val_loss: 5.8856e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 225us/step - loss: 4.9169e-06 - val_loss: 5.9269e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 237us/step - loss: 4.8087e-06 - val_loss: 5.9436e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.7345e-06 - val_loss: 5.9613e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.6801e-06 - val_loss: 5.9735e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.6394e-06 - val_loss: 5.9764e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.6102e-06 - val_loss: 5.9900e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 209us/step - loss: 4.5844e-06 - val_loss: 6.0101e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 208us/step - loss: 4.5608e-06 - val_loss: 6.0148e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 211us/step - loss: 4.5374e-06 - val_loss: 6.0152e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 4s 37us/step - loss: 5.1601e-06 - val_loss: 5.8991e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 35us/step - loss: 5.0012e-06 - val_loss: 5.8778e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.9420e-06 - val_loss: 5.8800e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8992e-06 - val_loss: 5.8623e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8729e-06 - val_loss: 5.8881e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8509e-06 - val_loss: 5.9002e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8315e-06 - val_loss: 5.9038e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8156e-06 - val_loss: 5.9098e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8018e-06 - val_loss: 5.9308e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.7878e-06 - val_loss: 5.9341e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 36us/step - loss: 5.1128e-06 - val_loss: 5.8884e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9691e-06 - val_loss: 5.8590e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9089e-06 - val_loss: 5.8690e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8736e-06 - val_loss: 5.8819e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8474e-06 - val_loss: 5.8654e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8238e-06 - val_loss: 5.8735e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8051e-06 - val_loss: 5.8923e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7893e-06 - val_loss: 5.9065e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7765e-06 - val_loss: 5.9075e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7617e-06 - val_loss: 5.9023e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 5.1284e-06 - val_loss: 5.6929e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 4.9391e-06 - val_loss: 5.7197e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.8358e-06 - val_loss: 5.7340e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.7567e-06 - val_loss: 5.7268e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.6921e-06 - val_loss: 5.7448e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.6308e-06 - val_loss: 5.7691e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.5834e-06 - val_loss: 5.7767e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 49us/step - loss: 4.5403e-06 - val_loss: 5.7517e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.4967e-06 - val_loss: 5.7771e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.4654e-06 - val_loss: 5.7906e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 36us/step - loss: 5.0669e-06 - val_loss: 5.8674e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9087e-06 - val_loss: 5.8612e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8517e-06 - val_loss: 5.8618e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8173e-06 - val_loss: 5.8531e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7882e-06 - val_loss: 5.8606e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7666e-06 - val_loss: 5.8720e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7463e-06 - val_loss: 5.8758e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7321e-06 - val_loss: 5.8653e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7188e-06 - val_loss: 5.8861e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7071e-06 - val_loss: 5.9026e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 36us/step - loss: 5.1098e-06 - val_loss: 5.8705e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9669e-06 - val_loss: 5.8447e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9078e-06 - val_loss: 5.8694e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8717e-06 - val_loss: 5.8403e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8437e-06 - val_loss: 5.8545e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8236e-06 - val_loss: 5.8736e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8024e-06 - val_loss: 5.8466e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7888e-06 - val_loss: 5.8739e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7748e-06 - val_loss: 5.8806e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7624e-06 - val_loss: 5.8823e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 25us/step - loss: 5.0450e-06 - val_loss: 5.6411e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9144e-06 - val_loss: 5.6169e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8542e-06 - val_loss: 5.5766e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8203e-06 - val_loss: 5.6084e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7948e-06 - val_loss: 5.6206e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7733e-06 - val_loss: 5.5808e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7553e-06 - val_loss: 5.6113e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7395e-06 - val_loss: 5.6032e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7240e-06 - val_loss: 5.6228e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7120e-06 - val_loss: 5.6320e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 4s 35us/step - loss: 5.0525e-06 - val_loss: 5.8483e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.9154e-06 - val_loss: 5.8494e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.8538e-06 - val_loss: 5.8602e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.8169e-06 - val_loss: 5.8318e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7929e-06 - val_loss: 5.8496e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7714e-06 - val_loss: 5.8621e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7535e-06 - val_loss: 5.8748e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7381e-06 - val_loss: 5.8713e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7254e-06 - val_loss: 5.8966e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7129e-06 - val_loss: 5.8816e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 4s 36us/step - loss: 5.0754e-06 - val_loss: 5.8494e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.9368e-06 - val_loss: 5.8507e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.8766e-06 - val_loss: 5.8478e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.8423e-06 - val_loss: 5.8460e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.8125e-06 - val_loss: 5.8645e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7904e-06 - val_loss: 5.8591e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7720e-06 - val_loss: 5.8673e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.7555e-06 - val_loss: 5.8768e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.7406e-06 - val_loss: 5.8711e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.7283e-06 - val_loss: 5.8746e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 25us/step - loss: 5.0399e-06 - val_loss: 5.6273e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9001e-06 - val_loss: 5.6419e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 25us/step - loss: 4.8390e-06 - val_loss: 5.5959e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8008e-06 - val_loss: 5.5616e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7749e-06 - val_loss: 5.6069e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7505e-06 - val_loss: 5.5954e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7326e-06 - val_loss: 5.5919e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7186e-06 - val_loss: 5.5954e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 25us/step - loss: 4.7033e-06 - val_loss: 5.5952e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6903e-06 - val_loss: 5.6114e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 245us/step - loss: 5.0602e-06 - val_loss: 5.8815e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 220us/step - loss: 4.8787e-06 - val_loss: 5.9015e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 211us/step - loss: 4.7643e-06 - val_loss: 5.9101e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 210us/step - loss: 4.6865e-06 - val_loss: 5.9253e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 209us/step - loss: 4.6288e-06 - val_loss: 5.9387e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 224us/step - loss: 4.5831e-06 - val_loss: 5.9532e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 209us/step - loss: 4.5482e-06 - val_loss: 5.9573e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 206us/step - loss: 4.5212e-06 - val_loss: 5.9776e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 207us/step - loss: 4.4984e-06 - val_loss: 5.9863e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.4765e-06 - val_loss: 5.9850e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 4s 35us/step - loss: 5.1032e-06 - val_loss: 5.8685e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.9459e-06 - val_loss: 5.8642e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8811e-06 - val_loss: 5.8504e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8452e-06 - val_loss: 5.8438e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8154e-06 - val_loss: 5.8689e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.7954e-06 - val_loss: 5.8493e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.7762e-06 - val_loss: 5.8608e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.7617e-06 - val_loss: 5.8683e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.7462e-06 - val_loss: 5.8858e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.7345e-06 - val_loss: 5.9099e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 5.0635e-06 - val_loss: 5.8594e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9185e-06 - val_loss: 5.8297e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8625e-06 - val_loss: 5.8438e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8245e-06 - val_loss: 5.8600e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7957e-06 - val_loss: 5.8645e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7726e-06 - val_loss: 5.8548e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7548e-06 - val_loss: 5.8687e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7370e-06 - val_loss: 5.8670e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7229e-06 - val_loss: 5.8762e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7125e-06 - val_loss: 5.8768e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 5.0626e-06 - val_loss: 5.6826e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.8916e-06 - val_loss: 5.7617e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.8129e-06 - val_loss: 5.7587e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.7129e-06 - val_loss: 5.7487e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.6575e-06 - val_loss: 5.7348e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.5808e-06 - val_loss: 5.7573e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 49us/step - loss: 4.5313e-06 - val_loss: 5.7527e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.4841e-06 - val_loss: 5.7535e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.4478e-06 - val_loss: 5.7784e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.4144e-06 - val_loss: 5.7874e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 5.0275e-06 - val_loss: 5.8278e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8670e-06 - val_loss: 5.8308e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8073e-06 - val_loss: 5.8199e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7694e-06 - val_loss: 5.8233e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7437e-06 - val_loss: 5.8301e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7216e-06 - val_loss: 5.8259e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7020e-06 - val_loss: 5.8437e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6849e-06 - val_loss: 5.8402e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6722e-06 - val_loss: 5.8534e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6600e-06 - val_loss: 5.8689e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 35us/step - loss: 5.0609e-06 - val_loss: 5.8287e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.9201e-06 - val_loss: 5.8257e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8601e-06 - val_loss: 5.8342e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8206e-06 - val_loss: 5.8618e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7978e-06 - val_loss: 5.8539e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7736e-06 - val_loss: 5.8337e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7563e-06 - val_loss: 5.8446e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7429e-06 - val_loss: 5.8512e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7259e-06 - val_loss: 5.8553e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7136e-06 - val_loss: 5.8424e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 5.0058e-06 - val_loss: 5.6319e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8683e-06 - val_loss: 5.5724e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8132e-06 - val_loss: 5.6035e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7754e-06 - val_loss: 5.6055e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7496e-06 - val_loss: 5.5872e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7281e-06 - val_loss: 5.5834e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7104e-06 - val_loss: 5.6028e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6944e-06 - val_loss: 5.6019e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6800e-06 - val_loss: 5.6365e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6714e-06 - val_loss: 5.6316e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 5.0049e-06 - val_loss: 5.8309e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8668e-06 - val_loss: 5.8050e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.8105e-06 - val_loss: 5.8229e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7738e-06 - val_loss: 5.8284e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7484e-06 - val_loss: 5.8278e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7247e-06 - val_loss: 5.8460e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7081e-06 - val_loss: 5.8407e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6950e-06 - val_loss: 5.8522e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6778e-06 - val_loss: 5.8329e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6684e-06 - val_loss: 5.8679e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 35us/step - loss: 5.0282e-06 - val_loss: 5.8261e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.8887e-06 - val_loss: 5.8270e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8300e-06 - val_loss: 5.8187e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7926e-06 - val_loss: 5.8256e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7667e-06 - val_loss: 5.8354e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7427e-06 - val_loss: 5.8539e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7235e-06 - val_loss: 5.8376e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7075e-06 - val_loss: 5.8494e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6937e-06 - val_loss: 5.8495e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6819e-06 - val_loss: 5.8675e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9845e-06 - val_loss: 5.6168e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8474e-06 - val_loss: 5.5963e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7883e-06 - val_loss: 5.5703e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7524e-06 - val_loss: 5.6106e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7246e-06 - val_loss: 5.5855e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7042e-06 - val_loss: 5.5826e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6828e-06 - val_loss: 5.5647e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6697e-06 - val_loss: 5.5848e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6564e-06 - val_loss: 5.6172e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6439e-06 - val_loss: 5.5857e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 240us/step - loss: 5.0021e-06 - val_loss: 5.8609e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 222us/step - loss: 4.8223e-06 - val_loss: 5.8830e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 211us/step - loss: 4.7032e-06 - val_loss: 5.8979e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 209us/step - loss: 4.6309e-06 - val_loss: 5.9243e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 211us/step - loss: 4.5748e-06 - val_loss: 5.9190e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 227us/step - loss: 4.5311e-06 - val_loss: 5.9282e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 208us/step - loss: 4.4956e-06 - val_loss: 5.9413e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 210us/step - loss: 4.4692e-06 - val_loss: 5.9480e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.4468e-06 - val_loss: 5.9607e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.4262e-06 - val_loss: 5.9629e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 5.0537e-06 - val_loss: 5.8370e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8921e-06 - val_loss: 5.8042e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.8326e-06 - val_loss: 5.8234e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7960e-06 - val_loss: 5.8267e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7657e-06 - val_loss: 5.8344e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.7431e-06 - val_loss: 5.8271e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7254e-06 - val_loss: 5.8493e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7100e-06 - val_loss: 5.8574e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6949e-06 - val_loss: 5.8490e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6818e-06 - val_loss: 5.8600e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0120e-06 - val_loss: 5.8569e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8739e-06 - val_loss: 5.8187e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8086e-06 - val_loss: 5.8101e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7735e-06 - val_loss: 5.8287e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7438e-06 - val_loss: 5.8288e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7223e-06 - val_loss: 5.8412e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7051e-06 - val_loss: 5.8318e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6893e-06 - val_loss: 5.8542e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6753e-06 - val_loss: 5.8396e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6599e-06 - val_loss: 5.8474e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 39us/step - loss: 4.9881e-06 - val_loss: 5.6623e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.8102e-06 - val_loss: 5.7049e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.6898e-06 - val_loss: 5.7333e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 4.6149e-06 - val_loss: 5.7206e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.5459e-06 - val_loss: 5.7385e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.4855e-06 - val_loss: 5.7525e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.4382e-06 - val_loss: 5.7652e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 39us/step - loss: 4.3940e-06 - val_loss: 5.7753e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.3586e-06 - val_loss: 5.7931e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 39us/step - loss: 4.3237e-06 - val_loss: 5.8074e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.9735e-06 - val_loss: 5.8251e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8185e-06 - val_loss: 5.7912e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7569e-06 - val_loss: 5.8015e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7189e-06 - val_loss: 5.8092e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6931e-06 - val_loss: 5.8114e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6712e-06 - val_loss: 5.8034e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6499e-06 - val_loss: 5.8056e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6359e-06 - val_loss: 5.8378e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6211e-06 - val_loss: 5.8176e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6083e-06 - val_loss: 5.8193e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 5.0162e-06 - val_loss: 5.8229e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8687e-06 - val_loss: 5.7996e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8086e-06 - val_loss: 5.8010e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7748e-06 - val_loss: 5.8236e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7485e-06 - val_loss: 5.8078e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7233e-06 - val_loss: 5.8259e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7058e-06 - val_loss: 5.8128e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6896e-06 - val_loss: 5.8396e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6769e-06 - val_loss: 5.8264e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6630e-06 - val_loss: 5.8388e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9662e-06 - val_loss: 5.6066e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8278e-06 - val_loss: 5.6148e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7633e-06 - val_loss: 5.5586e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7282e-06 - val_loss: 5.5898e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7020e-06 - val_loss: 5.5810e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6795e-06 - val_loss: 5.6008e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6584e-06 - val_loss: 5.5894e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6443e-06 - val_loss: 5.5950e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6327e-06 - val_loss: 5.6150e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6193e-06 - val_loss: 5.6208e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 4s 35us/step - loss: 4.9740e-06 - val_loss: 5.8072e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.8243e-06 - val_loss: 5.7913e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7653e-06 - val_loss: 5.7861e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7285e-06 - val_loss: 5.7807e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7018e-06 - val_loss: 5.8269e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6835e-06 - val_loss: 5.8237e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6603e-06 - val_loss: 5.7972e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6441e-06 - val_loss: 5.8171e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6312e-06 - val_loss: 5.8281e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6165e-06 - val_loss: 5.8495e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 35us/step - loss: 4.9839e-06 - val_loss: 5.8491e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.8377e-06 - val_loss: 5.7916e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7762e-06 - val_loss: 5.7950e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.7391e-06 - val_loss: 5.8001e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.7114e-06 - val_loss: 5.8027e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.6889e-06 - val_loss: 5.8118e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.6717e-06 - val_loss: 5.8038e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6570e-06 - val_loss: 5.8200e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6428e-06 - val_loss: 5.8345e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.6300e-06 - val_loss: 5.8557e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9411e-06 - val_loss: 5.5750e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.8012e-06 - val_loss: 5.5526e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7415e-06 - val_loss: 5.5467e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7071e-06 - val_loss: 5.5533e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6756e-06 - val_loss: 5.5671e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6562e-06 - val_loss: 5.5435e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6361e-06 - val_loss: 5.5616e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6190e-06 - val_loss: 5.5635e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6050e-06 - val_loss: 5.5861e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5932e-06 - val_loss: 5.5697e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 236us/step - loss: 4.9567e-06 - val_loss: 5.8091e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 215us/step - loss: 4.7793e-06 - val_loss: 5.8414e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 207us/step - loss: 4.6589e-06 - val_loss: 5.8709e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.5785e-06 - val_loss: 5.8893e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 208us/step - loss: 4.5224e-06 - val_loss: 5.8976e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 217us/step - loss: 4.4799e-06 - val_loss: 5.9085e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.4461e-06 - val_loss: 5.9110e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 211us/step - loss: 4.4150e-06 - val_loss: 5.9411e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 218us/step - loss: 4.3892e-06 - val_loss: 5.9328e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 211us/step - loss: 4.3679e-06 - val_loss: 5.9460e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 35us/step - loss: 5.0153e-06 - val_loss: 5.8236e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8435e-06 - val_loss: 5.7921e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7803e-06 - val_loss: 5.7974e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7438e-06 - val_loss: 5.7980e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7142e-06 - val_loss: 5.8085e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6908e-06 - val_loss: 5.8074e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6721e-06 - val_loss: 5.8094e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6569e-06 - val_loss: 5.8232e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6442e-06 - val_loss: 5.8366e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6312e-06 - val_loss: 5.8290e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9684e-06 - val_loss: 5.8174e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8221e-06 - val_loss: 5.7922e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7627e-06 - val_loss: 5.8065e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7237e-06 - val_loss: 5.8090e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6975e-06 - val_loss: 5.8052e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6762e-06 - val_loss: 5.8033e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6560e-06 - val_loss: 5.8388e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6412e-06 - val_loss: 5.8149e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6260e-06 - val_loss: 5.8159e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6165e-06 - val_loss: 5.8430e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 4.9339e-06 - val_loss: 5.6480e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.7584e-06 - val_loss: 5.6989e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.6672e-06 - val_loss: 5.7027e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.5755e-06 - val_loss: 5.6978e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.5087e-06 - val_loss: 5.7073e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.4438e-06 - val_loss: 5.7364e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.3970e-06 - val_loss: 5.7322e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.3549e-06 - val_loss: 5.7434e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.3157e-06 - val_loss: 5.7681e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.2812e-06 - val_loss: 5.7655e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.9261e-06 - val_loss: 5.7954e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7722e-06 - val_loss: 5.7668e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7091e-06 - val_loss: 5.7955e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6733e-06 - val_loss: 5.7820e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6465e-06 - val_loss: 5.7958e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6269e-06 - val_loss: 5.7857e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6051e-06 - val_loss: 5.7851e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5907e-06 - val_loss: 5.8067e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5767e-06 - val_loss: 5.7984e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5652e-06 - val_loss: 5.8005e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.9738e-06 - val_loss: 5.7850e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8234e-06 - val_loss: 5.7917e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7656e-06 - val_loss: 5.7777e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7281e-06 - val_loss: 5.7919e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7000e-06 - val_loss: 5.7992e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6805e-06 - val_loss: 5.8044e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6602e-06 - val_loss: 5.8116e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6481e-06 - val_loss: 5.8164e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6329e-06 - val_loss: 5.8229e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6213e-06 - val_loss: 5.8380e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.9214e-06 - val_loss: 5.6147e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7838e-06 - val_loss: 5.5590e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7215e-06 - val_loss: 5.5635e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6809e-06 - val_loss: 5.5765e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6546e-06 - val_loss: 5.5441e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6342e-06 - val_loss: 5.5573e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6146e-06 - val_loss: 5.5783e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6007e-06 - val_loss: 5.5971e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5829e-06 - val_loss: 5.6102e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5734e-06 - val_loss: 5.5837e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.9224e-06 - val_loss: 5.7703e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7791e-06 - val_loss: 5.7841e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7189e-06 - val_loss: 5.7877e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6827e-06 - val_loss: 5.7664e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6572e-06 - val_loss: 5.7709e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6325e-06 - val_loss: 5.7868e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6149e-06 - val_loss: 5.7949e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5977e-06 - val_loss: 5.7883e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5852e-06 - val_loss: 5.8082e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5722e-06 - val_loss: 5.8207e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 4s 35us/step - loss: 4.9375e-06 - val_loss: 5.8102e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.7896e-06 - val_loss: 5.7713e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7319e-06 - val_loss: 5.7785e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6917e-06 - val_loss: 5.7814e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6678e-06 - val_loss: 5.7921e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6469e-06 - val_loss: 5.8022e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6259e-06 - val_loss: 5.8056e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6086e-06 - val_loss: 5.8122e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5975e-06 - val_loss: 5.8071e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5829e-06 - val_loss: 5.8147e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8970e-06 - val_loss: 5.5649e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7541e-06 - val_loss: 5.5599e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6961e-06 - val_loss: 5.5311e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6587e-06 - val_loss: 5.5601e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6297e-06 - val_loss: 5.5229e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6081e-06 - val_loss: 5.5521e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5904e-06 - val_loss: 5.5381e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.5730e-06 - val_loss: 5.5532e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.5603e-06 - val_loss: 5.5458e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5482e-06 - val_loss: 5.5576e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 238us/step - loss: 4.9249e-06 - val_loss: 5.8212e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 220us/step - loss: 4.7370e-06 - val_loss: 5.8515e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 210us/step - loss: 4.6105e-06 - val_loss: 5.8623e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.5269e-06 - val_loss: 5.8743e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 208us/step - loss: 4.4713e-06 - val_loss: 5.8608e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 213us/step - loss: 4.4300e-06 - val_loss: 5.8913e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 207us/step - loss: 4.3958e-06 - val_loss: 5.8946e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 200us/step - loss: 4.3667e-06 - val_loss: 5.9321e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 201us/step - loss: 4.3411e-06 - val_loss: 5.9376e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 199us/step - loss: 4.3214e-06 - val_loss: 5.9396e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.9681e-06 - val_loss: 5.7823e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7955e-06 - val_loss: 5.7730e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7323e-06 - val_loss: 5.7697e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6927e-06 - val_loss: 5.7942e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6660e-06 - val_loss: 5.7669e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6449e-06 - val_loss: 5.7866e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6262e-06 - val_loss: 5.7807e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6102e-06 - val_loss: 5.7980e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5958e-06 - val_loss: 5.7986e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5835e-06 - val_loss: 5.8392e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9234e-06 - val_loss: 5.7738e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7759e-06 - val_loss: 5.7933e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7148e-06 - val_loss: 5.7855e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6756e-06 - val_loss: 5.7818e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6449e-06 - val_loss: 5.7910e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6250e-06 - val_loss: 5.8009e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6094e-06 - val_loss: 5.7992e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5905e-06 - val_loss: 5.8031e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5759e-06 - val_loss: 5.8198e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5620e-06 - val_loss: 5.8386e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 47us/step - loss: 4.8951e-06 - val_loss: 5.6312e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.7289e-06 - val_loss: 5.6765e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.6150e-06 - val_loss: 5.7091e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.5282e-06 - val_loss: 5.6911e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.4667e-06 - val_loss: 5.7130e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.4055e-06 - val_loss: 5.7304e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.3567e-06 - val_loss: 5.7107e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.3166e-06 - val_loss: 5.7143e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.2804e-06 - val_loss: 5.7430e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.2480e-06 - val_loss: 5.7653e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.8810e-06 - val_loss: 5.7683e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7238e-06 - val_loss: 5.7500e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6602e-06 - val_loss: 5.7462e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6244e-06 - val_loss: 5.7499e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5989e-06 - val_loss: 5.7622e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5763e-06 - val_loss: 5.7581e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5577e-06 - val_loss: 5.7581e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5441e-06 - val_loss: 5.7732e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5311e-06 - val_loss: 5.7777e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5156e-06 - val_loss: 5.7980e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.9242e-06 - val_loss: 5.7763e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7799e-06 - val_loss: 5.7805e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7162e-06 - val_loss: 5.7652e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6822e-06 - val_loss: 5.7784e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6525e-06 - val_loss: 5.7692e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6283e-06 - val_loss: 5.7904e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6124e-06 - val_loss: 5.7932e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5956e-06 - val_loss: 5.7806e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5828e-06 - val_loss: 5.8128e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5699e-06 - val_loss: 5.8031e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8842e-06 - val_loss: 5.5873e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7340e-06 - val_loss: 5.5639e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6682e-06 - val_loss: 5.5653e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6338e-06 - val_loss: 5.5655e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6056e-06 - val_loss: 5.5367e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5811e-06 - val_loss: 5.5720e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5626e-06 - val_loss: 5.5574e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5472e-06 - val_loss: 5.5787e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5305e-06 - val_loss: 5.5710e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5183e-06 - val_loss: 5.5832e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.8784e-06 - val_loss: 5.8075e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.7299e-06 - val_loss: 5.7438e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6712e-06 - val_loss: 5.7486e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6283e-06 - val_loss: 5.7433e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6014e-06 - val_loss: 5.7577e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.5818e-06 - val_loss: 5.7695e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5600e-06 - val_loss: 5.7689e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5431e-06 - val_loss: 5.7732e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5280e-06 - val_loss: 5.7792e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5183e-06 - val_loss: 5.7779e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.8862e-06 - val_loss: 5.7975e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7408e-06 - val_loss: 5.7644e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6746e-06 - val_loss: 5.7653e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6403e-06 - val_loss: 5.7433e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6093e-06 - val_loss: 5.7637e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5874e-06 - val_loss: 5.7640e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5688e-06 - val_loss: 5.7845e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5517e-06 - val_loss: 5.7893e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5356e-06 - val_loss: 5.7929e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5242e-06 - val_loss: 5.8137e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 25us/step - loss: 4.8587e-06 - val_loss: 5.5457e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.7074e-06 - val_loss: 5.5133e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6453e-06 - val_loss: 5.5190e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6049e-06 - val_loss: 5.5179e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.5780e-06 - val_loss: 5.5130e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5556e-06 - val_loss: 5.5001e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5373e-06 - val_loss: 5.4930e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5226e-06 - val_loss: 5.5215e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5086e-06 - val_loss: 5.5082e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4960e-06 - val_loss: 5.5128e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 237us/step - loss: 4.9288e-06 - val_loss: 5.7975e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 218us/step - loss: 4.6900e-06 - val_loss: 5.8156e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.5628e-06 - val_loss: 5.8200e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 207us/step - loss: 4.4842e-06 - val_loss: 5.8444e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.4208e-06 - val_loss: 5.8543e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 215us/step - loss: 4.3791e-06 - val_loss: 5.8592e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 206us/step - loss: 4.3427e-06 - val_loss: 5.8786e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.3130e-06 - val_loss: 5.8888e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 206us/step - loss: 4.2882e-06 - val_loss: 5.8928e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.2671e-06 - val_loss: 5.9153e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.9207e-06 - val_loss: 5.7542e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7518e-06 - val_loss: 5.7396e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6859e-06 - val_loss: 5.7395e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6474e-06 - val_loss: 5.7638e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6203e-06 - val_loss: 5.7562e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5973e-06 - val_loss: 5.7601e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5802e-06 - val_loss: 5.7968e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5610e-06 - val_loss: 5.7724e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5482e-06 - val_loss: 5.7786e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.5339e-06 - val_loss: 5.8049e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.8699e-06 - val_loss: 5.7925e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7244e-06 - val_loss: 5.7506e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6614e-06 - val_loss: 5.7600e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6249e-06 - val_loss: 5.7817e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5955e-06 - val_loss: 5.7642e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5732e-06 - val_loss: 5.7871e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5551e-06 - val_loss: 5.7969e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5386e-06 - val_loss: 5.8017e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5246e-06 - val_loss: 5.7989e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5115e-06 - val_loss: 5.8163e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.8937e-06 - val_loss: 5.5867e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.6866e-06 - val_loss: 5.6183e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.5922e-06 - val_loss: 5.6700e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 4.5069e-06 - val_loss: 5.6395e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.4393e-06 - val_loss: 5.6418e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.3739e-06 - val_loss: 5.6889e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.3278e-06 - val_loss: 5.6592e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.2841e-06 - val_loss: 5.6673e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 38us/step - loss: 4.2488e-06 - val_loss: 5.6989e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.2142e-06 - val_loss: 5.6931e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.8420e-06 - val_loss: 5.7723e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6813e-06 - val_loss: 5.7155e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6187e-06 - val_loss: 5.7189e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5806e-06 - val_loss: 5.7430e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5517e-06 - val_loss: 5.7368e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5271e-06 - val_loss: 5.7463e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5111e-06 - val_loss: 5.7569e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4945e-06 - val_loss: 5.7535e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4773e-06 - val_loss: 5.7702e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4671e-06 - val_loss: 5.7717e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8818e-06 - val_loss: 5.7821e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7345e-06 - val_loss: 5.7487e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6716e-06 - val_loss: 5.7548e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6319e-06 - val_loss: 5.7566e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6079e-06 - val_loss: 5.7600e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5821e-06 - val_loss: 5.7655e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5655e-06 - val_loss: 5.7664e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5481e-06 - val_loss: 5.7834e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5355e-06 - val_loss: 5.7970e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5214e-06 - val_loss: 5.7831e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8269e-06 - val_loss: 5.5201e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6765e-06 - val_loss: 5.5062e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6178e-06 - val_loss: 5.4978e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5795e-06 - val_loss: 5.5067e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5510e-06 - val_loss: 5.5196e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5277e-06 - val_loss: 5.5349e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.5065e-06 - val_loss: 5.5201e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4906e-06 - val_loss: 5.5230e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4770e-06 - val_loss: 5.5458e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4642e-06 - val_loss: 5.5520e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.8404e-06 - val_loss: 5.8078e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6789e-06 - val_loss: 5.7359e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6177e-06 - val_loss: 5.7203e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5774e-06 - val_loss: 5.7235e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5524e-06 - val_loss: 5.7196e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5282e-06 - val_loss: 5.7516e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5099e-06 - val_loss: 5.7732e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.4939e-06 - val_loss: 5.7492e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.4810e-06 - val_loss: 5.7538e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.4654e-06 - val_loss: 5.7732e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.8487e-06 - val_loss: 5.7369e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6860e-06 - val_loss: 5.7115e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.6249e-06 - val_loss: 5.7260e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5850e-06 - val_loss: 5.7557e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5583e-06 - val_loss: 5.7543e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5360e-06 - val_loss: 5.7564e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.5170e-06 - val_loss: 5.7651e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.5059e-06 - val_loss: 5.7783e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.4881e-06 - val_loss: 5.7688e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.4734e-06 - val_loss: 5.7779e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.8059e-06 - val_loss: 5.5042e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6546e-06 - val_loss: 5.4841e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5970e-06 - val_loss: 5.4874e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5543e-06 - val_loss: 5.4652e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5252e-06 - val_loss: 5.4858e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5060e-06 - val_loss: 5.4842e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4864e-06 - val_loss: 5.5017e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4706e-06 - val_loss: 5.4999e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4541e-06 - val_loss: 5.5090e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4445e-06 - val_loss: 5.4955e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 243us/step - loss: 4.8587e-06 - val_loss: 5.7950e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 216us/step - loss: 4.6471e-06 - val_loss: 5.7819e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 210us/step - loss: 4.5193e-06 - val_loss: 5.8114e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 220us/step - loss: 4.4307e-06 - val_loss: 5.8246e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 201us/step - loss: 4.3709e-06 - val_loss: 5.8418e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.3322e-06 - val_loss: 5.8468e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.2976e-06 - val_loss: 5.8665e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.2677e-06 - val_loss: 5.8821e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 199us/step - loss: 4.2407e-06 - val_loss: 5.8878e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 200us/step - loss: 4.2199e-06 - val_loss: 5.8919e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8695e-06 - val_loss: 5.7444e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.6952e-06 - val_loss: 5.7392e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6340e-06 - val_loss: 5.7266e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5989e-06 - val_loss: 5.7482e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5700e-06 - val_loss: 5.7390e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5464e-06 - val_loss: 5.7493e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5277e-06 - val_loss: 5.7565e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5104e-06 - val_loss: 5.7545e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4980e-06 - val_loss: 5.7704e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4849e-06 - val_loss: 5.7768e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.8293e-06 - val_loss: 5.7772e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6772e-06 - val_loss: 5.7452e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6139e-06 - val_loss: 5.7548e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5736e-06 - val_loss: 5.7549e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5435e-06 - val_loss: 5.7511e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5234e-06 - val_loss: 5.7595e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5050e-06 - val_loss: 5.7619e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4884e-06 - val_loss: 5.7820e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4747e-06 - val_loss: 5.7622e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4593e-06 - val_loss: 5.7938e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 51us/step - loss: 4.8638e-06 - val_loss: 5.5691e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 4.6482e-06 - val_loss: 5.6117e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.5410e-06 - val_loss: 5.6485e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.4442e-06 - val_loss: 5.6452e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.3787e-06 - val_loss: 5.6534e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.3135e-06 - val_loss: 5.6702e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 39us/step - loss: 4.2706e-06 - val_loss: 5.6599e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.2243e-06 - val_loss: 5.6823e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.1768e-06 - val_loss: 5.6920e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.1404e-06 - val_loss: 5.7127e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8128e-06 - val_loss: 5.7174e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.6326e-06 - val_loss: 5.7102e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5684e-06 - val_loss: 5.7243e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5295e-06 - val_loss: 5.7152e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5000e-06 - val_loss: 5.7257e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4801e-06 - val_loss: 5.7278e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4628e-06 - val_loss: 5.7178e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4450e-06 - val_loss: 5.7041e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4285e-06 - val_loss: 5.7407e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4178e-06 - val_loss: 5.7484e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.8299e-06 - val_loss: 5.7353e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6833e-06 - val_loss: 5.7214e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6215e-06 - val_loss: 5.7263e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5835e-06 - val_loss: 5.7210e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5571e-06 - val_loss: 5.7563e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5333e-06 - val_loss: 5.7475e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5145e-06 - val_loss: 5.7549e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4974e-06 - val_loss: 5.7530e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4821e-06 - val_loss: 5.7672e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4724e-06 - val_loss: 5.7810e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7802e-06 - val_loss: 5.4762e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6303e-06 - val_loss: 5.4571e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5700e-06 - val_loss: 5.4804e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5256e-06 - val_loss: 5.4682e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5003e-06 - val_loss: 5.4683e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4771e-06 - val_loss: 5.4780e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4575e-06 - val_loss: 5.5099e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.4436e-06 - val_loss: 5.5028e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4309e-06 - val_loss: 5.5062e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4155e-06 - val_loss: 5.5118e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7855e-06 - val_loss: 5.7176e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.6319e-06 - val_loss: 5.7088e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.5693e-06 - val_loss: 5.6976e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.5304e-06 - val_loss: 5.7165e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.5023e-06 - val_loss: 5.7098e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.4793e-06 - val_loss: 5.7426e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.4614e-06 - val_loss: 5.7298e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.4434e-06 - val_loss: 5.7287e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.4296e-06 - val_loss: 5.7527e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.4182e-06 - val_loss: 5.7537e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.7991e-06 - val_loss: 5.7227e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6393e-06 - val_loss: 5.7057e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5784e-06 - val_loss: 5.7200e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5418e-06 - val_loss: 5.7188e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5120e-06 - val_loss: 5.7387e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.4902e-06 - val_loss: 5.7501e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4733e-06 - val_loss: 5.7446e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4560e-06 - val_loss: 5.7432e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4390e-06 - val_loss: 5.7548e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4274e-06 - val_loss: 5.7735e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7580e-06 - val_loss: 5.5143e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.6124e-06 - val_loss: 5.4969e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5473e-06 - val_loss: 5.4958e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5091e-06 - val_loss: 5.4450e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4826e-06 - val_loss: 5.4661e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4618e-06 - val_loss: 5.5003e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4439e-06 - val_loss: 5.4950e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4252e-06 - val_loss: 5.4840e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4097e-06 - val_loss: 5.4962e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3954e-06 - val_loss: 5.5165e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 231us/step - loss: 4.8521e-06 - val_loss: 5.7825e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 214us/step - loss: 4.6113e-06 - val_loss: 5.8069e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 224us/step - loss: 4.4789e-06 - val_loss: 5.8113e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.3918e-06 - val_loss: 5.8290e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.3325e-06 - val_loss: 5.8248e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 221us/step - loss: 4.2887e-06 - val_loss: 5.8402e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 206us/step - loss: 4.2522e-06 - val_loss: 5.8484e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 207us/step - loss: 4.2242e-06 - val_loss: 5.8558e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 216us/step - loss: 4.1997e-06 - val_loss: 5.8550e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 206us/step - loss: 4.1764e-06 - val_loss: 5.8798e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.8286e-06 - val_loss: 5.7105e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6544e-06 - val_loss: 5.6981e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5879e-06 - val_loss: 5.7155e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5498e-06 - val_loss: 5.7036e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.5214e-06 - val_loss: 5.7211e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4961e-06 - val_loss: 5.7198e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.4793e-06 - val_loss: 5.7269e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4631e-06 - val_loss: 5.7360e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4509e-06 - val_loss: 5.7591e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.4371e-06 - val_loss: 5.7439e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.7855e-06 - val_loss: 5.7261e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6253e-06 - val_loss: 5.7118e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5667e-06 - val_loss: 5.7525e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5254e-06 - val_loss: 5.7313e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4972e-06 - val_loss: 5.7257e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4751e-06 - val_loss: 5.7430e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4576e-06 - val_loss: 5.7421e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4413e-06 - val_loss: 5.7456e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4245e-06 - val_loss: 5.7422e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.4098e-06 - val_loss: 5.7755e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 47us/step - loss: 4.7844e-06 - val_loss: 5.5335e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.5736e-06 - val_loss: 5.5953e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 48us/step - loss: 4.4826e-06 - val_loss: 5.6520e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.3891e-06 - val_loss: 5.6278e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.3202e-06 - val_loss: 5.6424e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 47us/step - loss: 4.2585e-06 - val_loss: 5.6560e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.2032e-06 - val_loss: 5.6436e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.1603e-06 - val_loss: 5.6508e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.1223e-06 - val_loss: 5.6592e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.0858e-06 - val_loss: 5.6630e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 4s 35us/step - loss: 4.7517e-06 - val_loss: 5.6884e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5849e-06 - val_loss: 5.6779e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5261e-06 - val_loss: 5.6923e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.4827e-06 - val_loss: 5.7056e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.4571e-06 - val_loss: 5.7105e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.4337e-06 - val_loss: 5.6933e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.4134e-06 - val_loss: 5.7014e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 35us/step - loss: 4.3949e-06 - val_loss: 5.7021e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.3810e-06 - val_loss: 5.7290e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3700e-06 - val_loss: 5.7357e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.7836e-06 - val_loss: 5.7271e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6310e-06 - val_loss: 5.6980e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5687e-06 - val_loss: 5.6897e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5291e-06 - val_loss: 5.7157e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5031e-06 - val_loss: 5.7183e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4793e-06 - val_loss: 5.7195e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4627e-06 - val_loss: 5.7248e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4442e-06 - val_loss: 5.7379e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.4331e-06 - val_loss: 5.7545e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 4s 38us/step - loss: 4.4190e-06 - val_loss: 5.7423e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 25us/step - loss: 4.7376e-06 - val_loss: 5.4690e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.5910e-06 - val_loss: 5.4614e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.5216e-06 - val_loss: 5.4491e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.4837e-06 - val_loss: 5.4586e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.4553e-06 - val_loss: 5.4739e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.4330e-06 - val_loss: 5.4656e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.4133e-06 - val_loss: 5.4707e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.3972e-06 - val_loss: 5.4832e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3855e-06 - val_loss: 5.4815e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3714e-06 - val_loss: 5.4780e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.7416e-06 - val_loss: 5.6922e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.5830e-06 - val_loss: 5.6733e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5218e-06 - val_loss: 5.6834e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.4844e-06 - val_loss: 5.6902e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.4562e-06 - val_loss: 5.6811e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.4319e-06 - val_loss: 5.7075e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.4130e-06 - val_loss: 5.7027e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3987e-06 - val_loss: 5.7156e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.3831e-06 - val_loss: 5.7336e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3705e-06 - val_loss: 5.7454e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 35us/step - loss: 4.7573e-06 - val_loss: 5.7026e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5997e-06 - val_loss: 5.6827e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5360e-06 - val_loss: 5.6788e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.4963e-06 - val_loss: 5.7028e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.4647e-06 - val_loss: 5.7122e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.4432e-06 - val_loss: 5.7139e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.4248e-06 - val_loss: 5.7148e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.4086e-06 - val_loss: 5.7316e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.3943e-06 - val_loss: 5.7290e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.3809e-06 - val_loss: 5.7265e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.7172e-06 - val_loss: 5.4598e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.5641e-06 - val_loss: 5.4510e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.5032e-06 - val_loss: 5.4795e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.4641e-06 - val_loss: 5.4538e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.4355e-06 - val_loss: 5.4466e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4129e-06 - val_loss: 5.4912e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.3963e-06 - val_loss: 5.4543e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.3759e-06 - val_loss: 5.5026e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.3634e-06 - val_loss: 5.5403e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3524e-06 - val_loss: 5.4850e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 237us/step - loss: 4.7933e-06 - val_loss: 5.7083e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 216us/step - loss: 4.5536e-06 - val_loss: 5.7422e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.4270e-06 - val_loss: 5.7737e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.3514e-06 - val_loss: 5.7905e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.2880e-06 - val_loss: 5.8014e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 223us/step - loss: 4.2512e-06 - val_loss: 5.7943e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.2137e-06 - val_loss: 5.8192e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.1833e-06 - val_loss: 5.8310e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.1593e-06 - val_loss: 5.8640e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 205us/step - loss: 4.1395e-06 - val_loss: 5.8703e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 35us/step - loss: 4.7940e-06 - val_loss: 5.7073e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.6081e-06 - val_loss: 5.6931e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5427e-06 - val_loss: 5.6849e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5079e-06 - val_loss: 5.6832e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4756e-06 - val_loss: 5.7002e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4543e-06 - val_loss: 5.7019e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.4340e-06 - val_loss: 5.7082e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.4151e-06 - val_loss: 5.7319e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.4000e-06 - val_loss: 5.7204e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.3860e-06 - val_loss: 5.7190e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7277e-06 - val_loss: 5.7027e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5793e-06 - val_loss: 5.7305e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5173e-06 - val_loss: 5.6944e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4775e-06 - val_loss: 5.7026e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4479e-06 - val_loss: 5.7315e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.4260e-06 - val_loss: 5.7024e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.4088e-06 - val_loss: 5.7233e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.3909e-06 - val_loss: 5.7335e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.3755e-06 - val_loss: 5.7360e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3659e-06 - val_loss: 5.7420e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 48us/step - loss: 4.7773e-06 - val_loss: 5.5497e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 47us/step - loss: 4.5885e-06 - val_loss: 5.6244e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.4408e-06 - val_loss: 5.6257e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.3572e-06 - val_loss: 5.6173e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 46us/step - loss: 4.2903e-06 - val_loss: 5.6113e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.2240e-06 - val_loss: 5.6493e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.1800e-06 - val_loss: 5.6418e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.1300e-06 - val_loss: 5.6478e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.0928e-06 - val_loss: 5.6837e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.0553e-06 - val_loss: 5.6677e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7078e-06 - val_loss: 5.6981e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5422e-06 - val_loss: 5.6760e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4740e-06 - val_loss: 5.6511e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4361e-06 - val_loss: 5.6576e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4099e-06 - val_loss: 5.6804e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3876e-06 - val_loss: 5.6949e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3691e-06 - val_loss: 5.6792e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3525e-06 - val_loss: 5.6967e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3376e-06 - val_loss: 5.6995e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3249e-06 - val_loss: 5.7313e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.7428e-06 - val_loss: 5.6948e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5856e-06 - val_loss: 5.6892e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.5219e-06 - val_loss: 5.6898e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4824e-06 - val_loss: 5.6851e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4583e-06 - val_loss: 5.7022e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4341e-06 - val_loss: 5.7041e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4168e-06 - val_loss: 5.7107e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4016e-06 - val_loss: 5.7188e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3824e-06 - val_loss: 5.7167e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3700e-06 - val_loss: 5.7209e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6869e-06 - val_loss: 5.4594e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5386e-06 - val_loss: 5.4234e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4761e-06 - val_loss: 5.4431e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4388e-06 - val_loss: 5.4417e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4111e-06 - val_loss: 5.4311e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3899e-06 - val_loss: 5.4667e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3706e-06 - val_loss: 5.4666e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3535e-06 - val_loss: 5.4555e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3394e-06 - val_loss: 5.4599e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3257e-06 - val_loss: 5.4812e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6919e-06 - val_loss: 5.6777e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.5387e-06 - val_loss: 5.6670e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.4776e-06 - val_loss: 5.6641e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.4382e-06 - val_loss: 5.6424e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.4069e-06 - val_loss: 5.6703e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.3849e-06 - val_loss: 5.6842e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3674e-06 - val_loss: 5.6967e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3518e-06 - val_loss: 5.6788e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3358e-06 - val_loss: 5.6858e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3237e-06 - val_loss: 5.7196e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 4.7086e-06 - val_loss: 5.6564e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5516e-06 - val_loss: 5.6730e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4899e-06 - val_loss: 5.6626e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4521e-06 - val_loss: 5.6885e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4203e-06 - val_loss: 5.6753e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.3977e-06 - val_loss: 5.7087e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.3805e-06 - val_loss: 5.7078e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.3627e-06 - val_loss: 5.7158e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.3482e-06 - val_loss: 5.7172e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.3348e-06 - val_loss: 5.7147e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6698e-06 - val_loss: 5.4887e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5162e-06 - val_loss: 5.4385e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4520e-06 - val_loss: 5.4190e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4119e-06 - val_loss: 5.4565e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3878e-06 - val_loss: 5.4719e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3631e-06 - val_loss: 5.4874e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3433e-06 - val_loss: 5.4914e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3288e-06 - val_loss: 5.5143e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3145e-06 - val_loss: 5.5429e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3026e-06 - val_loss: 5.5245e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 226us/step - loss: 4.7807e-06 - val_loss: 5.7076e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 210us/step - loss: 4.5241e-06 - val_loss: 5.7019e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 201us/step - loss: 4.3979e-06 - val_loss: 5.7459e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 202us/step - loss: 4.3072e-06 - val_loss: 5.7544e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 198us/step - loss: 4.2389e-06 - val_loss: 5.7737e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 209us/step - loss: 4.1933e-06 - val_loss: 5.7972e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.1579e-06 - val_loss: 5.7902e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 200us/step - loss: 4.1266e-06 - val_loss: 5.7967e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 198us/step - loss: 4.1010e-06 - val_loss: 5.8180e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 199us/step - loss: 4.0780e-06 - val_loss: 5.8235e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.7432e-06 - val_loss: 5.6635e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5659e-06 - val_loss: 5.6646e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4976e-06 - val_loss: 5.6478e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.4586e-06 - val_loss: 5.6866e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4322e-06 - val_loss: 5.6698e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4049e-06 - val_loss: 5.6841e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3884e-06 - val_loss: 5.6684e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3685e-06 - val_loss: 5.7097e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.3539e-06 - val_loss: 5.6988e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3415e-06 - val_loss: 5.7094e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6898e-06 - val_loss: 5.7060e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5337e-06 - val_loss: 5.6799e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4699e-06 - val_loss: 5.7029e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4314e-06 - val_loss: 5.6765e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4039e-06 - val_loss: 5.7053e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3813e-06 - val_loss: 5.6883e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3619e-06 - val_loss: 5.6963e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3480e-06 - val_loss: 5.7326e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.3311e-06 - val_loss: 5.7352e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3186e-06 - val_loss: 5.7306e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.7339e-06 - val_loss: 5.5611e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.5286e-06 - val_loss: 5.5975e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.3957e-06 - val_loss: 5.6327e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.3033e-06 - val_loss: 5.6341e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.2328e-06 - val_loss: 5.6511e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.1797e-06 - val_loss: 5.6747e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.1290e-06 - val_loss: 5.6746e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 4.0839e-06 - val_loss: 5.6690e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.0445e-06 - val_loss: 5.6929e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.0081e-06 - val_loss: 5.6939e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6682e-06 - val_loss: 5.6584e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4965e-06 - val_loss: 5.6450e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4362e-06 - val_loss: 5.6502e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3939e-06 - val_loss: 5.6483e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3646e-06 - val_loss: 5.6591e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3442e-06 - val_loss: 5.6715e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3279e-06 - val_loss: 5.6654e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3119e-06 - val_loss: 5.6616e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.2942e-06 - val_loss: 5.6824e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.2798e-06 - val_loss: 5.6827e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6929e-06 - val_loss: 5.6654e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5423e-06 - val_loss: 5.6860e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4748e-06 - val_loss: 5.6850e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.4389e-06 - val_loss: 5.6864e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4110e-06 - val_loss: 5.6671e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3872e-06 - val_loss: 5.6881e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3676e-06 - val_loss: 5.6755e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3527e-06 - val_loss: 5.7171e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3390e-06 - val_loss: 5.7132e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3234e-06 - val_loss: 5.6933e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6493e-06 - val_loss: 5.4525e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4974e-06 - val_loss: 5.4538e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4333e-06 - val_loss: 5.4163e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3964e-06 - val_loss: 5.4141e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3677e-06 - val_loss: 5.4144e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3471e-06 - val_loss: 5.4149e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3275e-06 - val_loss: 5.4670e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3124e-06 - val_loss: 5.4542e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2972e-06 - val_loss: 5.4342e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2863e-06 - val_loss: 5.4554e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 34us/step - loss: 4.6636e-06 - val_loss: 5.6407e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.4935e-06 - val_loss: 5.6283e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.4340e-06 - val_loss: 5.6388e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.3939e-06 - val_loss: 5.6229e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3661e-06 - val_loss: 5.6518e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3463e-06 - val_loss: 5.6569e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.3257e-06 - val_loss: 5.6750e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.3110e-06 - val_loss: 5.6630e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.2950e-06 - val_loss: 5.6885e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.2812e-06 - val_loss: 5.6724e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6658e-06 - val_loss: 5.6696e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.5120e-06 - val_loss: 5.6478e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4492e-06 - val_loss: 5.6535e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4073e-06 - val_loss: 5.6476e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.3758e-06 - val_loss: 5.6957e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.3563e-06 - val_loss: 5.6880e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.3375e-06 - val_loss: 5.6766e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.3190e-06 - val_loss: 5.6847e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.3059e-06 - val_loss: 5.7015e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.2935e-06 - val_loss: 5.6900e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6390e-06 - val_loss: 5.5136e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4753e-06 - val_loss: 5.4255e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4107e-06 - val_loss: 5.4241e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3746e-06 - val_loss: 5.4642e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3430e-06 - val_loss: 5.4453e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3211e-06 - val_loss: 5.4780e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3032e-06 - val_loss: 5.4921e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2865e-06 - val_loss: 5.4766e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2712e-06 - val_loss: 5.4971e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2601e-06 - val_loss: 5.5054e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 234us/step - loss: 4.7145e-06 - val_loss: 5.6769e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 215us/step - loss: 4.4796e-06 - val_loss: 5.7031e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.3518e-06 - val_loss: 5.7367e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 198us/step - loss: 4.2628e-06 - val_loss: 5.7456e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 198us/step - loss: 4.1964e-06 - val_loss: 5.7427e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 203us/step - loss: 4.1526e-06 - val_loss: 5.7710e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 206us/step - loss: 4.1178e-06 - val_loss: 5.7767e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 198us/step - loss: 4.0880e-06 - val_loss: 5.7822e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 197us/step - loss: 4.0610e-06 - val_loss: 5.7925e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 199us/step - loss: 4.0393e-06 - val_loss: 5.7982e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.6952e-06 - val_loss: 5.6532e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.5158e-06 - val_loss: 5.6246e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.4491e-06 - val_loss: 5.6391e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.4109e-06 - val_loss: 5.6316e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3863e-06 - val_loss: 5.6723e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.3614e-06 - val_loss: 5.6523e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3422e-06 - val_loss: 5.6698e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3266e-06 - val_loss: 5.6736e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3097e-06 - val_loss: 5.6749e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.2971e-06 - val_loss: 5.7015e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6513e-06 - val_loss: 5.6416e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4918e-06 - val_loss: 5.6693e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4269e-06 - val_loss: 5.6517e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3877e-06 - val_loss: 5.6803e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3623e-06 - val_loss: 5.6861e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.3386e-06 - val_loss: 5.6789e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3189e-06 - val_loss: 5.6868e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3034e-06 - val_loss: 5.6880e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.2893e-06 - val_loss: 5.6952e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2792e-06 - val_loss: 5.7057e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.7040e-06 - val_loss: 5.5197e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.5009e-06 - val_loss: 5.5801e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.3663e-06 - val_loss: 5.6393e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 44us/step - loss: 4.2843e-06 - val_loss: 5.6113e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 43us/step - loss: 4.2225e-06 - val_loss: 5.6204e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 4.1557e-06 - val_loss: 5.6878e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 45us/step - loss: 4.1099e-06 - val_loss: 5.6539e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 38us/step - loss: 4.0615e-06 - val_loss: 5.6347e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.0197e-06 - val_loss: 5.6900e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 41us/step - loss: 3.9827e-06 - val_loss: 5.6794e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6309e-06 - val_loss: 5.6343e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4549e-06 - val_loss: 5.6047e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3910e-06 - val_loss: 5.6115e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3503e-06 - val_loss: 5.6413e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3241e-06 - val_loss: 5.6390e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2994e-06 - val_loss: 5.6532e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.2812e-06 - val_loss: 5.6523e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2665e-06 - val_loss: 5.6582e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2509e-06 - val_loss: 5.6711e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2365e-06 - val_loss: 5.6567e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.6668e-06 - val_loss: 5.6661e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.5043e-06 - val_loss: 5.6401e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4355e-06 - val_loss: 5.6453e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3997e-06 - val_loss: 5.6579e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3683e-06 - val_loss: 5.6558e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3453e-06 - val_loss: 5.6698e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3278e-06 - val_loss: 5.6837e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3106e-06 - val_loss: 5.6752e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2963e-06 - val_loss: 5.6936e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2824e-06 - val_loss: 5.6950e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 24us/step - loss: 4.6112e-06 - val_loss: 5.4091e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4562e-06 - val_loss: 5.4146e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3949e-06 - val_loss: 5.4065e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3565e-06 - val_loss: 5.3801e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3285e-06 - val_loss: 5.4258e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3087e-06 - val_loss: 5.4111e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2877e-06 - val_loss: 5.4020e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2716e-06 - val_loss: 5.3938e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2587e-06 - val_loss: 5.4304e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2448e-06 - val_loss: 5.4118e-06\n",
      "Train on 99996 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.6091e-06 - val_loss: 5.6467e-06\n",
      "Epoch 2/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.4584e-06 - val_loss: 5.6060e-06\n",
      "Epoch 3/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3930e-06 - val_loss: 5.6263e-06\n",
      "Epoch 4/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.3543e-06 - val_loss: 5.6414e-06\n",
      "Epoch 5/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.3262e-06 - val_loss: 5.6010e-06\n",
      "Epoch 6/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.3044e-06 - val_loss: 5.6346e-06\n",
      "Epoch 7/10\n",
      "99996/99996 [==============================] - 3s 32us/step - loss: 4.2842e-06 - val_loss: 5.6449e-06\n",
      "Epoch 8/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.2684e-06 - val_loss: 5.6604e-06\n",
      "Epoch 9/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.2558e-06 - val_loss: 5.6710e-06\n",
      "Epoch 10/10\n",
      "99996/99996 [==============================] - 3s 33us/step - loss: 4.2438e-06 - val_loss: 5.6558e-06\n",
      "Train on 100000 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.6208e-06 - val_loss: 5.6712e-06\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.4668e-06 - val_loss: 5.6318e-06\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.4032e-06 - val_loss: 5.6295e-06\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.3650e-06 - val_loss: 5.6320e-06\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.3383e-06 - val_loss: 5.6600e-06\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.3140e-06 - val_loss: 5.6675e-06\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.2969e-06 - val_loss: 5.6634e-06\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 4.2789e-06 - val_loss: 5.6818e-06\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.2660e-06 - val_loss: 5.6941e-06\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 32us/step - loss: 4.2511e-06 - val_loss: 5.6978e-06\n",
      "Train on 99998 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.5860e-06 - val_loss: 5.4198e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.4336e-06 - val_loss: 5.3935e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3717e-06 - val_loss: 5.3900e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3295e-06 - val_loss: 5.3825e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.3024e-06 - val_loss: 5.4424e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2789e-06 - val_loss: 5.4110e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2629e-06 - val_loss: 5.4357e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2448e-06 - val_loss: 5.4340e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2307e-06 - val_loss: 5.4674e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 2s 23us/step - loss: 4.2183e-06 - val_loss: 5.4617e-06\n",
      "Train on 5523 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "5523/5523 [==============================] - 1s 236us/step - loss: 4.6715e-06 - val_loss: 5.6646e-06\n",
      "Epoch 2/10\n",
      "5523/5523 [==============================] - 1s 217us/step - loss: 4.4486e-06 - val_loss: 5.6795e-06\n",
      "Epoch 3/10\n",
      "5523/5523 [==============================] - 1s 212us/step - loss: 4.3218e-06 - val_loss: 5.7133e-06\n",
      "Epoch 4/10\n",
      "5523/5523 [==============================] - 1s 220us/step - loss: 4.2283e-06 - val_loss: 5.7317e-06\n",
      "Epoch 5/10\n",
      "5523/5523 [==============================] - 1s 216us/step - loss: 4.1628e-06 - val_loss: 5.7255e-06\n",
      "Epoch 6/10\n",
      "5523/5523 [==============================] - 1s 222us/step - loss: 4.1125e-06 - val_loss: 5.7392e-06\n",
      "Epoch 7/10\n",
      "5523/5523 [==============================] - 1s 204us/step - loss: 4.0772e-06 - val_loss: 5.7556e-06\n",
      "Epoch 8/10\n",
      "5523/5523 [==============================] - 1s 197us/step - loss: 4.0459e-06 - val_loss: 5.7690e-06\n",
      "Epoch 9/10\n",
      "5523/5523 [==============================] - 1s 200us/step - loss: 4.0191e-06 - val_loss: 5.7961e-06\n",
      "Epoch 10/10\n",
      "5523/5523 [==============================] - 1s 207us/step - loss: 3.9942e-06 - val_loss: 5.7984e-06\n",
      "Train on 99998 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99998/99998 [==============================] - 3s 34us/step - loss: 4.6611e-06 - val_loss: 5.6401e-06\n",
      "Epoch 2/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.4807e-06 - val_loss: 5.5962e-06\n",
      "Epoch 3/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.4088e-06 - val_loss: 5.6209e-06\n",
      "Epoch 4/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3672e-06 - val_loss: 5.6275e-06\n",
      "Epoch 5/10\n",
      "99998/99998 [==============================] - 3s 33us/step - loss: 4.3397e-06 - val_loss: 5.6394e-06\n",
      "Epoch 6/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.3186e-06 - val_loss: 5.6573e-06\n",
      "Epoch 7/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.2985e-06 - val_loss: 5.6795e-06\n",
      "Epoch 8/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.2817e-06 - val_loss: 5.6638e-06\n",
      "Epoch 9/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.2677e-06 - val_loss: 5.6806e-06\n",
      "Epoch 10/10\n",
      "99998/99998 [==============================] - 3s 32us/step - loss: 4.2558e-06 - val_loss: 5.6945e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.6033e-06 - val_loss: 5.6590e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.4527e-06 - val_loss: 5.6677e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3870e-06 - val_loss: 5.6705e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.3471e-06 - val_loss: 5.6418e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3188e-06 - val_loss: 5.6561e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.2964e-06 - val_loss: 5.6590e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.2794e-06 - val_loss: 5.6733e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.2621e-06 - val_loss: 5.6665e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2486e-06 - val_loss: 5.7008e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2387e-06 - val_loss: 5.6924e-06\n",
      "Train on 1503 samples, validate on 1914 samples\n",
      "Epoch 1/10\n",
      "1503/1503 [==============================] - 0s 42us/step - loss: 4.6358e-06 - val_loss: 5.4568e-06\n",
      "Epoch 2/10\n",
      "1503/1503 [==============================] - 0s 38us/step - loss: 4.4184e-06 - val_loss: 5.5041e-06\n",
      "Epoch 3/10\n",
      "1503/1503 [==============================] - 0s 38us/step - loss: 4.2967e-06 - val_loss: 5.5564e-06\n",
      "Epoch 4/10\n",
      "1503/1503 [==============================] - 0s 36us/step - loss: 4.2100e-06 - val_loss: 5.5294e-06\n",
      "Epoch 5/10\n",
      "1503/1503 [==============================] - 0s 37us/step - loss: 4.1470e-06 - val_loss: 5.5470e-06\n",
      "Epoch 6/10\n",
      "1503/1503 [==============================] - 0s 39us/step - loss: 4.0872e-06 - val_loss: 5.5741e-06\n",
      "Epoch 7/10\n",
      "1503/1503 [==============================] - 0s 39us/step - loss: 4.0376e-06 - val_loss: 5.5563e-06\n",
      "Epoch 8/10\n",
      "1503/1503 [==============================] - 0s 40us/step - loss: 3.9994e-06 - val_loss: 5.5698e-06\n",
      "Epoch 9/10\n",
      "1503/1503 [==============================] - 0s 38us/step - loss: 3.9578e-06 - val_loss: 5.6076e-06\n",
      "Epoch 10/10\n",
      "1503/1503 [==============================] - 0s 39us/step - loss: 3.9248e-06 - val_loss: 5.5953e-06\n",
      "Train on 99999 samples, validate on 99996 samples\n",
      "Epoch 1/10\n",
      "99999/99999 [==============================] - 3s 34us/step - loss: 4.5822e-06 - val_loss: 5.6065e-06\n",
      "Epoch 2/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.4141e-06 - val_loss: 5.5939e-06\n",
      "Epoch 3/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3531e-06 - val_loss: 5.6033e-06\n",
      "Epoch 4/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.3093e-06 - val_loss: 5.6047e-06\n",
      "Epoch 5/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2793e-06 - val_loss: 5.6168e-06\n",
      "Epoch 6/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2596e-06 - val_loss: 5.6085e-06\n",
      "Epoch 7/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2403e-06 - val_loss: 5.6366e-06\n",
      "Epoch 8/10\n",
      "99999/99999 [==============================] - 3s 33us/step - loss: 4.2265e-06 - val_loss: 5.6233e-06\n",
      "Epoch 9/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2128e-06 - val_loss: 5.6482e-06\n",
      "Epoch 10/10\n",
      "99999/99999 [==============================] - 3s 32us/step - loss: 4.2000e-06 - val_loss: 5.6518e-06\n"
     ]
    }
   ],
   "source": [
    "%cd /gdrive/My Drive/S2\n",
    "#autoencoder = keras.models.load_model(ModelName)\n",
    "#!ls ./TOP/TRAIN/*.*.bin.xz\n",
    "#\n",
    "\n",
    "for e in range(20):\n",
    "  for i in range(7):\n",
    "    TrainOnFile(NAME(\"QCD\",\"TRAIN\",i%7,\"out\"),NAME(\"QCD\",\"TEST\",i%3,\"out\"),10)\n",
    "  #\n",
    "  for i in range(3):\n",
    "    TrainOnFile(NAME(\"QCD\",\"VAL\",i%7,\"out\"),NAME(\"QCD\",\"TEST\",i%3,\"out\"),10)\n",
    "  #\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2g8Vj4vc2XID"
   },
   "source": [
    "# Evaluation using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "NFWoSnh1e3o5",
    "outputId": "f191cfe2-5b55-4ffd-c3a8-dd53c373317a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999,)\n",
      "(100000,)\n",
      "(99999,)\n",
      "(100000,)\n",
      "(99998,)\n",
      "(100000,)\n",
      "(99996,)\n",
      "(100000,)\n",
      "(100000,)\n",
      "(100000,)\n",
      "(99998,)\n",
      "(100000,)\n",
      "(5523,)\n",
      "(5477,)\n",
      "(99996,)\n",
      "(100000,)\n",
      "(99998,)\n",
      "(100000,)\n",
      "(99996,)\n",
      "(100000,)\n",
      "(99999,)\n",
      "(100000,)\n",
      "(1914,)\n",
      "(2086,)\n",
      "(1503,)\n",
      "(1497,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(7) :\n",
    "  EvalOnFile(NAME(\"QCD\",\"TRAIN\",i,\"out\"),NAME(\"QCD\",\"TRAIN\",i,\"loss\"))\n",
    "  EvalOnFile(NAME(\"TOP\",\"TRAIN\",i,\"out\"),NAME(\"TOP\",\"TRAIN\",i,\"loss\"))\n",
    "#\n",
    "for i in range(3) :\n",
    "  EvalOnFile(NAME(\"QCD\",\"TEST\",i,\"out\"),NAME(\"QCD\",\"TEST\",i,\"loss\"))\n",
    "  EvalOnFile(NAME(\"TOP\",\"TEST\",i,\"out\"),NAME(\"TOP\",\"TEST\",i,\"loss\"))\n",
    "  EvalOnFile(NAME(\"QCD\",\"VAL\",i,\"out\"),NAME(\"QCD\",\"VAL\",i,\"loss\"))\n",
    "  EvalOnFile(NAME(\"TOP\",\"VAL\",i,\"out\"),NAME(\"TOP\",\"VAL\",i,\"loss\"))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIfGiATk2biQ"
   },
   "source": [
    "# Read the important data\n",
    "\n",
    "Here, we read the autoencoder loss, jet mass and nsubjettiness (mass and nsubjettiness were evaluated using the C programs `main.cc`, `main.hh` and `all.hh` found in [https://github.com/aravindhv10/CPP_Wrappers/tree/master/AntiQCD4/ML4JETS] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JiwgaLVZbuQb"
   },
   "outputs": [],
   "source": [
    "def ReadLossMassNsub(eventtype,sampletype,i):\n",
    "  loss = np.fromfile(NAME(eventtype,sampletype,i,\"loss\"), dtype=float)\n",
    "  mass = READ_XZ(NAME(eventtype,sampletype,i,\"mass\"))\n",
    "  nsub = READ_XZ(NAME(eventtype,sampletype,i,\"nsub\")).reshape(-1,5)\n",
    "  #print(nsub.shape)\n",
    "  out = np.ones((mass.shape[0],7))\n",
    "  for i in range(mass.shape[0]):\n",
    "    out[i][0] = loss[i]\n",
    "    out[i][1] = mass[i]\n",
    "    out[i][2] = nsub[i][0]\n",
    "    out[i][3] = nsub[i][1]\n",
    "    out[i][4] = nsub[i][2]\n",
    "    out[i][5] = nsub[i][3]\n",
    "    out[i][6] = nsub[i][4]\n",
    "  #\n",
    "  return out\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLENhh9Qc34c"
   },
   "outputs": [],
   "source": [
    "vars_qcd_train = ReadLossMassNsub(\"QCD\",\"TRAIN\",0)\n",
    "vars_qcd_train = np.append (vars_qcd_train,ReadLossMassNsub(\"QCD\",\"TRAIN\",1),0)\n",
    "vars_qcd_train = np.append (vars_qcd_train,ReadLossMassNsub(\"QCD\",\"TRAIN\",2),0)\n",
    "vars_qcd_train = np.append (vars_qcd_train,ReadLossMassNsub(\"QCD\",\"TRAIN\",3),0)\n",
    "vars_qcd_train = np.append (vars_qcd_train,ReadLossMassNsub(\"QCD\",\"TRAIN\",4),0)\n",
    "vars_qcd_train = np.append (vars_qcd_train,ReadLossMassNsub(\"QCD\",\"TRAIN\",5),0)\n",
    "vars_qcd_train = np.append (vars_qcd_train,ReadLossMassNsub(\"QCD\",\"TRAIN\",6),0)\n",
    "\n",
    "vars_qcd_test = ReadLossMassNsub(\"QCD\",\"TEST\",0)\n",
    "vars_qcd_test = np.append (vars_qcd_test,ReadLossMassNsub(\"QCD\",\"TEST\",1),0)\n",
    "vars_qcd_test = np.append (vars_qcd_test,ReadLossMassNsub(\"QCD\",\"TEST\",2),0)\n",
    "\n",
    "vars_qcd_val = ReadLossMassNsub(\"QCD\",\"VAL\",0)\n",
    "vars_qcd_val = np.append (vars_qcd_val,ReadLossMassNsub(\"QCD\",\"VAL\",1),0)\n",
    "vars_qcd_val = np.append (vars_qcd_val,ReadLossMassNsub(\"QCD\",\"VAL\",2),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnI-GaFbdEb8"
   },
   "outputs": [],
   "source": [
    "vars_top_train = ReadLossMassNsub(\"TOP\",\"TRAIN\",0)\n",
    "vars_top_train = np.append (vars_top_train,ReadLossMassNsub(\"TOP\",\"TRAIN\",1),0)\n",
    "vars_top_train = np.append (vars_top_train,ReadLossMassNsub(\"TOP\",\"TRAIN\",2),0)\n",
    "vars_top_train = np.append (vars_top_train,ReadLossMassNsub(\"TOP\",\"TRAIN\",3),0)\n",
    "vars_top_train = np.append (vars_top_train,ReadLossMassNsub(\"TOP\",\"TRAIN\",4),0)\n",
    "vars_top_train = np.append (vars_top_train,ReadLossMassNsub(\"TOP\",\"TRAIN\",5),0)\n",
    "vars_top_train = np.append (vars_top_train,ReadLossMassNsub(\"TOP\",\"TRAIN\",6),0)\n",
    "\n",
    "vars_top_test = ReadLossMassNsub(\"TOP\",\"TEST\",0)\n",
    "vars_top_test = np.append (vars_top_test,ReadLossMassNsub(\"TOP\",\"TEST\",1),0)\n",
    "vars_top_test = np.append (vars_top_test,ReadLossMassNsub(\"TOP\",\"TEST\",2),0)\n",
    "\n",
    "vars_top_val = ReadLossMassNsub(\"TOP\",\"VAL\",0)\n",
    "vars_top_val = np.append (vars_top_val,ReadLossMassNsub(\"TOP\",\"VAL\",1),0)\n",
    "vars_top_val = np.append (vars_top_val,ReadLossMassNsub(\"TOP\",\"VAL\",2),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MrYzbpxR27dZ"
   },
   "source": [
    "# Plotting and checking\n",
    "\n",
    "## Plot $\\epsilon$ (autoencoder loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "n3lmANBcdZtE",
    "outputId": "3823252d-7391-4dc8-bd99-eb7a0bf6ae67"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vars_qcd_test[:,0],100,(0.0,0.4),density=True,histtype='step')\n",
    "plt.hist(vars_top_test[:,0],100,(0.0,0.4),density=True,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFUJ5PJv3A5m"
   },
   "source": [
    "## Plot $m_J$ (jet mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "jEafHlUniZI7",
    "outputId": "92e351e5-feb5-4018-c039-47ba83406c22"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vars_qcd_test[:,1],100,(0.0,1000),density=True,histtype='step')\n",
    "plt.hist(vars_top_test[:,1],100,(0.0,1000),density=True,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5shkm7uX3DI9"
   },
   "source": [
    "## Plot jet $\\tau_1$ (nsubjettiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "iKCDWGRwvcOz",
    "outputId": "34148980-d20b-492f-b1b1-673e0ad7e0f0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vars_qcd_test[:,2],100,(0.0,100),density=True,histtype='step')\n",
    "plt.hist(vars_top_test[:,2],100,(0.0,100),density=True,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEuUz8dk3N07"
   },
   "source": [
    "## Plot jet $\\tau_2$ (nsubjettiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "wC4VDbMYv6V4",
    "outputId": "72342d94-616e-4856-f92e-c593ec261ac3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vars_qcd_test[:,3],100,(0.0,100),density=True,histtype='step')\n",
    "plt.hist(vars_top_test[:,3],100,(0.0,100),density=True,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8_LU2Wr3S7b"
   },
   "source": [
    "## Plot jet $\\tau_3$ (nsubjettiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "Xx4nm026v9SQ",
    "outputId": "baed003c-0d96-4f9e-b12b-623e07e0a333"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vars_qcd_test[:,4],100,(0.0,100),density=True,histtype='step')\n",
    "plt.hist(vars_top_test[:,4],100,(0.0,100),density=True,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJszVqNh3UXa"
   },
   "source": [
    "## Plot jet $\\tau_4$ (nsubjettiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "k3spZSrIwDbQ",
    "outputId": "a7d420dc-b341-4b9e-9633-c24548c86073"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vars_qcd_test[:,5],100,(0.0,100),density=True,histtype='step')\n",
    "plt.hist(vars_top_test[:,5],100,(0.0,100),density=True,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ck9o4aWB3h1v"
   },
   "source": [
    "# Plot ROC using only $\\epsilon$\n",
    "\n",
    "X axis is $\\epsilon_t$\n",
    "\n",
    "Y axis is $\\frac{1}{\\epsilon_{\\text{QCD}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "4tSY-2Fqirg6",
    "outputId": "1c8d48c5-16f6-467b-d33d-164bd9bf3fff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "[<matplotlib.lines.Line2D object at 0x7f83f96ba2b0>]"
      ],
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f83f96ba2b0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dx = (0.4 - 0.0) / 100.0\n",
    "qcdeff = np.ones((100))\n",
    "topeff = np.ones((100))\n",
    "for i in range(100):\n",
    "  xval = i*dx\n",
    "  qcdeff[i]=1.0/(Count(vars_qcd_test[:,0],xval)+0.0000000001)\n",
    "  topeff[i]=Count(vars_top_test[:,0],xval)\n",
    "plt.yscale('log')\n",
    "plt.plot(topeff,qcdeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTlGmaiT32jP"
   },
   "source": [
    "# Combining variables\n",
    "We now repeat the ROC exercise using boosted decision trees to combine the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjV-4co8jr3U"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def prepare (qcd_vars,top_vars) :\n",
    "  out_x = np.append(qcd_vars,top_vars,0)\n",
    "  out_y = np.append(np.zeros((qcd_vars.shape[0]),dtype='float32'),np.ones((top_vars.shape[0]),dtype='float32'),0)\n",
    "  return sklearn.utils.shuffle ( out_x , out_y , random_state=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N27Dp0BkjI4i"
   },
   "outputs": [],
   "source": [
    "train_x, train_y = prepare(vars_qcd_train,vars_top_train)\n",
    "test_x, test_y = prepare(vars_qcd_test,vars_top_test)\n",
    "val_x, val_y = prepare(vars_qcd_val,vars_top_val)\n",
    "param = { 'objective':'binary' , 'metric':'auc,binary_logloss,binary_error' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "mRwNJXDexrl5",
    "outputId": "ac1284d0-f4eb-4284-bc41-4d25a82fc807"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_x[:,0],100,(0.0,0.4),density=True,histtype='step')\n",
    "plt.hist(test_x[:,0],100,(0.0,0.4),density=True,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "fXfQ0Af3xRHi",
    "outputId": "5e38dbfe-1228-4746-c297-5521651a3e76"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_x[:,1],100,(0.0,1000),density=True,histtype='step')\n",
    "plt.hist(test_x[:,1],100,(0.0,1000),density=True,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IVQ8vKs4c4u"
   },
   "source": [
    "# Decision trees using only autoencoder loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "a2yToT0TmyFU",
    "outputId": "2003ce1d-9131-4fce-cc3d-2468068d09a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.662144\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833258\n",
      "[2]\tvalid_0's binary_logloss: 0.636785\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83344\n",
      "[3]\tvalid_0's binary_logloss: 0.615826\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833473\n",
      "[4]\tvalid_0's binary_logloss: 0.598368\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83349\n",
      "[5]\tvalid_0's binary_logloss: 0.583739\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83351\n",
      "[6]\tvalid_0's binary_logloss: 0.571426\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833528\n",
      "[7]\tvalid_0's binary_logloss: 0.561019\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833539\n",
      "[8]\tvalid_0's binary_logloss: 0.552206\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833541\n",
      "[9]\tvalid_0's binary_logloss: 0.544718\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833547\n",
      "[10]\tvalid_0's binary_logloss: 0.538346\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833555\n",
      "[11]\tvalid_0's binary_logloss: 0.532917\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833557\n",
      "[12]\tvalid_0's binary_logloss: 0.528285\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833558\n",
      "[13]\tvalid_0's binary_logloss: 0.524331\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833562\n",
      "[14]\tvalid_0's binary_logloss: 0.520951\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833563\n",
      "[15]\tvalid_0's binary_logloss: 0.518061\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833564\n",
      "[16]\tvalid_0's binary_logloss: 0.515588\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833565\n",
      "[17]\tvalid_0's binary_logloss: 0.513472\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833565\n",
      "[18]\tvalid_0's binary_logloss: 0.511662\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833559\n",
      "[19]\tvalid_0's binary_logloss: 0.510111\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833559\n",
      "[20]\tvalid_0's binary_logloss: 0.508783\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833558\n",
      "[21]\tvalid_0's binary_logloss: 0.507644\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833558\n",
      "[22]\tvalid_0's binary_logloss: 0.506669\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833558\n",
      "[23]\tvalid_0's binary_logloss: 0.505831\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833558\n",
      "[24]\tvalid_0's binary_logloss: 0.505112\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833556\n",
      "[25]\tvalid_0's binary_logloss: 0.504496\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833556\n",
      "[26]\tvalid_0's binary_logloss: 0.503967\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833555\n",
      "[27]\tvalid_0's binary_logloss: 0.503512\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833556\n",
      "[28]\tvalid_0's binary_logloss: 0.503122\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833554\n",
      "[29]\tvalid_0's binary_logloss: 0.502787\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833554\n",
      "[30]\tvalid_0's binary_logloss: 0.502498\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833555\n",
      "[31]\tvalid_0's binary_logloss: 0.50225\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833555\n",
      "[32]\tvalid_0's binary_logloss: 0.502036\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833555\n",
      "[33]\tvalid_0's binary_logloss: 0.501851\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833558\n",
      "[34]\tvalid_0's binary_logloss: 0.501691\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833559\n",
      "[35]\tvalid_0's binary_logloss: 0.501555\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833559\n",
      "[36]\tvalid_0's binary_logloss: 0.501437\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833559\n",
      "[37]\tvalid_0's binary_logloss: 0.501335\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833564\n",
      "[38]\tvalid_0's binary_logloss: 0.501246\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833564\n",
      "[39]\tvalid_0's binary_logloss: 0.50117\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833562\n",
      "[40]\tvalid_0's binary_logloss: 0.501104\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83356\n",
      "[41]\tvalid_0's binary_logloss: 0.501046\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833559\n",
      "[42]\tvalid_0's binary_logloss: 0.500997\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833557\n",
      "[43]\tvalid_0's binary_logloss: 0.500954\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833555\n",
      "[44]\tvalid_0's binary_logloss: 0.500915\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833555\n",
      "[45]\tvalid_0's binary_logloss: 0.500882\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833554\n",
      "[46]\tvalid_0's binary_logloss: 0.500852\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833553\n",
      "[47]\tvalid_0's binary_logloss: 0.500827\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833553\n",
      "[48]\tvalid_0's binary_logloss: 0.500806\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833552\n",
      "[49]\tvalid_0's binary_logloss: 0.500787\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833552\n",
      "[50]\tvalid_0's binary_logloss: 0.500771\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83355\n",
      "[51]\tvalid_0's binary_logloss: 0.500756\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83355\n",
      "[52]\tvalid_0's binary_logloss: 0.500744\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83355\n",
      "[53]\tvalid_0's binary_logloss: 0.500733\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83355\n",
      "[54]\tvalid_0's binary_logloss: 0.500722\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83355\n",
      "[55]\tvalid_0's binary_logloss: 0.500714\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833548\n",
      "[56]\tvalid_0's binary_logloss: 0.500707\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833543\n",
      "[57]\tvalid_0's binary_logloss: 0.5007\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833542\n",
      "[58]\tvalid_0's binary_logloss: 0.500694\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83354\n",
      "[59]\tvalid_0's binary_logloss: 0.500689\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833539\n",
      "[60]\tvalid_0's binary_logloss: 0.500685\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83354\n",
      "[61]\tvalid_0's binary_logloss: 0.50068\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833542\n",
      "[62]\tvalid_0's binary_logloss: 0.500677\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83354\n",
      "[63]\tvalid_0's binary_logloss: 0.500674\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833539\n",
      "[64]\tvalid_0's binary_logloss: 0.500672\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833539\n",
      "[65]\tvalid_0's binary_logloss: 0.50067\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833539\n",
      "[66]\tvalid_0's binary_logloss: 0.500668\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833539\n",
      "[67]\tvalid_0's binary_logloss: 0.500667\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833538\n",
      "[68]\tvalid_0's binary_logloss: 0.500665\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833539\n",
      "[69]\tvalid_0's binary_logloss: 0.500664\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833538\n",
      "[70]\tvalid_0's binary_logloss: 0.500663\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833537\n",
      "[71]\tvalid_0's binary_logloss: 0.500662\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833537\n",
      "[72]\tvalid_0's binary_logloss: 0.500661\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833537\n",
      "[73]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833537\n",
      "[74]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833536\n",
      "[75]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833534\n",
      "[76]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833532\n",
      "[77]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833531\n",
      "[78]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833531\n",
      "[79]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83353\n",
      "[80]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.83353\n",
      "[81]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833528\n",
      "[82]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833528\n",
      "[83]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833526\n",
      "[84]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833526\n",
      "[85]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833526\n",
      "[86]\tvalid_0's binary_logloss: 0.500659\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833525\n",
      "[87]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833525\n",
      "[88]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833525\n",
      "[89]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833524\n",
      "[90]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833524\n",
      "[91]\tvalid_0's binary_logloss: 0.50066\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833525\n",
      "[92]\tvalid_0's binary_logloss: 0.500661\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833525\n",
      "[93]\tvalid_0's binary_logloss: 0.500661\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833525\n",
      "[94]\tvalid_0's binary_logloss: 0.500661\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833525\n",
      "[95]\tvalid_0's binary_logloss: 0.500661\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833525\n",
      "[96]\tvalid_0's binary_logloss: 0.500661\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833524\n",
      "[97]\tvalid_0's binary_logloss: 0.500662\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833524\n",
      "[98]\tvalid_0's binary_logloss: 0.500662\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833524\n",
      "[99]\tvalid_0's binary_logloss: 0.500662\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833521\n",
      "[100]\tvalid_0's binary_logloss: 0.500662\tvalid_0's binary_error: 0.246362\tvalid_0's auc: 0.833521\n"
     ]
    }
   ],
   "source": [
    "num_round = 100\n",
    "#train_data = lgb.Dataset( train_x[:,0:0] , label=train_y )\n",
    "#val_data   = lgb.Dataset( val_x[:,0:0]   , label=val_y   )\n",
    "train_data = lgb.Dataset( train_x[:,0].reshape((-1,1)) , label=train_y )\n",
    "val_data   = lgb.Dataset( val_x[:,0].reshape((-1,1))   , label=val_y   )\n",
    "bst = lgb.train(param, train_data, num_round, valid_sets=val_data)\n",
    "pred_qcd_test = bst.predict(vars_qcd_test[:,0].reshape((-1,1)))\n",
    "pred_top_test = bst.predict(vars_top_test[:,0].reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unZAP76l4llm"
   },
   "source": [
    "## Plot the ROC from the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "coodzKuxz4W3",
    "outputId": "be78ef2d-7991-4a85-edea-2ebc5774e8a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "[<matplotlib.lines.Line2D object at 0x7f83f94876a0>]"
      ],
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f83f94876a0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = 0.0000001\n",
    "num = 1000\n",
    "dx = ( 1.0 + (epsilon*2) ) / num\n",
    "qcdeff_loss = np.ones((num))\n",
    "topeff_loss = np.ones((num))\n",
    "for i in range(num):\n",
    "  xval = (i*dx) - epsilon\n",
    "  qcdeff_loss[i]=1.0/(Count(pred_qcd_test,xval)+epsilon)\n",
    "  topeff_loss[i]=Count(pred_top_test,xval)\n",
    "plt.yscale('log')\n",
    "plt.plot(topeff_loss,qcdeff_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jchqyNs56-QB"
   },
   "source": [
    "# Train BDT using all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5z0Rws0kocE2",
    "outputId": "0e717888-ead8-4d76-939c-30f22c6a72ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.622961\tvalid_0's binary_error: 0.0853604\tvalid_0's auc: 0.966623\n",
      "[2]\tvalid_0's binary_logloss: 0.565327\tvalid_0's binary_error: 0.0832066\tvalid_0's auc: 0.96846\n",
      "[3]\tvalid_0's binary_logloss: 0.517087\tvalid_0's binary_error: 0.0829262\tvalid_0's auc: 0.970094\n",
      "[4]\tvalid_0's binary_logloss: 0.476226\tvalid_0's binary_error: 0.0813306\tvalid_0's auc: 0.971207\n",
      "[5]\tvalid_0's binary_logloss: 0.441305\tvalid_0's binary_error: 0.0809361\tvalid_0's auc: 0.972396\n",
      "[6]\tvalid_0's binary_logloss: 0.411219\tvalid_0's binary_error: 0.0803058\tvalid_0's auc: 0.97309\n",
      "[7]\tvalid_0's binary_logloss: 0.385232\tvalid_0's binary_error: 0.0796929\tvalid_0's auc: 0.973353\n",
      "[8]\tvalid_0's binary_logloss: 0.362555\tvalid_0's binary_error: 0.0791569\tvalid_0's auc: 0.973636\n",
      "[9]\tvalid_0's binary_logloss: 0.342904\tvalid_0's binary_error: 0.0791941\tvalid_0's auc: 0.973917\n",
      "[10]\tvalid_0's binary_logloss: 0.325526\tvalid_0's binary_error: 0.078807\tvalid_0's auc: 0.974135\n",
      "[11]\tvalid_0's binary_logloss: 0.310303\tvalid_0's binary_error: 0.0784621\tvalid_0's auc: 0.974332\n",
      "[12]\tvalid_0's binary_logloss: 0.296732\tvalid_0's binary_error: 0.0782115\tvalid_0's auc: 0.974586\n",
      "[13]\tvalid_0's binary_logloss: 0.28463\tvalid_0's binary_error: 0.077817\tvalid_0's auc: 0.974828\n",
      "[14]\tvalid_0's binary_logloss: 0.273872\tvalid_0's binary_error: 0.0775539\tvalid_0's auc: 0.975217\n",
      "[15]\tvalid_0's binary_logloss: 0.264532\tvalid_0's binary_error: 0.0774224\tvalid_0's auc: 0.975428\n",
      "[16]\tvalid_0's binary_logloss: 0.256024\tvalid_0's binary_error: 0.0771991\tvalid_0's auc: 0.975675\n",
      "[17]\tvalid_0's binary_logloss: 0.248632\tvalid_0's binary_error: 0.0769832\tvalid_0's auc: 0.975844\n",
      "[18]\tvalid_0's binary_logloss: 0.241924\tvalid_0's binary_error: 0.0767772\tvalid_0's auc: 0.976018\n",
      "[19]\tvalid_0's binary_logloss: 0.235924\tvalid_0's binary_error: 0.0766581\tvalid_0's auc: 0.976206\n",
      "[20]\tvalid_0's binary_logloss: 0.230643\tvalid_0's binary_error: 0.0764522\tvalid_0's auc: 0.976311\n",
      "[21]\tvalid_0's binary_logloss: 0.225873\tvalid_0's binary_error: 0.0763678\tvalid_0's auc: 0.976535\n",
      "[22]\tvalid_0's binary_logloss: 0.221596\tvalid_0's binary_error: 0.0761619\tvalid_0's auc: 0.976701\n",
      "[23]\tvalid_0's binary_logloss: 0.217756\tvalid_0's binary_error: 0.0760427\tvalid_0's auc: 0.976898\n",
      "[24]\tvalid_0's binary_logloss: 0.214328\tvalid_0's binary_error: 0.0759286\tvalid_0's auc: 0.976975\n",
      "[25]\tvalid_0's binary_logloss: 0.211155\tvalid_0's binary_error: 0.0757475\tvalid_0's auc: 0.977093\n",
      "[26]\tvalid_0's binary_logloss: 0.208335\tvalid_0's binary_error: 0.0756209\tvalid_0's auc: 0.977201\n",
      "[27]\tvalid_0's binary_logloss: 0.205829\tvalid_0's binary_error: 0.0755514\tvalid_0's auc: 0.977285\n",
      "[28]\tvalid_0's binary_logloss: 0.203477\tvalid_0's binary_error: 0.0754919\tvalid_0's auc: 0.977429\n",
      "[29]\tvalid_0's binary_logloss: 0.201434\tvalid_0's binary_error: 0.0753554\tvalid_0's auc: 0.977518\n",
      "[30]\tvalid_0's binary_logloss: 0.199539\tvalid_0's binary_error: 0.0752189\tvalid_0's auc: 0.977622\n",
      "[31]\tvalid_0's binary_logloss: 0.197822\tvalid_0's binary_error: 0.0751073\tvalid_0's auc: 0.977715\n",
      "[32]\tvalid_0's binary_logloss: 0.196356\tvalid_0's binary_error: 0.0749633\tvalid_0's auc: 0.977761\n",
      "[33]\tvalid_0's binary_logloss: 0.19492\tvalid_0's binary_error: 0.0749882\tvalid_0's auc: 0.977857\n",
      "[34]\tvalid_0's binary_logloss: 0.193638\tvalid_0's binary_error: 0.0748591\tvalid_0's auc: 0.977914\n",
      "[35]\tvalid_0's binary_logloss: 0.192526\tvalid_0's binary_error: 0.074745\tvalid_0's auc: 0.97795\n",
      "[36]\tvalid_0's binary_logloss: 0.191441\tvalid_0's binary_error: 0.0746581\tvalid_0's auc: 0.978021\n",
      "[37]\tvalid_0's binary_logloss: 0.190486\tvalid_0's binary_error: 0.0745762\tvalid_0's auc: 0.978103\n",
      "[38]\tvalid_0's binary_logloss: 0.189625\tvalid_0's binary_error: 0.0745341\tvalid_0's auc: 0.978164\n",
      "[39]\tvalid_0's binary_logloss: 0.188876\tvalid_0's binary_error: 0.0744695\tvalid_0's auc: 0.9782\n",
      "[40]\tvalid_0's binary_logloss: 0.188205\tvalid_0's binary_error: 0.0744125\tvalid_0's auc: 0.978227\n",
      "[41]\tvalid_0's binary_logloss: 0.18754\tvalid_0's binary_error: 0.0743628\tvalid_0's auc: 0.978278\n",
      "[42]\tvalid_0's binary_logloss: 0.186977\tvalid_0's binary_error: 0.0743455\tvalid_0's auc: 0.978304\n",
      "[43]\tvalid_0's binary_logloss: 0.186437\tvalid_0's binary_error: 0.0742561\tvalid_0's auc: 0.978348\n",
      "[44]\tvalid_0's binary_logloss: 0.185954\tvalid_0's binary_error: 0.074271\tvalid_0's auc: 0.978383\n",
      "[45]\tvalid_0's binary_logloss: 0.185535\tvalid_0's binary_error: 0.0742661\tvalid_0's auc: 0.978402\n",
      "[46]\tvalid_0's binary_logloss: 0.185129\tvalid_0's binary_error: 0.0742586\tvalid_0's auc: 0.978432\n",
      "[47]\tvalid_0's binary_logloss: 0.184756\tvalid_0's binary_error: 0.0742338\tvalid_0's auc: 0.978462\n",
      "[48]\tvalid_0's binary_logloss: 0.184447\tvalid_0's binary_error: 0.074214\tvalid_0's auc: 0.978476\n",
      "[49]\tvalid_0's binary_logloss: 0.184149\tvalid_0's binary_error: 0.074209\tvalid_0's auc: 0.978494\n",
      "[50]\tvalid_0's binary_logloss: 0.183872\tvalid_0's binary_error: 0.0741941\tvalid_0's auc: 0.978518\n",
      "[51]\tvalid_0's binary_logloss: 0.183611\tvalid_0's binary_error: 0.0741643\tvalid_0's auc: 0.978544\n",
      "[52]\tvalid_0's binary_logloss: 0.183399\tvalid_0's binary_error: 0.0741643\tvalid_0's auc: 0.978558\n",
      "[53]\tvalid_0's binary_logloss: 0.183186\tvalid_0's binary_error: 0.0741246\tvalid_0's auc: 0.978573\n",
      "[54]\tvalid_0's binary_logloss: 0.183004\tvalid_0's binary_error: 0.0741122\tvalid_0's auc: 0.978589\n",
      "[55]\tvalid_0's binary_logloss: 0.182805\tvalid_0's binary_error: 0.0740551\tvalid_0's auc: 0.978609\n",
      "[56]\tvalid_0's binary_logloss: 0.182641\tvalid_0's binary_error: 0.0740452\tvalid_0's auc: 0.978626\n",
      "[57]\tvalid_0's binary_logloss: 0.182478\tvalid_0's binary_error: 0.0739857\tvalid_0's auc: 0.978651\n",
      "[58]\tvalid_0's binary_logloss: 0.182339\tvalid_0's binary_error: 0.0739584\tvalid_0's auc: 0.978666\n",
      "[59]\tvalid_0's binary_logloss: 0.182201\tvalid_0's binary_error: 0.073941\tvalid_0's auc: 0.978682\n",
      "[60]\tvalid_0's binary_logloss: 0.182081\tvalid_0's binary_error: 0.0739336\tvalid_0's auc: 0.978697\n",
      "[61]\tvalid_0's binary_logloss: 0.181958\tvalid_0's binary_error: 0.0738914\tvalid_0's auc: 0.978712\n",
      "[62]\tvalid_0's binary_logloss: 0.181858\tvalid_0's binary_error: 0.0738889\tvalid_0's auc: 0.978719\n",
      "[63]\tvalid_0's binary_logloss: 0.181773\tvalid_0's binary_error: 0.073874\tvalid_0's auc: 0.978728\n",
      "[64]\tvalid_0's binary_logloss: 0.181676\tvalid_0's binary_error: 0.0738591\tvalid_0's auc: 0.978738\n",
      "[65]\tvalid_0's binary_logloss: 0.18159\tvalid_0's binary_error: 0.0738541\tvalid_0's auc: 0.978752\n",
      "[66]\tvalid_0's binary_logloss: 0.181495\tvalid_0's binary_error: 0.0738393\tvalid_0's auc: 0.978764\n",
      "[67]\tvalid_0's binary_logloss: 0.181426\tvalid_0's binary_error: 0.0737996\tvalid_0's auc: 0.978773\n",
      "[68]\tvalid_0's binary_logloss: 0.18136\tvalid_0's binary_error: 0.0737797\tvalid_0's auc: 0.978779\n",
      "[69]\tvalid_0's binary_logloss: 0.181303\tvalid_0's binary_error: 0.0737648\tvalid_0's auc: 0.978786\n",
      "[70]\tvalid_0's binary_logloss: 0.181231\tvalid_0's binary_error: 0.0737425\tvalid_0's auc: 0.978799\n",
      "[71]\tvalid_0's binary_logloss: 0.181174\tvalid_0's binary_error: 0.0737251\tvalid_0's auc: 0.978808\n",
      "[72]\tvalid_0's binary_logloss: 0.181098\tvalid_0's binary_error: 0.0737276\tvalid_0's auc: 0.978821\n",
      "[73]\tvalid_0's binary_logloss: 0.181046\tvalid_0's binary_error: 0.0737003\tvalid_0's auc: 0.97883\n",
      "[74]\tvalid_0's binary_logloss: 0.180995\tvalid_0's binary_error: 0.0736953\tvalid_0's auc: 0.97884\n",
      "[75]\tvalid_0's binary_logloss: 0.180965\tvalid_0's binary_error: 0.0736978\tvalid_0's auc: 0.978844\n",
      "[76]\tvalid_0's binary_logloss: 0.180917\tvalid_0's binary_error: 0.0737028\tvalid_0's auc: 0.978854\n",
      "[77]\tvalid_0's binary_logloss: 0.180876\tvalid_0's binary_error: 0.073673\tvalid_0's auc: 0.978859\n",
      "[78]\tvalid_0's binary_logloss: 0.18098\tvalid_0's binary_error: 0.0737499\tvalid_0's auc: 0.978826\n",
      "[79]\tvalid_0's binary_logloss: 0.180919\tvalid_0's binary_error: 0.0737276\tvalid_0's auc: 0.978843\n",
      "[80]\tvalid_0's binary_logloss: 0.180902\tvalid_0's binary_error: 0.0737226\tvalid_0's auc: 0.978845\n",
      "[81]\tvalid_0's binary_logloss: 0.180851\tvalid_0's binary_error: 0.0737003\tvalid_0's auc: 0.978854\n",
      "[82]\tvalid_0's binary_logloss: 0.18085\tvalid_0's binary_error: 0.0737177\tvalid_0's auc: 0.978858\n",
      "[83]\tvalid_0's binary_logloss: 0.181185\tvalid_0's binary_error: 0.073735\tvalid_0's auc: 0.978789\n",
      "[84]\tvalid_0's binary_logloss: 0.180873\tvalid_0's binary_error: 0.0736929\tvalid_0's auc: 0.978848\n",
      "[85]\tvalid_0's binary_logloss: 0.181682\tvalid_0's binary_error: 0.0737847\tvalid_0's auc: 0.97869\n",
      "[86]\tvalid_0's binary_logloss: 0.180951\tvalid_0's binary_error: 0.0736953\tvalid_0's auc: 0.978843\n",
      "[87]\tvalid_0's binary_logloss: 0.182026\tvalid_0's binary_error: 0.0738467\tvalid_0's auc: 0.978608\n",
      "[88]\tvalid_0's binary_logloss: 0.181596\tvalid_0's binary_error: 0.0737921\tvalid_0's auc: 0.978714\n",
      "[89]\tvalid_0's binary_logloss: 0.181593\tvalid_0's binary_error: 0.0737797\tvalid_0's auc: 0.978735\n",
      "[90]\tvalid_0's binary_logloss: 0.181912\tvalid_0's binary_error: 0.0737648\tvalid_0's auc: 0.978733\n",
      "[91]\tvalid_0's binary_logloss: 0.182002\tvalid_0's binary_error: 0.0737499\tvalid_0's auc: 0.978725\n",
      "[92]\tvalid_0's binary_logloss: 0.183225\tvalid_0's binary_error: 0.0737921\tvalid_0's auc: 0.978622\n",
      "[93]\tvalid_0's binary_logloss: 0.18129\tvalid_0's binary_error: 0.0737102\tvalid_0's auc: 0.978782\n",
      "[94]\tvalid_0's binary_logloss: 0.1848\tvalid_0's binary_error: 0.0738442\tvalid_0's auc: 0.978481\n",
      "[95]\tvalid_0's binary_logloss: 0.182605\tvalid_0's binary_error: 0.0737326\tvalid_0's auc: 0.978695\n",
      "[96]\tvalid_0's binary_logloss: 0.181453\tvalid_0's binary_error: 0.0737028\tvalid_0's auc: 0.978767\n",
      "[97]\tvalid_0's binary_logloss: 0.182119\tvalid_0's binary_error: 0.0737177\tvalid_0's auc: 0.978731\n",
      "[98]\tvalid_0's binary_logloss: 0.181225\tvalid_0's binary_error: 0.0736804\tvalid_0's auc: 0.978788\n",
      "[99]\tvalid_0's binary_logloss: 0.181689\tvalid_0's binary_error: 0.073678\tvalid_0's auc: 0.978767\n",
      "[100]\tvalid_0's binary_logloss: 0.18192\tvalid_0's binary_error: 0.0736953\tvalid_0's auc: 0.978757\n"
     ]
    }
   ],
   "source": [
    "num_round = 100\n",
    "train_data = lgb.Dataset( train_x[:,0:6] , label=train_y )\n",
    "val_data   = lgb.Dataset( val_x[:,0:6]   , label=val_y   )\n",
    "bst = lgb.train(param, train_data, num_round, valid_sets=val_data)\n",
    "pred_qcd_test = bst.predict(vars_qcd_test[:,0:6])\n",
    "pred_top_test = bst.predict(vars_top_test[:,0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b5nL9zx-if4"
   },
   "source": [
    "## Plot ROC using above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "VYRrgZFUqYh3",
    "outputId": "7fa309e4-0647-4431-bcc0-61e0e9d803e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "[<matplotlib.lines.Line2D object at 0x7f83f8910198>]"
      ],
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f83f8910198>]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = 0.0000001\n",
    "num = 1000\n",
    "dx = ( 1.0 + (epsilon*2) ) / num\n",
    "qcdeff_all = np.ones((num))\n",
    "topeff_all = np.ones((num))\n",
    "for i in range(num):\n",
    "  xval = (i*dx) - epsilon\n",
    "  qcdeff_all[i]=1.0/(Count(pred_qcd_test,xval)+epsilon)\n",
    "  topeff_all[i]=Count(pred_top_test,xval)\n",
    "plt.yscale('log')\n",
    "plt.plot(topeff_all,qcdeff_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYiDEZ0dAPXJ"
   },
   "source": [
    "# Not using the autoencoder loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AJUVWnB6_qrl",
    "outputId": "450026e8-3f25-4e1f-c09e-151c1f81c186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.623859\tvalid_0's binary_error: 0.0913977\tvalid_0's auc: 0.963412\n",
      "[2]\tvalid_0's binary_logloss: 0.567135\tvalid_0's binary_error: 0.0903108\tvalid_0's auc: 0.965867\n",
      "[3]\tvalid_0's binary_logloss: 0.519743\tvalid_0's binary_error: 0.0886483\tvalid_0's auc: 0.967105\n",
      "[4]\tvalid_0's binary_logloss: 0.479773\tvalid_0's binary_error: 0.0884746\tvalid_0's auc: 0.967698\n",
      "[5]\tvalid_0's binary_logloss: 0.445674\tvalid_0's binary_error: 0.0876632\tvalid_0's auc: 0.968226\n",
      "[6]\tvalid_0's binary_logloss: 0.416379\tvalid_0's binary_error: 0.0875987\tvalid_0's auc: 0.968576\n",
      "[7]\tvalid_0's binary_logloss: 0.391039\tvalid_0's binary_error: 0.0869039\tvalid_0's auc: 0.968837\n",
      "[8]\tvalid_0's binary_logloss: 0.369023\tvalid_0's binary_error: 0.0863853\tvalid_0's auc: 0.969173\n",
      "[9]\tvalid_0's binary_logloss: 0.349887\tvalid_0's binary_error: 0.0861867\tvalid_0's auc: 0.969489\n",
      "[10]\tvalid_0's binary_logloss: 0.333154\tvalid_0's binary_error: 0.0858468\tvalid_0's auc: 0.969647\n",
      "[11]\tvalid_0's binary_logloss: 0.318567\tvalid_0's binary_error: 0.0857649\tvalid_0's auc: 0.96985\n",
      "[12]\tvalid_0's binary_logloss: 0.305561\tvalid_0's binary_error: 0.0855242\tvalid_0's auc: 0.970059\n",
      "[13]\tvalid_0's binary_logloss: 0.294193\tvalid_0's binary_error: 0.0854473\tvalid_0's auc: 0.9702\n",
      "[14]\tvalid_0's binary_logloss: 0.284185\tvalid_0's binary_error: 0.0855441\tvalid_0's auc: 0.970396\n",
      "[15]\tvalid_0's binary_logloss: 0.275267\tvalid_0's binary_error: 0.0851743\tvalid_0's auc: 0.970586\n",
      "[16]\tvalid_0's binary_logloss: 0.267416\tvalid_0's binary_error: 0.0850304\tvalid_0's auc: 0.970708\n",
      "[17]\tvalid_0's binary_logloss: 0.260334\tvalid_0's binary_error: 0.0848766\tvalid_0's auc: 0.970927\n",
      "[18]\tvalid_0's binary_logloss: 0.25409\tvalid_0's binary_error: 0.0845738\tvalid_0's auc: 0.971049\n",
      "[19]\tvalid_0's binary_logloss: 0.248504\tvalid_0's binary_error: 0.0845192\tvalid_0's auc: 0.971221\n",
      "[20]\tvalid_0's binary_logloss: 0.243633\tvalid_0's binary_error: 0.0845118\tvalid_0's auc: 0.971314\n",
      "[21]\tvalid_0's binary_logloss: 0.239249\tvalid_0's binary_error: 0.0844671\tvalid_0's auc: 0.971511\n",
      "[22]\tvalid_0's binary_logloss: 0.235288\tvalid_0's binary_error: 0.0843158\tvalid_0's auc: 0.971697\n",
      "[23]\tvalid_0's binary_logloss: 0.23172\tvalid_0's binary_error: 0.0841669\tvalid_0's auc: 0.971782\n",
      "[24]\tvalid_0's binary_logloss: 0.22857\tvalid_0's binary_error: 0.0841793\tvalid_0's auc: 0.971922\n",
      "[25]\tvalid_0's binary_logloss: 0.225771\tvalid_0's binary_error: 0.0840875\tvalid_0's auc: 0.972036\n",
      "[26]\tvalid_0's binary_logloss: 0.223256\tvalid_0's binary_error: 0.0840056\tvalid_0's auc: 0.972103\n",
      "[27]\tvalid_0's binary_logloss: 0.220897\tvalid_0's binary_error: 0.0838369\tvalid_0's auc: 0.972248\n",
      "[28]\tvalid_0's binary_logloss: 0.218888\tvalid_0's binary_error: 0.0837326\tvalid_0's auc: 0.972319\n",
      "[29]\tvalid_0's binary_logloss: 0.217088\tvalid_0's binary_error: 0.0836681\tvalid_0's auc: 0.972397\n",
      "[30]\tvalid_0's binary_logloss: 0.215382\tvalid_0's binary_error: 0.0836061\tvalid_0's auc: 0.972509\n",
      "[31]\tvalid_0's binary_logloss: 0.213933\tvalid_0's binary_error: 0.0835465\tvalid_0's auc: 0.972553\n",
      "[32]\tvalid_0's binary_logloss: 0.212598\tvalid_0's binary_error: 0.083487\tvalid_0's auc: 0.972606\n",
      "[33]\tvalid_0's binary_logloss: 0.211456\tvalid_0's binary_error: 0.0834795\tvalid_0's auc: 0.972661\n",
      "[34]\tvalid_0's binary_logloss: 0.210373\tvalid_0's binary_error: 0.0834101\tvalid_0's auc: 0.972715\n",
      "[35]\tvalid_0's binary_logloss: 0.209421\tvalid_0's binary_error: 0.0834175\tvalid_0's auc: 0.972758\n",
      "[36]\tvalid_0's binary_logloss: 0.208533\tvalid_0's binary_error: 0.0833182\tvalid_0's auc: 0.972811\n",
      "[37]\tvalid_0's binary_logloss: 0.207762\tvalid_0's binary_error: 0.0832934\tvalid_0's auc: 0.972865\n",
      "[38]\tvalid_0's binary_logloss: 0.207069\tvalid_0's binary_error: 0.0832512\tvalid_0's auc: 0.972908\n",
      "[39]\tvalid_0's binary_logloss: 0.206412\tvalid_0's binary_error: 0.0832115\tvalid_0's auc: 0.972955\n",
      "[40]\tvalid_0's binary_logloss: 0.205866\tvalid_0's binary_error: 0.0831818\tvalid_0's auc: 0.972981\n",
      "[41]\tvalid_0's binary_logloss: 0.205357\tvalid_0's binary_error: 0.0831694\tvalid_0's auc: 0.973013\n",
      "[42]\tvalid_0's binary_logloss: 0.204924\tvalid_0's binary_error: 0.0831321\tvalid_0's auc: 0.97303\n",
      "[43]\tvalid_0's binary_logloss: 0.204507\tvalid_0's binary_error: 0.0830676\tvalid_0's auc: 0.973057\n",
      "[44]\tvalid_0's binary_logloss: 0.204125\tvalid_0's binary_error: 0.0830453\tvalid_0's auc: 0.973085\n",
      "[45]\tvalid_0's binary_logloss: 0.203783\tvalid_0's binary_error: 0.0830254\tvalid_0's auc: 0.973106\n",
      "[46]\tvalid_0's binary_logloss: 0.203452\tvalid_0's binary_error: 0.0829783\tvalid_0's auc: 0.973146\n",
      "[47]\tvalid_0's binary_logloss: 0.203186\tvalid_0's binary_error: 0.0829659\tvalid_0's auc: 0.973166\n",
      "[48]\tvalid_0's binary_logloss: 0.202931\tvalid_0's binary_error: 0.0828865\tvalid_0's auc: 0.973185\n",
      "[49]\tvalid_0's binary_logloss: 0.202684\tvalid_0's binary_error: 0.0829014\tvalid_0's auc: 0.973211\n",
      "[50]\tvalid_0's binary_logloss: 0.202469\tvalid_0's binary_error: 0.0828666\tvalid_0's auc: 0.973228\n",
      "[51]\tvalid_0's binary_logloss: 0.202267\tvalid_0's binary_error: 0.0828493\tvalid_0's auc: 0.973249\n",
      "[52]\tvalid_0's binary_logloss: 0.202077\tvalid_0's binary_error: 0.0827897\tvalid_0's auc: 0.97327\n",
      "[53]\tvalid_0's binary_logloss: 0.201912\tvalid_0's binary_error: 0.0827525\tvalid_0's auc: 0.973288\n",
      "[54]\tvalid_0's binary_logloss: 0.201771\tvalid_0's binary_error: 0.0827748\tvalid_0's auc: 0.973297\n",
      "[55]\tvalid_0's binary_logloss: 0.201626\tvalid_0's binary_error: 0.0827252\tvalid_0's auc: 0.973314\n",
      "[56]\tvalid_0's binary_logloss: 0.2015\tvalid_0's binary_error: 0.0827301\tvalid_0's auc: 0.97333\n",
      "[57]\tvalid_0's binary_logloss: 0.201387\tvalid_0's binary_error: 0.0827301\tvalid_0's auc: 0.973341\n",
      "[58]\tvalid_0's binary_logloss: 0.201285\tvalid_0's binary_error: 0.0827103\tvalid_0's auc: 0.973355\n",
      "[59]\tvalid_0's binary_logloss: 0.201198\tvalid_0's binary_error: 0.082688\tvalid_0's auc: 0.973362\n",
      "[60]\tvalid_0's binary_logloss: 0.201094\tvalid_0's binary_error: 0.0826706\tvalid_0's auc: 0.973377\n",
      "[61]\tvalid_0's binary_logloss: 0.201027\tvalid_0's binary_error: 0.0826805\tvalid_0's auc: 0.973385\n",
      "[62]\tvalid_0's binary_logloss: 0.200951\tvalid_0's binary_error: 0.0826557\tvalid_0's auc: 0.973393\n",
      "[63]\tvalid_0's binary_logloss: 0.200893\tvalid_0's binary_error: 0.0826731\tvalid_0's auc: 0.973399\n",
      "[64]\tvalid_0's binary_logloss: 0.200835\tvalid_0's binary_error: 0.0826433\tvalid_0's auc: 0.97341\n",
      "[65]\tvalid_0's binary_logloss: 0.200783\tvalid_0's binary_error: 0.0826408\tvalid_0's auc: 0.973418\n",
      "[66]\tvalid_0's binary_logloss: 0.200732\tvalid_0's binary_error: 0.0826284\tvalid_0's auc: 0.973428\n",
      "[67]\tvalid_0's binary_logloss: 0.200682\tvalid_0's binary_error: 0.0826582\tvalid_0's auc: 0.973435\n",
      "[68]\tvalid_0's binary_logloss: 0.200644\tvalid_0's binary_error: 0.0826234\tvalid_0's auc: 0.973443\n",
      "[69]\tvalid_0's binary_logloss: 0.200611\tvalid_0's binary_error: 0.0826259\tvalid_0's auc: 0.973446\n",
      "[70]\tvalid_0's binary_logloss: 0.200561\tvalid_0's binary_error: 0.0826334\tvalid_0's auc: 0.973455\n",
      "[71]\tvalid_0's binary_logloss: 0.200525\tvalid_0's binary_error: 0.0825937\tvalid_0's auc: 0.973464\n",
      "[72]\tvalid_0's binary_logloss: 0.200498\tvalid_0's binary_error: 0.0825738\tvalid_0's auc: 0.97347\n",
      "[73]\tvalid_0's binary_logloss: 0.200469\tvalid_0's binary_error: 0.0825416\tvalid_0's auc: 0.973475\n",
      "[74]\tvalid_0's binary_logloss: 0.200426\tvalid_0's binary_error: 0.0824944\tvalid_0's auc: 0.973483\n",
      "[75]\tvalid_0's binary_logloss: 0.200401\tvalid_0's binary_error: 0.0824894\tvalid_0's auc: 0.973486\n",
      "[76]\tvalid_0's binary_logloss: 0.200362\tvalid_0's binary_error: 0.0824522\tvalid_0's auc: 0.973491\n",
      "[77]\tvalid_0's binary_logloss: 0.200612\tvalid_0's binary_error: 0.082477\tvalid_0's auc: 0.973452\n",
      "[78]\tvalid_0's binary_logloss: 0.200546\tvalid_0's binary_error: 0.082482\tvalid_0's auc: 0.973431\n",
      "[79]\tvalid_0's binary_logloss: 0.200428\tvalid_0's binary_error: 0.0824597\tvalid_0's auc: 0.973474\n",
      "[80]\tvalid_0's binary_logloss: 0.200414\tvalid_0's binary_error: 0.0824671\tvalid_0's auc: 0.973478\n",
      "[81]\tvalid_0's binary_logloss: 0.200556\tvalid_0's binary_error: 0.0824696\tvalid_0's auc: 0.973425\n",
      "[82]\tvalid_0's binary_logloss: 0.200911\tvalid_0's binary_error: 0.0824497\tvalid_0's auc: 0.973452\n",
      "[83]\tvalid_0's binary_logloss: 0.200525\tvalid_0's binary_error: 0.0824746\tvalid_0's auc: 0.973437\n",
      "[84]\tvalid_0's binary_logloss: 0.200749\tvalid_0's binary_error: 0.0824597\tvalid_0's auc: 0.973461\n",
      "[85]\tvalid_0's binary_logloss: 0.200767\tvalid_0's binary_error: 0.0824299\tvalid_0's auc: 0.973459\n",
      "[86]\tvalid_0's binary_logloss: 0.200927\tvalid_0's binary_error: 0.0824398\tvalid_0's auc: 0.973432\n",
      "[87]\tvalid_0's binary_logloss: 0.20065\tvalid_0's binary_error: 0.082482\tvalid_0's auc: 0.973415\n",
      "[88]\tvalid_0's binary_logloss: 0.201492\tvalid_0's binary_error: 0.0825316\tvalid_0's auc: 0.973312\n",
      "[89]\tvalid_0's binary_logloss: 0.201028\tvalid_0's binary_error: 0.0824572\tvalid_0's auc: 0.973402\n",
      "[90]\tvalid_0's binary_logloss: 0.200738\tvalid_0's binary_error: 0.0824423\tvalid_0's auc: 0.973425\n",
      "[91]\tvalid_0's binary_logloss: 0.200592\tvalid_0's binary_error: 0.0824373\tvalid_0's auc: 0.973447\n",
      "[92]\tvalid_0's binary_logloss: 0.201672\tvalid_0's binary_error: 0.0824547\tvalid_0's auc: 0.97339\n",
      "[93]\tvalid_0's binary_logloss: 0.200542\tvalid_0's binary_error: 0.0824125\tvalid_0's auc: 0.973457\n",
      "[94]\tvalid_0's binary_logloss: 0.200957\tvalid_0's binary_error: 0.0824373\tvalid_0's auc: 0.973434\n",
      "[95]\tvalid_0's binary_logloss: 0.200524\tvalid_0's binary_error: 0.0824646\tvalid_0's auc: 0.97346\n",
      "[96]\tvalid_0's binary_logloss: 0.20089\tvalid_0's binary_error: 0.082482\tvalid_0's auc: 0.973428\n",
      "[97]\tvalid_0's binary_logloss: 0.200773\tvalid_0's binary_error: 0.082482\tvalid_0's auc: 0.973432\n",
      "[98]\tvalid_0's binary_logloss: 0.200933\tvalid_0's binary_error: 0.0825316\tvalid_0's auc: 0.973345\n",
      "[99]\tvalid_0's binary_logloss: 0.200915\tvalid_0's binary_error: 0.0825217\tvalid_0's auc: 0.973383\n",
      "[100]\tvalid_0's binary_logloss: 0.20093\tvalid_0's binary_error: 0.0825316\tvalid_0's auc: 0.973403\n"
     ]
    }
   ],
   "source": [
    "num_round = 100\n",
    "train_data = lgb.Dataset( train_x[:,1:6] , label=train_y )\n",
    "val_data   = lgb.Dataset( val_x[:,1:6]   , label=val_y   )\n",
    "bst = lgb.train(param, train_data, num_round, valid_sets=val_data)\n",
    "pred_qcd_test = bst.predict(vars_qcd_test[:,1:6])\n",
    "pred_top_test = bst.predict(vars_top_test[:,1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PahL9BDcAMVx"
   },
   "source": [
    "## Plot ROC for above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "EThJSSs6_tVL",
    "outputId": "dc5b324d-ef0b-4f56-af91-ff6b5d91b9b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "[<matplotlib.lines.Line2D object at 0x7f83f9de0d68>]"
      ],
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f83f9de0d68>]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = 0.0000001\n",
    "num = 1000\n",
    "dx = ( 1.0 + (epsilon*2) ) / num\n",
    "qcdeff_noloss = np.ones((num))\n",
    "topeff_noloss = np.ones((num))\n",
    "for i in range(num):\n",
    "  xval = (i*dx) - epsilon\n",
    "  qcdeff_noloss[i]=1.0/(Count(pred_qcd_test,xval)+epsilon)\n",
    "  topeff_noloss[i]=Count(pred_top_test,xval)\n",
    "plt.yscale('log')\n",
    "plt.plot(topeff_noloss,qcdeff_noloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fQfkj0C_639"
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"topeff_loss\",topeff_loss)\n",
    "np.savetxt(\"qcdeff_loss\",qcdeff_loss)\n",
    "np.savetxt(\"topeff_all\",topeff_all)\n",
    "np.savetxt(\"qcdeff_all\",qcdeff_all)\n",
    "np.savetxt(\"topeff_noloss\",topeff_noloss)\n",
    "np.savetxt(\"qcdeff_noloss\",qcdeff_noloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZrBZENzMDEL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML4JetsCompare.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
